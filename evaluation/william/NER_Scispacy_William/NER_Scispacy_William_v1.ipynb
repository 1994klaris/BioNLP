{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "import os\n",
    "from spacy import displacy\n",
    "from spacy.pipeline import Tagger\n",
    "import json \n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths to pipeline,models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_md = '/home/william/Courses/EDAN70/en_core_sci_md-0.2.4/en_core_sci_md/en_core_sci_md-0.2.4'\n",
    "bionlpg13cg = '/home/william/Courses/EDAN70/en_ner_bionlp13cg_md-0.2.3/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.2.3'\n",
    "craft = '/home/william/Courses/EDAN70/en_ner_craft_md-0.2.3/en_ner_craft_md/en_ner_craft_md-0.2.3'\n",
    "bc5cdr = '/home/william/Courses/EDAN70/en_ner_bc5cdr_md-0.2.4/en_ner_bc5cdr_md/en_ner_bc5cdr_md-0.2.4'\n",
    "path_data = '/home/william/Courses/EDAN70/comm_use_subset_100_new/comm_use_subset_100/'\n",
    "path_csv = '/home/william/Courses/EDAN70/comm_use_subset_100_new/metadata_comm_use_subset_100.csv'\n",
    "path_out_bionlpg13cg = '/home/william/Courses/EDAN70/out/bionlpg13cg/'\n",
    "path_out_craft = '/home/william/Courses/EDAN70/out/craft/'\n",
    "path_out_bc5cdr = '/home/william/Courses/EDAN70/out/bc5cdr/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Spacy pipeline and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sci_md = spacy.load(sci_md)\n",
    "nlp_bionlpg13cg = spacy.load(bionlpg13cg)\n",
    "nlp_craft = spacy.load(craft)\n",
    "nlp_bc5cdr = spacy.load(bc5cdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scan all file names (not necessarily ordered in the same way as in file folder) and put them in an initial data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(path_data)\n",
    "json_files = [pos_json for pos_json in os.listdir(path_data) if pos_json.endswith('.json')]\n",
    "data = {}\n",
    "for filename in json_files:\n",
    "    with open(path_data + filename) as f:\n",
    "        data[filename] = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add all texts in a dictionary with their article_id as key and separate title, abstract, and body texts in a better way than before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_data = {}\n",
    "for key in data:\n",
    "    texts = []\n",
    "    abstract = []\n",
    "    for i in range(len(data[key]['body_text'])): \n",
    "        texts.append(data[key]['body_text'][i]['text'])\n",
    "    for i in range(len(data[key]['abstract'])):\n",
    "        abstract.append(data[key]['abstract'][i]['text'])\n",
    "    structured_data[key] = {'title':data[key]['metadata']['title'], 'abstract':abstract, 'texts':texts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CSV data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313d6762ff0c7e18ed7af39482b04fbd2d280bc7.json\n",
      "406aad9b3f74f619a1aac3f29ad531b28fb7c37e.json\n",
      "1cdf5f7fb665c7d9efaf8562af501d3b3bdd9273.json\n",
      "54774da1d5cb436f9bac63726a08b399b05c6992.json\n",
      "b8423ebcd07e62c48ca7205a06fee753f9199e58.json\n",
      "a03298211c385b7bd1d1ebdc98b1cc69efe7f54d.json\n",
      "fd31ade73c720f893a5ddb2370df90a84511ec4a.json\n",
      "1951545d70e8568083cde219d2583c1aae0c4061.json\n",
      "88c599b1de8a17cd519f9f72aaaea28392f697f0.json\n",
      "914c8d326beaccabd37199a9903b27fa7f9b0586.json\n",
      "f6a6f1fcc991eaf6e733d88adb25a3a7a8e65546.json\n",
      "849541788c0fe480a2fb9e13b20f3937e759b249.json\n",
      "5e11e3105842cf7cedf10e26bff170a52c23cf00.json\n",
      "0b65341c3090421acaac37ec4c93212277be55bb.json\n",
      "3c67252392670f7eb8bd7ec3c01daf8e2b3a6da9.json\n",
      "4997e92b014bb26f8184eae26662a09ffcf4f733.json\n",
      "fae8f01ff5ef7b1737b60dfba0e183bdf482aaba.json\n",
      "dfbb7bb506552b06794dbcefe56911e2a91a908b.json\n",
      "5fc3eedb09f3b7624fec7d2b9afedfffbab47a21.json\n",
      "7d9f6bae2b7ced5834b4e92e7ef805d85fdea405.json\n",
      "80993091f576dc7fdbec10552b45b4af5eec2b8b.json\n",
      "2e8bc0044aba50cc5c208dbab92d35493e05eb74.json\n",
      "efd27ff0ac04dd60838266386aaebb5df80f4fa9.json\n",
      "4c4dd391e165ce712cb8871ce3082927bbf21ec9.json\n",
      "0072159e1ebecc889e9bcabb58bb45c47e18a403.json\n",
      "cee3dd6c1927b863e94e9493295d99213401f158.json\n",
      "399cef5a5c8ca96a0ac33ee31dec722eac145a7b.json\n",
      "ea97faa57e408d3c7a5b84dbe3da7ae4feb9992e.json\n",
      "4631d1a1d9384ce4aa9d96d6695417357855ce84.json\n",
      "42a8d5fd943cd29a518ec9683f549f105aac92f8.json\n",
      "c8df0fd2c7d06947d2bb15ebed6ce0bad831b297.json\n",
      "70adc25ffcec7b3d1871ea866658c3cd01080f49.json\n",
      "145bb000d606bc2ea82e7e11f79363796bc9b7b1.json\n",
      "9b7a0ad7b6c7f59e7a6cf1dc9d07912a273d19b5.json\n",
      "1392d02729b5e2f1a63ff3ff259e10a8c45e5a55.json\n",
      "204a803a89944c81ecd8b50ddfb85e78db5df7f3.json\n",
      "235855da90347b259f4d6b244821f3c1c264c04e.json\n",
      "6548b941beede8bb57cde83a928e4383b54697ad.json\n",
      "9c1c6037784f97a987e79a92077dad0d252b7236.json\n",
      "fd521d37325a60e6ed92aa9bcf4541b9a68ee90e.json\n",
      "24c4787033b7069da07ce6b0b8f85f6fd5ad84b2.json\n",
      "2a3a68a67776a01043a6a156b69be803c7118a6d.json\n",
      "fdf03dcd427aa029a6b213288a7e862519decafb.json\n",
      "372caa549b07492de5fd7064e9cadc62bef0f478.json\n",
      "8d14b700a065187eb4d8b01e0e9b6e3e37e5d09b.json\n",
      "2d266d6c9ef81ffefa8fc5ba319851d220e71cbd.json\n",
      "36ce44a073e35fa159c1aa4fa86286643a6e9993.json\n",
      "296e24d0dc3c17633eeffea57bd23e968a1c8a5e.json\n",
      "bfbb1dddd2a6d304edf8c9cf1086613210f4a8f6.json\n",
      "73761176155e77d6873806dd9579f59e5419111a.json\n",
      "8bcd86c97a06dbdff096f6cb164f9dab35f879f9.json\n",
      "c0f7f46699d0ecf0c7ead699d564d952508e6f81.json\n",
      "e703a33d5803f827c3d57a19385c0c2cbc2830e9.json\n",
      "08dd1169c7aa45621e5f48bf6246b049e071e445.json\n",
      "d2057831023248265b009ed7fb7710123c69feaa.json\n",
      "98936f4aa8ad9759352811bbb4aa531c2f061cd7.json\n",
      "37142e322bc30493a97e2aff05533919897e39ef.json\n",
      "8eb3bdc336b90a70201e28477934062f67afcbcf.json\n",
      "dcc4db04a2a7001e0b9931be5648c7f0cc233620.json\n",
      "686258462f76bc196e1b056ebbbb93a6f53c982e.json\n",
      "9d2c6b24e096eac147115b77f295e0ddcb5435c6.json\n",
      "c3e92ff23f71052b519afe647c6d69f0bf1b8e0c.json\n",
      "f2d95ab758e878195eefa64325961667efeb4c14.json\n",
      "4fc15d2af497369e0e7aa99dc78a403bdfa4790f.json\n",
      "a4ac0a556416712bc053f001c5bc6931c908bfd1.json\n",
      "3c988a00ed94308e36a8814b391ead01480efb60.json\n",
      "208f4d527c8ac85c04dd9e431babfaaa54e66428.json\n",
      "642e03f3b71051b01c77af78f51bb4ef5d00ad08.json\n",
      "e658837e4d77ef0bccdd849648e2db595f5ec65b.json\n",
      "fc9a6a00498ee4dfeaf7c201c3341aca6ad21409.json\n",
      "8ec3cfefcf560550c37dbc48107102bf2c893036.json\n",
      "74d9ff6a1e0b0d53a989aa00d2eddfd3d4f4dbd5.json\n",
      "35cf9c43b3b1028de85e24946f7bbc0436b24190.json\n",
      "15f2b1915443fff0466f0dd89c7c0ab6761833b4.json\n",
      "da309b7481a117d1aad2212d42ad3b7c0c68abf3.json\n",
      "4ec1787f65505b0751777ff1f2cb40d13c0d2b96.json\n",
      "5edd56f96660760a5f8ae69efd7d9e976a00c4ee.json\n",
      "f4016789973bc61cdb84c457c95e15426e2143b3.json\n",
      "2f9d66c86c6ef08d8e3679e2cd01c2b140e68a32.json\n",
      "4a94f3fe57da5dad4c965d59faf0facc2039175a.json\n",
      "85eb641e06b0d6b1a0b202275add0c5d27e53d71.json\n",
      "9697849eeaed7f72a196d9ca4d4d8cfad6a8031e.json\n",
      "cc03e915afeb7acfe93595b75a38f7b58bf2d752.json\n",
      "cdf3bcf39150a5ef5537651e90d93cbccd097f79.json\n",
      "6a6032614741732c373d0a939ce85785519c3fa6.json\n",
      "c4127920bfadc5eaa2ff7d2180f8e1de7e3a673a.json\n",
      "b5af7319ee1fd5e9cc3cf8c5836b19a408ac519b.json\n",
      "d408854267a5f3330fab52531265d9a38a91e60f.json\n",
      "68ca53b6414e682661c4decd41eb9ff00574d862.json\n",
      "1a52cac849bdb5840d115cf513ec8cc63dc2d56c.json\n",
      "773219a6abb0977144db395a5d94f63ce2eb03c6.json\n",
      "5e85184d2ba432306c3c21f22113aa590b09fe89.json\n",
      "1adf578fb1c7019c2293b2e9c7cc3f036f9bc15b.json\n",
      "0d7c95184b8a77a7fc0e35995ddc804ba4145429.json\n",
      "c296f6a98b4eb94a69e4caef2df38ccf6e498511.json\n",
      "f7f482ba4aaa3ef0d620d9aad8c9635f88806d8e.json\n",
      "77c5a33cacec66e3eecf748902a04cd245df6e0f.json\n",
      "8d40046d4ced2a558530e5e242fabbee66b213d7.json\n",
      "c6e4261e1a6aa596741c8af357661827bff2c468.json\n",
      "0a4ff36a0de0c6c9efdcc2f20e3223a31e4c05a0.json\n"
     ]
    }
   ],
   "source": [
    "keys = structured_data.keys()\n",
    "csv_data = {}\n",
    "#print(keys)\n",
    "with open(path_csv, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "        else:\n",
    "            key = row[1]\n",
    "            keys_helper = key.split(\"; \")\n",
    "            for i in range(len(keys_helper)):\n",
    "                key_temp = keys_helper[i] + '.json'\n",
    "                for k in keys: \n",
    "                    if(key_temp == k):\n",
    "                        key = key_temp\n",
    "                        break\n",
    "      \n",
    "            csv_data[key] = {'cord_uid': row[0], 'sourcedb': row[2], 'sourceid': row[5]}\n",
    "for key in csv_data:\n",
    "    print(key)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data structure by printing an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TNF-α Acts as an Immunoregulator in the Mouse Brain by Reducing the Incidence of Severe Disease Following Japanese Encephalitis Virus Infection\n",
      "1\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "key = '77c5a33cacec66e3eecf748902a04cd245df6e0f.json'\n",
    "print(structured_data[key]['title'])\n",
    "print(len(structured_data[key]['abstract']))\n",
    "print(len(structured_data[key]['texts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize dicts for processed articles and add processed articles for each pipeline/model HEAVY TASK which\n",
    "is separated into different cells in order to be run on HP Spectre x360 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sci_md = {}\n",
    "doc_bionlpg13cg = {}\n",
    "doc_craft = {}\n",
    "doc_bc5cdr = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all keys in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f2d95ab758e878195eefa64325961667efeb4c14.json\n",
      "b5af7319ee1fd5e9cc3cf8c5836b19a408ac519b.json\n",
      "406aad9b3f74f619a1aac3f29ad531b28fb7c37e.json\n",
      "42a8d5fd943cd29a518ec9683f549f105aac92f8.json\n",
      "773219a6abb0977144db395a5d94f63ce2eb03c6.json\n",
      "37142e322bc30493a97e2aff05533919897e39ef.json\n",
      "c296f6a98b4eb94a69e4caef2df38ccf6e498511.json\n",
      "68ca53b6414e682661c4decd41eb9ff00574d862.json\n",
      "372caa549b07492de5fd7064e9cadc62bef0f478.json\n",
      "4c4dd391e165ce712cb8871ce3082927bbf21ec9.json\n",
      "dfbb7bb506552b06794dbcefe56911e2a91a908b.json\n",
      "2a3a68a67776a01043a6a156b69be803c7118a6d.json\n",
      "fc9a6a00498ee4dfeaf7c201c3341aca6ad21409.json\n",
      "1cdf5f7fb665c7d9efaf8562af501d3b3bdd9273.json\n",
      "208f4d527c8ac85c04dd9e431babfaaa54e66428.json\n",
      "8d14b700a065187eb4d8b01e0e9b6e3e37e5d09b.json\n",
      "8ec3cfefcf560550c37dbc48107102bf2c893036.json\n",
      "4fc15d2af497369e0e7aa99dc78a403bdfa4790f.json\n",
      "73761176155e77d6873806dd9579f59e5419111a.json\n",
      "fdf03dcd427aa029a6b213288a7e862519decafb.json\n",
      "8bcd86c97a06dbdff096f6cb164f9dab35f879f9.json\n",
      "9d2c6b24e096eac147115b77f295e0ddcb5435c6.json\n",
      "5e85184d2ba432306c3c21f22113aa590b09fe89.json\n",
      "7d9f6bae2b7ced5834b4e92e7ef805d85fdea405.json\n",
      "a03298211c385b7bd1d1ebdc98b1cc69efe7f54d.json\n",
      "c3e92ff23f71052b519afe647c6d69f0bf1b8e0c.json\n",
      "15f2b1915443fff0466f0dd89c7c0ab6761833b4.json\n",
      "80993091f576dc7fdbec10552b45b4af5eec2b8b.json\n",
      "1392d02729b5e2f1a63ff3ff259e10a8c45e5a55.json\n",
      "fd521d37325a60e6ed92aa9bcf4541b9a68ee90e.json\n",
      "fd31ade73c720f893a5ddb2370df90a84511ec4a.json\n",
      "9b7a0ad7b6c7f59e7a6cf1dc9d07912a273d19b5.json\n",
      "bfbb1dddd2a6d304edf8c9cf1086613210f4a8f6.json\n",
      "54774da1d5cb436f9bac63726a08b399b05c6992.json\n",
      "88c599b1de8a17cd519f9f72aaaea28392f697f0.json\n",
      "1951545d70e8568083cde219d2583c1aae0c4061.json\n",
      "2f9d66c86c6ef08d8e3679e2cd01c2b140e68a32.json\n",
      "cc03e915afeb7acfe93595b75a38f7b58bf2d752.json\n",
      "e703a33d5803f827c3d57a19385c0c2cbc2830e9.json\n",
      "cdf3bcf39150a5ef5537651e90d93cbccd097f79.json\n",
      "08dd1169c7aa45621e5f48bf6246b049e071e445.json\n",
      "da309b7481a117d1aad2212d42ad3b7c0c68abf3.json\n",
      "c6e4261e1a6aa596741c8af357661827bff2c468.json\n",
      "8eb3bdc336b90a70201e28477934062f67afcbcf.json\n",
      "204a803a89944c81ecd8b50ddfb85e78db5df7f3.json\n",
      "e658837e4d77ef0bccdd849648e2db595f5ec65b.json\n",
      "3c67252392670f7eb8bd7ec3c01daf8e2b3a6da9.json\n",
      "9c1c6037784f97a987e79a92077dad0d252b7236.json\n",
      "5edd56f96660760a5f8ae69efd7d9e976a00c4ee.json\n",
      "5e11e3105842cf7cedf10e26bff170a52c23cf00.json\n",
      "4631d1a1d9384ce4aa9d96d6695417357855ce84.json\n",
      "0a4ff36a0de0c6c9efdcc2f20e3223a31e4c05a0.json\n",
      "85eb641e06b0d6b1a0b202275add0c5d27e53d71.json\n",
      "0d7c95184b8a77a7fc0e35995ddc804ba4145429.json\n",
      "35cf9c43b3b1028de85e24946f7bbc0436b24190.json\n",
      "0072159e1ebecc889e9bcabb58bb45c47e18a403.json\n",
      "313d6762ff0c7e18ed7af39482b04fbd2d280bc7.json\n",
      "1adf578fb1c7019c2293b2e9c7cc3f036f9bc15b.json\n",
      "f6a6f1fcc991eaf6e733d88adb25a3a7a8e65546.json\n",
      "ea97faa57e408d3c7a5b84dbe3da7ae4feb9992e.json\n",
      "2d266d6c9ef81ffefa8fc5ba319851d220e71cbd.json\n",
      "399cef5a5c8ca96a0ac33ee31dec722eac145a7b.json\n",
      "24c4787033b7069da07ce6b0b8f85f6fd5ad84b2.json\n",
      "fae8f01ff5ef7b1737b60dfba0e183bdf482aaba.json\n",
      "1a52cac849bdb5840d115cf513ec8cc63dc2d56c.json\n",
      "c4127920bfadc5eaa2ff7d2180f8e1de7e3a673a.json\n",
      "d2057831023248265b009ed7fb7710123c69feaa.json\n",
      "4a94f3fe57da5dad4c965d59faf0facc2039175a.json\n",
      "0b65341c3090421acaac37ec4c93212277be55bb.json\n",
      "642e03f3b71051b01c77af78f51bb4ef5d00ad08.json\n",
      "70adc25ffcec7b3d1871ea866658c3cd01080f49.json\n",
      "98936f4aa8ad9759352811bbb4aa531c2f061cd7.json\n",
      "efd27ff0ac04dd60838266386aaebb5df80f4fa9.json\n",
      "914c8d326beaccabd37199a9903b27fa7f9b0586.json\n",
      "cee3dd6c1927b863e94e9493295d99213401f158.json\n",
      "a4ac0a556416712bc053f001c5bc6931c908bfd1.json\n",
      "4997e92b014bb26f8184eae26662a09ffcf4f733.json\n",
      "f4016789973bc61cdb84c457c95e15426e2143b3.json\n",
      "686258462f76bc196e1b056ebbbb93a6f53c982e.json\n",
      "145bb000d606bc2ea82e7e11f79363796bc9b7b1.json\n",
      "d408854267a5f3330fab52531265d9a38a91e60f.json\n",
      "f7f482ba4aaa3ef0d620d9aad8c9635f88806d8e.json\n",
      "5fc3eedb09f3b7624fec7d2b9afedfffbab47a21.json\n",
      "849541788c0fe480a2fb9e13b20f3937e759b249.json\n",
      "296e24d0dc3c17633eeffea57bd23e968a1c8a5e.json\n",
      "9697849eeaed7f72a196d9ca4d4d8cfad6a8031e.json\n",
      "6a6032614741732c373d0a939ce85785519c3fa6.json\n",
      "b8423ebcd07e62c48ca7205a06fee753f9199e58.json\n",
      "3c988a00ed94308e36a8814b391ead01480efb60.json\n",
      "c0f7f46699d0ecf0c7ead699d564d952508e6f81.json\n",
      "dcc4db04a2a7001e0b9931be5648c7f0cc233620.json\n",
      "36ce44a073e35fa159c1aa4fa86286643a6e9993.json\n",
      "235855da90347b259f4d6b244821f3c1c264c04e.json\n",
      "77c5a33cacec66e3eecf748902a04cd245df6e0f.json\n",
      "4ec1787f65505b0751777ff1f2cb40d13c0d2b96.json\n",
      "6548b941beede8bb57cde83a928e4383b54697ad.json\n",
      "2e8bc0044aba50cc5c208dbab92d35493e05eb74.json\n",
      "8d40046d4ced2a558530e5e242fabbee66b213d7.json\n",
      "c8df0fd2c7d06947d2bb15ebed6ce0bad831b297.json\n",
      "74d9ff6a1e0b0d53a989aa00d2eddfd3d4f4dbd5.json\n"
     ]
    }
   ],
   "source": [
    "for key in structured_data:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying spacy pipeline and all of the models to the data set, separated into different cells with a counter to check progress in loading, 100 represents a completed task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for key in structured_data:\n",
    "    title = nlp_sci_md(structured_data[key]['title'])\n",
    "    abstract = []\n",
    "    texts = []\n",
    "                       \n",
    "    if(len(structured_data[key]['abstract']) != 0):\n",
    "        for i in range(len(structured_data[key]['abstract'])):\n",
    "            abstract.append(nlp_sci_md(structured_data[key]['abstract'][i]))\n",
    "    if(len(structured_data[key]['texts']) != 0):\n",
    "        for i in range(len(structured_data[key]['texts'])):\n",
    "            texts.append(nlp_sci_md(structured_data[key]['texts'][i]))\n",
    "    \n",
    "    if(len(abstract) == 0 and len(texts) == 0):\n",
    "        doc_sci_md[key] = {'title':title}\n",
    "    elif(len(abstract) != 0 and len(texts) == 0):\n",
    "        doc_sci_md[key] = {'title':title, 'abstract':abstract,}\n",
    "    elif(len(abstract) == 0 and len(texts) != 0):\n",
    "        doc_sci_md[key] = {'title':title, 'texts':texts}\n",
    "    elif(len(abstract) != 0 and len(texts) != 0):\n",
    "        doc_sci_md[key] = {'title':title,  'abstract':abstract, 'texts':texts}\n",
    "    \n",
    "    n = n + 1\n",
    "    print(n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for key in structured_data:\n",
    "    title = nlp_bionlpg13cg(structured_data[key]['title'])\n",
    "    abstract = []\n",
    "    texts = []\n",
    "                       \n",
    "    if(len(structured_data[key]['abstract']) != 0):\n",
    "        for i in range(len(structured_data[key]['abstract'])):\n",
    "            abstract.append(nlp_bionlpg13cg(structured_data[key]['abstract'][i]))\n",
    "    if(len(structured_data[key]['texts']) != 0):\n",
    "        for i in range(len(structured_data[key]['texts'])):\n",
    "            texts.append(nlp_bionlpg13cg(structured_data[key]['texts'][i]))\n",
    "    \n",
    "    if(len(abstract) == 0 and len(texts) == 0):\n",
    "        doc_bionlpg13cg[key] = {'title':title}\n",
    "    elif(len(abstract) != 0 and len(texts) == 0):\n",
    "        doc_bionlpg13cg[key] = {'title':title, 'abstract':abstract,}\n",
    "    elif(len(abstract) == 0 and len(texts) != 0):\n",
    "        doc_bionlpg13cg[key] = {'title':title, 'texts':texts}\n",
    "    elif(len(abstract) != 0 and len(texts) != 0):\n",
    "        doc_bionlpg13cg[key] = {'title':title,  'abstract':abstract, 'texts':texts}\n",
    "    \n",
    "    n = n + 1\n",
    "    print(n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for key in structured_data:\n",
    "    title = nlp_craft(structured_data[key]['title'])\n",
    "    abstract = []\n",
    "    texts = []\n",
    "                       \n",
    "    if(len(structured_data[key]['abstract']) != 0):\n",
    "        for i in range(len(structured_data[key]['abstract'])):\n",
    "            abstract.append(nlp_craft(structured_data[key]['abstract'][i]))\n",
    "    if(len(structured_data[key]['texts']) != 0):\n",
    "        for i in range(len(structured_data[key]['texts'])):\n",
    "            texts.append(nlp_craft(structured_data[key]['texts'][i]))\n",
    "    \n",
    "    if(len(abstract) == 0 and len(texts) == 0):\n",
    "        doc_craft[key] = {'title':title}\n",
    "    elif(len(abstract) != 0 and len(texts) == 0):\n",
    "        doc_craft[key] = {'title':title, 'abstract':abstract,}\n",
    "    elif(len(abstract) == 0 and len(texts) != 0):\n",
    "        doc_craft[key] = {'title':title, 'texts':texts}\n",
    "    elif(len(abstract) != 0 and len(texts) != 0):\n",
    "        doc_craft[key] = {'title':title,  'abstract':abstract, 'texts':texts}\n",
    "    \n",
    "    n = n + 1\n",
    "    print(n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for key in structured_data:\n",
    "    title = nlp_bc5cdr(structured_data[key]['title'])\n",
    "    abstract = []\n",
    "    texts = []\n",
    "                       \n",
    "    if(len(structured_data[key]['abstract']) != 0):\n",
    "        for i in range(len(structured_data[key]['abstract'])):\n",
    "            abstract.append(nlp_bc5cdr(structured_data[key]['abstract'][i]))\n",
    "    if(len(structured_data[key]['texts']) != 0):\n",
    "        for i in range(len(structured_data[key]['texts'])):\n",
    "            texts.append(nlp_bc5cdr(structured_data[key]['texts'][i]))\n",
    "    \n",
    "    if(len(abstract) == 0 and len(texts) == 0):\n",
    "        doc_bc5cdr[key] = {'title':title}\n",
    "    elif(len(abstract) != 0 and len(texts) == 0):\n",
    "        doc_bc5cdr[key] = {'title':title, 'abstract':abstract,}\n",
    "    elif(len(abstract) == 0 and len(texts) != 0):\n",
    "        doc_bc5cdr[key] = {'title':title, 'texts':texts}\n",
    "    elif(len(abstract) != 0 and len(texts) != 0):\n",
    "        doc_bc5cdr[key] = {'title':title,  'abstract':abstract, 'texts':texts}\n",
    "    \n",
    "    n = n + 1\n",
    "    print(n)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Zika virus, vaccine, envelope-NS1 polyprotein)\n",
      "(Inhibition, Putative Dihydropyrimidinase, Pseudomonas aeruginosa, PAO1, Flavonoids, Substrates, Cyclic Amidohydrolases)\n",
      "(Toll-Like Receptor 3, Respiratory Viral Infections)\n",
      "(Prevalence, dental caries, associated factors, primary school children, population-based cross-sectional study, Riyadh, Saudi Arabia)\n",
      "(Stabilized, coronavirus, spikes, resistant, conformational, induced, receptor recognition, proteolysis, OPEN)\n",
      "(exercises, improve, public health, preparedness, Asia, Middle East, Africa)\n",
      "(Pathogenic, mechanisms, intravascular coagulation, lungs, BVDV-infected calves, BHV-1)\n",
      "(Targeting, Nasal Mucosa, Japanese, Encephalitis Virus, Non-Vector-Borne Transmission)\n",
      "(Citation, Zhao C, Zhao W, NLRP3, Player, Antiviral Responses)\n",
      "(Detection, Cytomegalovirus Antibodies, Biosensor, Imaging Ellipsometry)\n",
      "(Assessment of, Knowledge, Attitude, Practice, Prevention, Respiratory Tract Infections, Hajj, Umrah Pilgrims, Malaysia)\n",
      "(Vpu-interacting Protein SGTA, Expression, Non- glycosylated, Species, OPEN)\n",
      "(Identification, Characterization, Non-Structural Protein, Bluetongue Virus)\n",
      "(Reviewers',)\n",
      "(Health Á, resilience, arctic)\n",
      "(Prediction, RNA Pseudoknots, Heuristic Modeling, Mapping, Sequential)\n",
      "(Preliminary Study, Viral Metagenomics, French Bat Species, Contact, Humans, Identification, New Mammalian Viruses)\n",
      "(Malaria, Journal, Changes, var gene, mRNA, erythrocytic development, phenotypically, Plasmodium falciparum, parasites)\n",
      "(operating pathogen microarray, EOPM, screening, vertebrate pathogens)\n",
      "(Glycyrrhizic Acid, Treatment, Liver Diseases, Literature Review)\n",
      "(BMC, Public Health Factors, associated with, nosocomial, SARS-CoV, transmission, healthcare workers, Hanoi, Vietnam)\n",
      "(Molecular Imaging, Reveals, Progressive Pulmonary Inflammation, Lower Airways, Ferrets, Infected, H1N1 Pandemic Influenza Virus)\n",
      "(Neuronal, CCL2, expression, inflammatory monocyte infiltration, brain, acute virus infection)\n",
      "(Characterization, Rift Valley Fever Virus, MP-12, Strain, Encoding, NSs, Punta Toro Virus, Sandfly Fever, Sicilian Virus)\n",
      "(Systematic Identification, Novel, Essential, Affecting, Bromovirus RNA, Replication)\n",
      "(Economic Analysis, Costs, Associated with, Pre-Weaning Management, Dairy Heifers)\n",
      "(Detection, Orbivirus)\n",
      "()\n",
      "(effectiveness, conference, experience, improving, undergraduate medical, nursing students, attitudes, inter-professional education, Asian country)\n",
      "(Identification, CD8 + cytotoxic T lymphocyte epitopes, porcine, reproductive, respiratory syndrome virus matrix protein, BALB/c mice)\n",
      "(Distribution, antibodies, influenza virus, pigs, farrow-to-finish farms, Minas Gerais state, Brazil)\n",
      "(Waiting Time, Inter-Country Spread, Pandemic Influenza)\n",
      "(Severe Acute Respiratory Syndrome, 3a, 8a, Replication, Pathogenesis)\n",
      "(China,)\n",
      "(section, journal Frontiers, Microbiology Citation, Capsid Protein, VP1, Coxsackievirus B, Induces, Cell Cycle Arrest, Up-Regulating Heat Shock Protein)\n",
      "(Factors, emergence, arboviruses, strategies, challenges, limitations, control)\n",
      "(Monoclonal Antibodies, Glyco-Engineered Pichia pastoris)\n",
      "(Isolation, characterization, avian, coronavirus, healthy, Eclectus parrots, Eclectus roratus, Indonesia)\n",
      "(Prevalence, enteropathogens, antibiotic, sensitivity, pattern, puppies, hemorrhagic gastroenteritis)\n",
      "(Etiology, Clinical Characteristics, Influenza-Like Illness, ILI, Outpatients, Beijing)\n",
      "(Evaluation, Systems, Extraction, RNA, Stool, Suspensions, MS-2, Coliphage, Exogenous, RT-PCR, Inhibition)\n",
      "(Crystal Structure, Full-Length, Japanese, Encephalitis Virus)\n",
      "(Coronavirus Cell, Entry, Endo-/ Lysosomal Pathway, Proteolysis-Dependent)\n",
      "(human, trials, deployed, combat, infectious diseases, Zika virus, human)\n",
      "(antioxidants, Antioxidant Defence Systems, Oxidative Stress, Poultry Biology)\n",
      "(Survey, Ixodes pacificus, Ticks, California, Reveals, Diversity, Microorganisms, Novel, Widespread, Anaplasmataceae, Species)\n",
      "()\n",
      "(ARTICLE, Irreversible inhibitors, 3C protease, Coxsackie virus, templated assembly, protein-binding fragments)\n",
      "(Immunogenicity, Virus, Particle Forming, Baculoviral DNA Vaccine, Pandemic Influenza)\n",
      "(IFITM, Proteins, Restrict, Viral Membrane)\n",
      "(Population-based surveys, interventions, mental health, China)\n",
      "(Single detection, human, viral load, severe, respiratory tract, infections, healthy, children)\n",
      "()\n",
      "(diagnostics, Basics, Advancements, Diagnosis, Bacterial Lower Respiratory Tract Infections)\n",
      "()\n",
      "(Chaperone-Mediated, Autophagy Protein, BAG3, Negatively, Regulates, Ebola, Marburg, VP40-Mediated, Egress)\n",
      "(Exploitation, glycosylation, enveloped virus, pathobiology)\n",
      "(Antibodies, vaccines, Middle East respiratory syndrome)\n",
      "(Sequence, Structure Analysis, Distantly-Related Viruses, Reveals, Extensive, Gene Transfer, Viruses, Hosts, Viruses)\n",
      "()\n",
      "(Autologous Antibody Capture, Immunogenic Viruses, Viral Discovery)\n",
      "(FLDS, Comprehensive dsRNA Sequencing Method, Intracellular, RNA Virus Surveillance)\n",
      "(comparative epidemiology, Pathogenic avian, h5n1, h5n6, Vietnamese, spatiotemporal, Distribution, risk Factors)\n",
      "(Species-specific vulnerability, RanBP2, evolution, SIV, transmitted, African apes)\n",
      "(N-terminal domains, FLASH, Lsm11, heterotrimer, histone pre-mRNA 3'-end processing)\n",
      "(Temporal Analysis, Honey Bee Microbiome, Reveals, Viruses, Seasonal Prevalence, Viruses, Nosema, Crithidia)\n",
      "(Viroporins, Two-Stage Membrane Protein)\n",
      "(Percutaneous Vaccination, Effective, Delivery, MVA, MVA-Vectored Vaccines)\n",
      "(Comparative, proteomic analysis, responses, porcine, lymph nodes, virulent, attenuated, homologous African swine fever virus)\n",
      "(Fatal, Community-acquired Pneumonia, Children, Caused, Re-emergent Human Adenovirus 7d, Associated with, Severity, Illness, Fatality)\n",
      "(Transcriptional profiles, PBMCs, pigs, infected, genetically, diverse, porcine, reproductive, respiratory syndrome virus)\n",
      "()\n",
      "(Aetiology, Acute Respiratory Tract Infections, Hospitalised Children, Cyprus)\n",
      "(Pseudoknots, RNA Structures)\n",
      "(Bat Airway, Epithelial Cells, Tool, Study, Zoonotic Viruses)\n",
      "(Development, Plastic-Based Microfluidic Immunosensor Chip, Detection, H1N1)\n",
      "(DGV, Dengue Genographic Viewer)\n",
      "(BMC, Pulmonary Medicine, HSV-1-induced pneumonitis, patients, standard, immunosuppressive therapy, rheumatic, vasculitic, connective tissue disease)\n",
      "(section, journal Frontiers, Immunology IFN-Lambda 3, Antiviral Protection, Porcine Epidemic, Inducing, Antiviral Transcript, Porcine, Intestinal, Epithelia)\n",
      "(Widespread, Divergence, CEACAM/PSG Genes, Vertebrates, Humans, Sensitivity)\n",
      "(Feline, Infectious Peritonitis, Systemic Inflammatory Disease, Liver, Heart, Pathogenesis)\n",
      "(autophagy elongation complex, ATG5, positively, regulates, HCV, replication, wild-type)\n",
      "(viruses, Regulation, Stress Responses, Translational Control, Coronavirus)\n",
      "()\n",
      "(Elevated, adipogenesis, marrow mesenchymal stem cells, steroid-associated osteonecrosis, development)\n",
      "(Broad, neutralizing, human, monoclonal antibody, H7N9 strains)\n",
      "(Recombinant, Rotaviruses, Rescued, Reverse Genetics, Reveal, NSP5, Hyperphosphorylation, Viral Factories, VIRAL, EXPRESSION)\n",
      "(Assessing, dengue, Tokyo)\n",
      "(Single, Distinct, Membrane Topologies, Impact, Function, Infectious Bronchitis, Coronavirus E Protein)\n",
      "(Behavioural, intentions, response, influenza pandemic)\n",
      "(Yourself, Home, Viral Hijacking, PI3K/Akt Signaling Pathway)\n",
      "(Q&A, pathogens)\n",
      "(Expression, Foot-and-Mouth Disease Virus, Capsid Proteins, Silkworm-Baculovirus Expression System, Utilization, Subunit Vaccine)\n",
      "(TNF-α, Immunoregulator, Mouse, Brain, Reducing, Incidence, Severe Disease, Japanese, Encephalitis Virus Infection)\n",
      "(programmed ribosomal frameshifting, alternative proteomes)\n",
      "(Povidone-iodine hand wash, hand rub, in vitro, virucidal, efficacy, Ebola virus, modified vaccinia virus Ankara, European test virus, enveloped viruses)\n",
      "(Investigation, antiviral state, interferon-inducible transmembrane protein 1, induced, H9N2 virus, inactivated, viral particle, human, endothelial cells)\n",
      "(Immuno-modulating, properties, Tulathromycin, porcine, monocyte-derived macrophages, infected, porcine, reproductive, respiratory syndrome virus)\n",
      "(Computational, design, coding schemes, protein pairs)\n",
      "(Linkages, Chiropteran, diversity, ecosystem services, fragmented forest, conservation)\n"
     ]
    }
   ],
   "source": [
    "for key in doc_sci_md:\n",
    "    print(doc_sci_md[key]['title'].ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize dictionaries to store tagged entities in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_bionlpg13cg = {}\n",
    "chemicals_bionlpg13cg = {}\n",
    "\n",
    "species_craft = {}\n",
    "chemicals_craft = {}\n",
    "\n",
    "disease_bc5cdr = {}\n",
    "chemicals_bc5cdr = {}      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag all entities that are either diseases, chemicals or species for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIONLPG13CG\n",
    "for key in structured_data:\n",
    "    \n",
    "    species_bionlpg13cg_title = []\n",
    "    species_bionlpg13cg_abstract = []\n",
    "    species_bionlpg13cg_texts = {}\n",
    "\n",
    "    chemicals_bionlpg13cg_title = []\n",
    "    chemicals_bionlpg13cg_abstract = []\n",
    "    chemicals_bionlpg13cg_texts = {}\n",
    " \n",
    "\n",
    "    #Title\n",
    "    for e in doc_bionlpg13cg[key]['title'].ents:\n",
    "        if(e.label_ == 'ORGANISM'):\n",
    "            species_bionlpg13cg_title.append(e)\n",
    "        if(e.label_ == 'SIMPLE_CHEMICAL'):\n",
    "            chemicals_bionlpg13cg_title.append(e)\n",
    "\n",
    "    #Abstract    \n",
    "    if('abstract' in doc_bionlpg13cg[key].keys()):\n",
    "        for n in range(len(doc_bionlpg13cg[key]['abstract'])):\n",
    "            for e in doc_bionlpg13cg[key]['abstract'][n].ents:\n",
    "                if(e.label_ == 'ORGANISM'):\n",
    "                    species_bionlpg13cg_abstract.append(e)\n",
    "                if(e.label_ == 'SIMPLE_CHEMICAL'):\n",
    "                    chemicals_bionlpg13cg_abstract.append(e)\n",
    "\n",
    "    #Texts    \n",
    "    if('texts' in doc_bionlpg13cg[key].keys()):\n",
    "        for n in range(len(doc_bionlpg13cg[key]['texts'])):\n",
    "            help_species = []\n",
    "            help_chemicals = []\n",
    "            for e in doc_bionlpg13cg[key]['texts'][n].ents:\n",
    "                if(e.label_ == 'ORGANISM'):\n",
    "                    help_species.append(e)\n",
    "                if(e.label_ == 'SIMPLE_CHEMICAL'):\n",
    "                    help_chemicals.append(e)\n",
    "            species_bionlpg13cg_texts[n] = help_species\n",
    "            chemicals_bionlpg13cg_texts[n] = help_chemicals\n",
    "    \n",
    "    species_bionlpg13cg[key] = {'title':species_bionlpg13cg_title, 'abstract':species_bionlpg13cg_abstract\n",
    "                               ,'texts':species_bionlpg13cg_texts}\n",
    "    chemicals_bionlpg13cg[key] = {'title':chemicals_bionlpg13cg_title, 'abstract':chemicals_bionlpg13cg_abstract\n",
    "                               ,'texts':chemicals_bionlpg13cg_texts}\n",
    "    \n",
    "    for i in range(len(species_bionlpg13cg_texts)):\n",
    "        section = 'text_' + str(i)\n",
    "        species_bionlpg13cg[key][section] = species_bionlpg13cg_texts[i]\n",
    "        \n",
    "    for i in range(len(chemicals_bionlpg13cg_texts)):\n",
    "        section = 'text_' + str(i)\n",
    "        chemicals_bionlpg13cg[key][section] = chemicals_bionlpg13cg_texts[i]\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRAFT\n",
    "for key in structured_data:\n",
    "    \n",
    "    species_craft_title = []\n",
    "    species_craft_abstract = []\n",
    "    species_craft_texts = {}\n",
    "\n",
    "    chemicals_craft_title = []\n",
    "    chemicals_craft_abstract = []\n",
    "    chemicals_craft_texts = {}\n",
    " \n",
    "\n",
    "    #Title\n",
    "    for e in doc_craft[key]['title'].ents:\n",
    "        if(e.label_ == 'TAXON'):\n",
    "            species_craft_title.append(e)\n",
    "        if(e.label_ == 'CHEBI'):\n",
    "            chemicals_craft_title.append(e)\n",
    "\n",
    "    #Abstract    \n",
    "    if('abstract' in doc_craft[key].keys()):\n",
    "        for n in range(len(doc_craft[key]['abstract'])):\n",
    "            for e in doc_craft[key]['abstract'][n].ents:\n",
    "                if(e.label_ == 'TAXON'):\n",
    "                    species_craft_abstract.append(e)\n",
    "                if(e.label_ == 'CHEBI'):\n",
    "                    chemicals_craft_abstract.append(e)\n",
    "\n",
    "    #Texts    \n",
    "    if('texts' in doc_craft[key].keys()):\n",
    "        for n in range(len(doc_craft[key]['texts'])):\n",
    "            help_species = []\n",
    "            help_chemicals = []\n",
    "            for e in doc_craft[key]['texts'][n].ents:\n",
    "                if(e.label_ == 'TAXON'):\n",
    "                    help_species.append(e)\n",
    "                if(e.label_ == 'CHEBI'):\n",
    "                    help_chemicals.append(e)\n",
    "            species_craft_texts[n] = help_species\n",
    "            chemicals_craft_texts[n] = help_chemicals\n",
    "    \n",
    "    species_craft[key] = {'title':species_craft_title, 'abstract':species_craft_abstract\n",
    "                               ,'texts':species_craft_texts}\n",
    "    chemicals_craft[key] = {'title':chemicals_craft_title, 'abstract':chemicals_craft_abstract\n",
    "                               ,'texts':chemicals_craft_texts}\n",
    "    \n",
    "    for i in range(len(species_craft_texts)):\n",
    "        section = 'text_' + str(i)\n",
    "        species_craft[key][section] = species_craft_texts[i]\n",
    "        \n",
    "    for i in range(len(chemicals_craft_texts)):\n",
    "        section = 'text_' + str(i)\n",
    "        chemicals_craft[key][section] = chemicals_craft_texts[i]\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BC5CDR\n",
    "for key in structured_data:\n",
    "    \n",
    "    disease_bc5cdr_title = []\n",
    "    disease_bc5cdr_abstract = []\n",
    "    disease_bc5cdr_texts = {}\n",
    " \n",
    "    chemicals_bc5cdr_title = [] \n",
    "    chemicals_bc5cdr_abstract = [] \n",
    "    chemicals_bc5cdr_texts = {} \n",
    "\n",
    "    #Title\n",
    "    for e in doc_bc5cdr[key]['title'].ents:\n",
    "        if(e.label_ == 'DISEASE'):\n",
    "             disease_bc5cdr_title.append(e)\n",
    "        if(e.label_ == 'CHEMICAL'):\n",
    "            chemicals_bc5cdr_title.append(e)\n",
    "\n",
    "    #Abstract    \n",
    "    if('abstract' in doc_bc5cdr[key].keys()):\n",
    "        for n in range(len(doc_bc5cdr[key]['abstract'])):\n",
    "            for e in doc_bc5cdr[key]['abstract'][n].ents:\n",
    "                if(e.label_ == 'DISEASE'):\n",
    "                     disease_bc5cdr_abstract.append(e)\n",
    "                if(e.label_ == 'CHEMICAL'):\n",
    "                    chemicals_bc5cdr_abstract.append(e)\n",
    "\n",
    "    #Texts    \n",
    "    if('texts' in doc_bc5cdr[key].keys()):\n",
    "        for n in range(len(doc_bc5cdr[key]['texts'])):\n",
    "            help_disease = []\n",
    "            help_chemicals = []\n",
    "            for e in doc_bc5cdr[key]['texts'][n].ents:\n",
    "                if(e.label_ == 'DISEASE'):\n",
    "                    help_disease.append(e)\n",
    "                if(e.label_ == 'CHEMICAL'):\n",
    "                    help_chemicals.append(e)\n",
    "            disease_bc5cdr_texts[n] = help_disease\n",
    "            chemicals_bc5cdr_texts[n] = help_chemicals\n",
    "            \n",
    "    disease_bc5cdr[key] = {'title': disease_bc5cdr_title, 'abstract': disease_bc5cdr_abstract\n",
    "                               ,'texts': disease_bc5cdr_texts}\n",
    "    chemicals_bc5cdr[key] = {'title':chemicals_bc5cdr_title, 'abstract':chemicals_bc5cdr_abstract\n",
    "                               ,'texts':chemicals_bc5cdr_texts}\n",
    "    \n",
    "    for i in range(len(disease_bc5cdr_texts)):\n",
    "        section = 'text_' + str(i)\n",
    "        disease_bc5cdr[key][section] = disease_bc5cdr_texts[i]\n",
    "        \n",
    "    for i in range(len(chemicals_bc5cdr_texts)):\n",
    "        section = 'text_' + str(i)\n",
    "        chemicals_bc5cdr[key][section] = chemicals_bc5cdr_texts[i]\n",
    "\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "2\n",
      "GMT-5 ORGANISM B\n"
     ]
    }
   ],
   "source": [
    "key = '80993091f576dc7fdbec10552b45b4af5eec2b8b.json'\n",
    "print(species_bionlpg13cg[key]['title'])\n",
    "print(len(species_bionlpg13cg[key]['abstract'][0]))\n",
    "e = (species_bionlpg13cg[key]['texts'][3][0])\n",
    "for x in e:\n",
    "    print(x.text, x.ent_type_, x.ent_iob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW\n"
     ]
    }
   ],
   "source": [
    "for p in chemicals_bc5cdr[key]['abstract']:\n",
    "    print(p.text, p.start_char, p.end_char, p.label_)\n",
    "\n",
    "print(\"NEW\")\n",
    "\n",
    "for x in chemicals_bc5cdr[key]['abstract']:\n",
    "    for e in x:\n",
    "        print(e.text,e.ent_type_, e.ent_iob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = {}\n",
    "for key in structured_data:\n",
    "    abstracts[key] = \"\"\n",
    "    for i in range(len(structured_data[key]['abstract'])):\n",
    "        abstracts[key] = abstracts[key] + structured_data[key]['abstract'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILE WRITER ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDenotation(string, e, model, object, id):\n",
    "    string = string + \"\\n    {\"\n",
    "    string = string + \"\\n      \\\"id\\\": \\\"\" + model + \"_T\" + str(id) + \"\\\",\"\n",
    "    string = string + \"\\n      \\\"span\\\": {\"\n",
    "    string = string + \"\\n        \\\"begin\\\": \" + str(e.start_char) + \",\" \n",
    "    string = string + \"\\n        \\\"end\\\": \" + str(e.end_char)\n",
    "    string = string + \"\\n      },\"\n",
    "    string = string + \"\\n      \\\"obj\\\": \\\"\" + object + \"\\\"\"\n",
    "    string = string + \"\\n    }\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringBuilder(string, e, model, object, id): \n",
    "    string = addDenotation(string, e, model, object, id)\n",
    "    string = string + \",\"            \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDenotations(string, key, section, model):\n",
    "    id = 1\n",
    "    bool = False\n",
    "    string = string + \"\\n  \\\"denotations\\\": [\"\n",
    "    if(model == 'bionlpg13cg'):\n",
    "        for e in species_bionlpg13cg[key][section]:\n",
    "            string = stringBuilder(string, e, model, 'species', id)\n",
    "            id = id + 1    \n",
    "            bool = True\n",
    "        for e in chemicals_bionlpg13cg[key][section]:\n",
    "            string = stringBuilder(string, e, model, 'chemical', id)\n",
    "            id = id + 1\n",
    "            bool = True\n",
    "            \n",
    "    if(model == 'craft'):  \n",
    "        for e in species_craft[key][section]:\n",
    "            string = stringBuilder(string, e, model, 'species', id)\n",
    "            id = id + 1   \n",
    "            bool = True\n",
    "        for e in chemicals_craft[key][section]:\n",
    "            string = stringBuilder(string, e, model, 'chemical', id)\n",
    "            id = id + 1\n",
    "            bool = True\n",
    "            \n",
    "    if(model == 'bc5cdr'):\n",
    "        for e in disease_bc5cdr[key][section]:\n",
    "            string = stringBuilder(string, e, model, 'disease', id)\n",
    "            id = id + 1         \n",
    "            bool = True\n",
    "        for e in chemicals_bc5cdr[key][section]:\n",
    "            string = stringBuilder(string, e, model, 'chemical', id)\n",
    "            id = id + 1\n",
    "            bool = True\n",
    "   \n",
    "    if(bool):\n",
    "        string = string[:-1]\n",
    "        string = string + \"\\n  ] \\n}\"\n",
    "    else:\n",
    "        string = string + \"]\\n}\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer(key, id):\n",
    "    string = \"{ \\n  \\\"cord_uid\\\": \\\"\" + csv_data[key]['cord_uid'] + \"\\\",\"\n",
    "    string = string + \"\\n  \\\"sourcedb\\\": \\\"\" + csv_data[key]['sourcedb'] + \"\\\",\"\n",
    "    string = string + \"\\n  \\\"sourceid\\\": \\\"\" + csv_data[key]['sourceid'] + \"\\\",\"\n",
    "    string = string + \"\\n  \\\"divid\\\": \\\"\" + str(id) + \"\\\",\"\n",
    "    return string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def writerTitle(key, section, model):\n",
    "    string = \"\\n  \\\"text\\\": \\\"\" + structured_data[key][section] + \"\\\",\"\n",
    "    string = string + \"\\n  \\\"project\\\": \\\"cdlai_CORD-19\\\",\"\n",
    "    string = addDenotations(string, key, section, model)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writerAbstract(key, section, model):\n",
    "    string = \"\\n  \\\"text\\\": \\\"\" + abstracts[key] + \"\\\",\"\n",
    "    string = string + \"\\n  \\\"project\\\": \\\"cdlai_CORD-19\\\",\"\n",
    "    string = addDenotations(string, key, section, model)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writerBody(key, section, model, n):\n",
    "    string = \"\\n  \\\"text\\\": \\\"\" + structured_data[key]['texts'][n] + \"\\\",\"\n",
    "    string = string + \"\\n  \\\"project\\\": \\\"cdlai_CORD-19\\\",\"\n",
    "    string = addDenotations(string, key, section, model)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out(path, model):\n",
    "    for key in structured_data:\n",
    "        name_title = csv_data[key]['cord_uid'] + '-0-title.json'\n",
    "        with open(path + name_title, 'w') as file:\n",
    "            string = writer(key, 0) + writerTitle(key, 'title', model)\n",
    "            file.write(string)\n",
    "\n",
    "        if(len(structured_data[key]['abstract']) != 0):\n",
    "            name_abstract = csv_data[key]['cord_uid'] + '-1-abstract.json'\n",
    "            with open(path + name_abstract, 'w') as file:\n",
    "                string = writer(key, 1) + writerAbstract(key, 'abstract', model)\n",
    "                file.write(string)\n",
    "\n",
    "        for i in range(len(structured_data[key]['texts'])):\n",
    "            n = i + 2\n",
    "            name_text = csv_data[key]['cord_uid'] + '-' + str(n) + '-body_text.json'\n",
    "            section = 'text_' + str(i)\n",
    "            with open(path + name_text, 'w') as file:\n",
    "                string = writer(key, n) + writerBody(key, section, model, i)\n",
    "                file.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "out(path_out_bionlpg13cg, 'bionlpg13cg')\n",
    "out(path_out_craft, 'craft')\n",
    "out(path_out_bc5cdr, 'bc5cdr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
