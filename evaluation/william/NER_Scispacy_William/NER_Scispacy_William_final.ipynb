{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import copy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "import os\n",
    "from spacy import displacy\n",
    "from spacy.pipeline import Tagger\n",
    "import json \n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths to pipeline,models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_bionlpg13cg = '/home/william/Courses/EDAN70/models/en_ner_bionlp13cg_md-0.2.3/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.2.3'\n",
    "path_craft = '/home/william/Courses/EDAN70/models/en_ner_craft_md-0.2.3/en_ner_craft_md/en_ner_craft_md-0.2.3'\n",
    "path_bc5cdr = '/home/william/Courses/EDAN70/models/en_ner_bc5cdr_md-0.2.4/en_ner_bc5cdr_md/en_ner_bc5cdr_md-0.2.4'\n",
    "path_jnlpba = '/home/william/Courses/EDAN70/models/en_ner_jnlpba_md-0.2.4/en_ner_jnlpba_md/en_ner_jnlpba_md-0.2.4'\n",
    "\n",
    "'''path_csv = '/home/william/Courses/EDAN70/data/gold_standard/metadata_gold_standard_subset_10.csv'\n",
    "path_data = '/home/william/Courses/EDAN70/data/gold_standard/articles/'\n",
    "path_out = '/home/william/Courses/EDAN70/evaluation/gold_standard/test/'\n",
    "path_true = '/home/william/Courses/EDAN70/evaluation/gold_standard/true/'''\n",
    "\n",
    "path_csv = '/home/william/Courses/EDAN70/data/comm_use_subset_100_new/metadata_comm_use_subset_100.csv'\n",
    "path_data = '/home/william/Courses/EDAN70/data/comm_use_subset_100_new/comm_use_subset_100/'\n",
    "path_out = '/home/william/Courses/EDAN70/out/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important, Line up model with path and specify which entities to tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_models = [path_bionlpg13cg, path_craft, path_bc5cdr, path_jnlpba]\n",
    "models = ['bionlpg13cg', 'craft', 'bc5cdr', 'jnlpba']\n",
    "classes = {'bionlpg13cg': ['GENE_OR_GENE_PRODUCT', 'ORGANISM', 'SIMPLE_CHEMICAL', 'CANCER'], \n",
    "           'craft': ['GGP', 'TAXON', 'CHEBI'], \n",
    "           'bc5cdr': ['DISEASE', 'CHEMICAL'], \n",
    "           'jnlpba': ['PROTEIN']}\n",
    "classes_set = ['SPECIES', 'DISEASE', 'PROTEIN', 'CHEMICAL']\n",
    "classes_dict = {'bionlpg13cg': ['SPECIES', 'PROTEIN', 'CHEMICAL', 'DISEASE'], \n",
    "           'craft': ['PROTEIN', 'SPECIES', 'CHEMICAL'], \n",
    "           'bc5cdr': ['DISEASE', 'CHEMICAL'], \n",
    "           'jnlpba': ['PROTEIN'],\n",
    "           'unified': ['SPECIES', 'DISEASE', 'PROTEIN', 'CHEMICAL'],\n",
    "           'unified_unique': ['SPECIES', 'DISEASE', 'PROTEIN', 'CHEMICAL']}\n",
    "sections = ['title', 'abstract', 'texts']\n",
    "labels = {'ORGANISM': 'SPECIES', 'TAXON': 'SPECIES',\n",
    "          'CANCER': 'DISEASE', 'DISEASE': 'DISEASE', \n",
    "          'GENE_OR_GENE_PRODUCT': 'PROTEIN', 'GGP': 'PROTEIN', 'PROTEIN': 'PROTEIN',\n",
    "          'SIMPLE_CHEMICAL': 'CHEMICAL', 'CHEBI': 'CHEMICAL', 'CHEMICAL': 'CHEMICAL'} \n",
    "priorities = {'bionlpg13cg': {'GENE_OR_GENE_PRODUCT': 59, 'ORGANISM': 12, 'SIMPLE_CHEMICAL': 1, 'CANCER': 1}, \n",
    "              'craft': {'GGP': 64, 'TAXON': 18, 'CHEBI': 1}, \n",
    "              'bc5cdr': {'DISEASE': 33, 'CHEMICAL': 1}, \n",
    "              'jnlpba': {'PROTEIN': 56}}\n",
    "'''priorities = {'bionlpg13cg': {'GENE_OR_GENE_PRODUCT': 1, 'ORGANISM': 1, 'SIMPLE_CHEMICAL': 1, 'CANCER': 1}, \n",
    "              'craft': {'GGP': 1, 'TAXON': 1, 'CHEBI': 1}, \n",
    "              'bc5cdr': {'DISEASE': 1, 'CHEMICAL': 1}, \n",
    "              'jnlpba': {'PROTEIN': 1}}'''\n",
    "classes_translation = {'Virus_SARS-CoV' : 'SPECIES', 'Virus_SARS-CoV-2': 'SPECIES', 'Virus_family': 'SPECIES', \n",
    "                       'Virus_other': 'SPECIES', 'Disease_COVID-19': 'DISEASE', 'Disease_COVID': 'DISEASE',\n",
    "                       'Disease_other': 'DISEASE', 'Protein': 'PROTEIN'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NER models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_models = {}\n",
    "for i in range(len(paths_models)):\n",
    "    nlp_models[models[i]] = spacy.load(paths_models[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan all file names (not necessarily ordered in the same way as in file folder) and put them in an initial data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(path_data)\n",
    "json_files = [pos_json for pos_json in os.listdir(path_data) if pos_json.endswith('.json')]\n",
    "data = {}\n",
    "for filename in json_files:\n",
    "    with open(path_data + filename) as f:\n",
    "        data[filename] = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all texts in a dictionary with their cord_uid as key and separate title, abstract, and body texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = data.keys()\n",
    "structured_data = {}\n",
    "for key in keys:\n",
    "    texts = []\n",
    "    abstract = []\n",
    "    for i in range(len(data[key]['body_text'])): \n",
    "        texts.append(data[key]['body_text'][i]['text'])\n",
    "    for i in range(len(data[key]['abstract'])):\n",
    "        abstract.append(data[key]['abstract'][i]['text'])\n",
    "    structured_data[key] = {'title':data[key]['metadata']['title'], 'abstract':abstract, 'texts':texts}\n",
    "\n",
    "abstracts = {}\n",
    "for key in structured_data:\n",
    "    abstracts[key] = \"\"\n",
    "    for i in range(len(structured_data[key]['abstract'])):\n",
    "        abstracts[key] = abstracts[key] + structured_data[key]['abstract'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = {}\n",
    "with open(path_csv, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "        else:\n",
    "            key = row[1]\n",
    "            keys_helper = key.split(\"; \")\n",
    "            for i in range(len(keys_helper)):\n",
    "                key_temp = keys_helper[i] + '.json'\n",
    "                for k in keys: \n",
    "                    if(key_temp == k):\n",
    "                        key = key_temp\n",
    "                        break\n",
    "      \n",
    "            csv_data[key] = {'cord_uid': row[0], 'sourcedb': row[2], 'sourceid': row[5], 'title': row[10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all articles for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_models = {}\n",
    "for key in keys: \n",
    "    doc_models[key] = {}\n",
    "    for model in models:\n",
    "        abstract = []\n",
    "        texts = []\n",
    "        title = nlp_models[model]((structured_data[key]['title']))\n",
    "            \n",
    "        for i in range(len(structured_data[key]['abstract'])):\n",
    "            abstract.append(nlp_models[model]((structured_data[key]['abstract'][i])))\n",
    "                \n",
    "        for i in range(len(structured_data[key]['texts'])):\n",
    "            texts.append(nlp_models[model]((structured_data[key]['texts'][i])))\n",
    "            \n",
    "        doc_models[key][model] = {'title':title, 'abstract':abstract, 'texts':texts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag all entities of the selected classes that are declared at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSection(id):\n",
    "    return 'text_' + str(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "global entities \n",
    "entities = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allEntities():\n",
    "    entities = dict()\n",
    "    for k in keys:\n",
    "        entities[k] = dict()\n",
    "        for m in models: \n",
    "            entities[k][m] = dict()\n",
    "            for s in sections:\n",
    "                if(s == 'texts'):\n",
    "                    for i in range(len(doc_models[k][m][s])):\n",
    "                        section_temp = getSection(i)\n",
    "                        entities[k][m][section_temp] = dict()\n",
    "                else:\n",
    "                    entities[k][m][s] = dict()\n",
    "\n",
    "                for c in classes[m]:\n",
    "                    ents = []\n",
    "\n",
    "                    if(s == 'title'):\n",
    "                        for e in doc_models[k][m][s].ents:\n",
    "                            if(e.label_ == c):\n",
    "                                ents.append(e)\n",
    "                        entities[k][m][s][c] = ents\n",
    "\n",
    "                    elif(s == 'abstract'):\n",
    "                        ents_abstract = []\n",
    "                        for i in range(len(doc_models[k][m][s])):\n",
    "                            for e in doc_models[k][m][s][i].ents:\n",
    "                                if(e.label_ == c):\n",
    "                                    ents.append(e)\n",
    "                        entities[k][m][s][c] = ents\n",
    "\n",
    "                    elif(s == 'texts'):\n",
    "                        for i in range(len(doc_models[k][m][s])):\n",
    "                            section = getSection(i)\n",
    "                            for e in doc_models[k][m][s][i].ents:\n",
    "                                if(e.label_ == c):\n",
    "                                    ents.append(e)\n",
    "                            entities[k][m][section][c] = ents\n",
    "                            ents = []\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = allEntities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help methods used to extract the unique entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the span "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSpan(start, end):\n",
    "    span = end - start\n",
    "    return span "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns true if the first entity has a span >= to that of the second entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkLonger(e1, e2):\n",
    "    if(calculateSpan(e1.start_char, e1.end_char) >= calculateSpan(e2.start_char, e2.end_char)):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns true if the two entities are of the same class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkType(e1,e2):\n",
    "    if(labels[e1.label_] == labels[e2.label_]):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns true if the two entities have at least one character in the same space in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSame(e1, e2):   \n",
    "    if(e1.start_char <= e2.end_char and e1.end_char >= e2.start_char):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns true if the first entity has priority >= to that of the second entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkPriority(m1, m2, c1, c2):\n",
    "    if(priorities[m1][c1] >= priorities[m2][c2]):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIdentical(e1,e2,m1,m2):\n",
    "    if(m1 == m2 and e1 == e2):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printE(e):\n",
    "    print(e.text + \" - \" + str(e.start_char) + \" - \" + str(e.end_char) + \" - \" + e.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting unique entities into a separate list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runUniqueScript(k, s):\n",
    "    for m1 in models:\n",
    "        for c1 in classes[m1]:\n",
    "            n = -1\n",
    "            for e1 in entities[k][m1][s][c1]: \n",
    "                n = n + 1\n",
    "                flag = False\n",
    "                r = {}\n",
    "    \n",
    "                for m2 in models: \n",
    "                    if(flag):\n",
    "                        break\n",
    "                    for c2 in classes[m2]:  \n",
    "                        m = - 1\n",
    "                        if(flag):\n",
    "                            break\n",
    "                        for e2 in entities[k][m2][s][c2]: \n",
    "                            if(flag or checkIdentical(e1,e2,m1,m2)):\n",
    "                                break \n",
    "                    \n",
    "                            if(checkSame(e1,e2)):\n",
    "                                if(checkType(e1,e2)):\n",
    "                                    if(checkLonger(e1,e2)):\n",
    "                                        r[e2] = [k, m2, s, c2]\n",
    "                                    else:\n",
    "                                        entities[k][m1][s][c1].remove(e1)\n",
    "                                        flag = True\n",
    "                                else:\n",
    "                                    if(checkPriority(m1,m2,c1,c2)):\n",
    "                                        r[e2] = [k, m2, s, c2]\n",
    "                                    else:\n",
    "                                        entities[k][m1][s][c1].remove(e1)\n",
    "                                        flag = True\n",
    "                                   \n",
    "                if(not flag):\n",
    "                    for e in r.keys():\n",
    "                        entities[r[e][0]][r[e][1]][r[e][2]][r[e][3]].remove(e)\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueEntities():\n",
    "    for k in keys:\n",
    "        for s in sections:\n",
    "            if(s == 'texts'):\n",
    "                for i in range(len(structured_data[k]['texts'])):\n",
    "                    section = getSection(i)\n",
    "                    runUniqueScript(k,section)\n",
    "            else:\n",
    "                runUniqueScript(k,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILE WRITER, Uses Pubannotation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDenotation(string, e, model, label, id):\n",
    "    string = string + \"\\n    {\"\n",
    "    string = string + \"\\n      \\\"id\\\": \\\"\" + model + \"_T\" + str(id) + \"\\\",\"\n",
    "    #string = string + \"\\n      \\\"id\\\": \\\"\" + label + \"\\\",\"\n",
    "    string = string + \"\\n      \\\"span\\\": {\"\n",
    "    string = string + \"\\n        \\\"begin\\\": \" + str(e.start_char) + \",\" \n",
    "    string = string + \"\\n        \\\"end\\\": \" + str(e.end_char)\n",
    "    string = string + \"\\n      },\"\n",
    "    #string = string + \"\\n      \\\"obj\\\": \\\"\" + model + \"_T\" + str(id) + \"\\\"\"\n",
    "    string = string + \"\\n      \\\"obj\\\": \\\"\" + label + \"\\\"\"\n",
    "    string = string + \"\\n    },\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDenotations(string, key, model, section):\n",
    "    id = 1\n",
    "    bool = False\n",
    "    string = string + \"\\n  \\\"denotations\\\": [\"\n",
    "        \n",
    "    for m in model: \n",
    "        for c in classes[m]:\n",
    "            for e in entities[key][m][section][c]:\n",
    "                string = addDenotation(string, e, m, labels[c], id)\n",
    "                bool = True\n",
    "                id = id + 1\n",
    "   \n",
    "    if(bool):\n",
    "        string = string[:-1]\n",
    "        string = string + \"\\n  ] \\n}\"\n",
    "    else:\n",
    "        string = string + \"]\\n}\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer(key, id):\n",
    "    #string = \"{ \\n  \\\"cord_uid\\\": \\\"\" + key + \"\\\",\" --- When CSV is missing\n",
    "    string = \"{ \\n  \\\"cord_uid\\\": \\\"\" + csv_data[key]['cord_uid'] + \"\\\",\"\n",
    "    string = string + \"\\n  \\\"sourcedb\\\": \\\"\" + csv_data[key]['sourcedb'] + \"\\\",\"\n",
    "    string = string + \"\\n  \\\"sourceid\\\": \\\"\" + csv_data[key]['sourceid'] + \"\\\",\"\n",
    "    string + \"\\n  \\\"divid\\\": \\\"\" + str(id) + \"\\\",\"\n",
    "    return string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def writerTitle(key, model, section):\n",
    "    string = \"\\n\".ljust(4) + \"\\\"text\\\": \\\"\" + structured_data[key][section].replace('\"', '\\\\\"') + \"\\\",\"\n",
    "    string = string + \"\\n  \\\"project\\\": \\\"cdlai_CORD-19\\\",\"\n",
    "    string = addDenotations(string, key, model, section)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writerAbstract(key, model, section):\n",
    "    string = \"\\n\".ljust(4) + \"\\\"text\\\": \\\"\" + abstracts[key].replace('\"', '\\\\\"') + \"\\\",\"\n",
    "    #string = \"\\n    \\\"text\\\": \" + abstracts[key] + \",\"\n",
    "    string = string + \"\\n  \\\"project\\\": \\\"cdlai_CORD-19\\\",\"\n",
    "    string = addDenotations(string, key, model, section)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writerBody(key, model, section, n):\n",
    "    string = \"\\n\".ljust(4) + \"\\\"text\\\": \\\"\" + structured_data[key]['texts'][n].replace('\"', '\\\\\"') + \"\\\",\"\n",
    "    string = string + \"\\n  \\\"project\\\": \\\"cdlai_CORD-19\\\",\"\n",
    "    string = addDenotations(string, key, model, section)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out(path, model):\n",
    "    for key in keys:\n",
    "        name_title = csv_data[key]['cord_uid'] + '-0-title.json'\n",
    "        #name_title = key + '-0-title.json' --- When CSV is missing\n",
    "        with open(path + name_title, 'w') as file:\n",
    "            string = writer(key, 0) + writerTitle(key, model, sections[0])\n",
    "            file.write(string)\n",
    "        \n",
    "        if(len(structured_data[key]['abstract']) != 0):\n",
    "            name_abstract = csv_data[key]['cord_uid'] + '-1-abstract.json'\n",
    "            #name_abstract = key + '-1-abstract.json' --- When CSV is missing\n",
    "            with open(path + name_abstract, 'w') as file:\n",
    "                string = writer(key, 1) + writerAbstract(key, model, sections[1])\n",
    "                file.write(string)\n",
    "\n",
    "        for i in range(len(structured_data[key]['texts'])):\n",
    "            n = i + 2\n",
    "            name_text = csv_data[key]['cord_uid'] + '-' + str(n) + '-body_text.json'\n",
    "            #name_text = key + '-' + str(n) + '-body_text.json' --- When CSV is missing\n",
    "            section = getSection(i)\n",
    "            with open(path + name_text, 'w') as file:\n",
    "                string = writer(key, n) + writerBody(key, model, section, i)\n",
    "                file.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = allEntities()\n",
    "for model in models: \n",
    "    path = path_out + model + '/'\n",
    "    #print(path)\n",
    "    out(path, [model])\n",
    "    \n",
    "path_unified = path_out + 'unified/'\n",
    "out(path_unified, models)\n",
    "\n",
    "uniqueEntities()\n",
    "path_unified_unique = path_out + 'unified_unique/'\n",
    "out(path_unified_unique, models)\n",
    "entities = allEntities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC7003341.json\n",
      "PMC7054940.json\n",
      "PMC6988272.json\n",
      "PMC7077245.json\n",
      "PMC7110798.json\n",
      "PMC7033720.json\n",
      "PMC7094943.json\n",
      "31996494.json\n",
      "32013309.json\n",
      "PMC7159299.json\n"
     ]
    }
   ],
   "source": [
    "for e in entities:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_files = os.listdir(path_data)\n",
    "json_true_files = [pos_json for pos_json in os.listdir(path_true) if pos_json.endswith('.json')]\n",
    "d = {}\n",
    "for filename in json_true_files:\n",
    "    with open(path_true + filename) as f:\n",
    "        d[filename] = json.load(f)\n",
    "\n",
    "for k in d.keys(): \n",
    "    for e in d[k]['denotations']:\n",
    "        if((e['id']) in classes_translation):\n",
    "            e['id'] = classes_translation[e['id']]\n",
    "\n",
    "for k in d.keys():\n",
    "    with open(path_true + k, 'w') as file:\n",
    "        json.dump(d[k], file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator from Dict. group: Annie and Sofie, thanks! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PubAnnotation evaluator for COVID-19\n",
    "\n",
    "Authors:\n",
    "    Annie Tallind, Lund University, Faculty of Engineering\n",
    "    Kaggle ID: atllnd\n",
    "    Github ID: annietllnd\n",
    "\n",
    "    Sofi Flink, Lund University, Faculty of Engineering\n",
    "    Kaggle ID: sofiflinck\n",
    "    Github ID: obakanue\n",
    "\n",
    "    TODO:\n",
    "     - Should we check correct word class?\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "# true_positives = some_function() # number of true positives\n",
    "# false_positives = some_other_function() # number of false positives\n",
    "# true_negatives = other_function() # number of true negatives\n",
    "# false_negatives = last_one() # number of false negatives\n",
    "# for any is_checked = False in tagger_output_dict -> False positives\n",
    "# for any is_checked = False in true_output_dict -> False negatives\n",
    "\n",
    "\n",
    "def print_progress(nbr_pubannotations_evaluated, total_pubannotations):\n",
    "    \"\"\"\n",
    "    Prints estimated progress based on number of total articles and number of articles processed.\n",
    "    \"\"\"\n",
    "    print_progress_bar(nbr_pubannotations_evaluated,\n",
    "                       total_pubannotations,\n",
    "                       prefix='EVALUATION PROGRESS\\t',\n",
    "                       suffix='COMPLETE')\n",
    "\n",
    "\n",
    "def print_progress_bar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='█'):\n",
    "    \"\"\"\n",
    "    Author: StackOverflow\n",
    "            User Greenstick\n",
    "            Question 30740258\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filled_length = int(length * iteration // total)\n",
    "    bar = fill * filled_length + '-' * (length - filled_length)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end='', flush=True)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total:\n",
    "        print()\n",
    "\n",
    "\n",
    "class PubannotationEvaluator:\n",
    "    \"\"\"\n",
    "    PubAnnotationEvaluator evaluates output compared to a true output, the arguments are the directory paths to the\n",
    "    location of the outputs and a set containing what word classes/dictionaries to be evaluated.\n",
    "    \"\"\"\n",
    "    def __init__(self, tagger_output_dir_path, true_output_dir_path, word_classes_set):\n",
    "        self.tagger_output_dicts = dict()\n",
    "        self.true_output_dicts = dict()\n",
    "        self.word_classes_result_dict = dict()\n",
    "        self.recall_values = list()\n",
    "        self.precision_values = list()\n",
    "\n",
    "        self.word_classes_set = word_classes_set\n",
    "        self.true_positives = self.true_negatives = self.false_positives = self.false_negatives = \\\n",
    "            self.nbr_true_entities = self.recall_value = self.precision_value = self.total_true_positives = \\\n",
    "            self.total_false_positives = self.total_false_negatives = self.iteration_nbr = 0\n",
    "\n",
    "        self.__generate_result_dict(self.word_classes_set)\n",
    "\n",
    "        self.__load_output(tagger_output_dir_path, 1)\n",
    "        self.__load_output(true_output_dir_path, 0)\n",
    "\n",
    "        self.processes_total = len(self.tagger_output_dicts) + len(self.word_classes_result_dict)\n",
    "\n",
    "    def __generate_result_dict(self, word_classes_set):\n",
    "        \"\"\"\n",
    "        Initializes a dictionary containing all the results for respective word class.\n",
    "        \"\"\"\n",
    "        for word_class in word_classes_set:\n",
    "            self.word_classes_result_dict[word_class] = {'total': {'amount': 0,\n",
    "                                                                   'entities': list()\n",
    "                                                                   },\n",
    "                                                         'true_positives': {'amount': 0,\n",
    "                                                                            'entities': list()\n",
    "                                                                            },\n",
    "                                                         'false_positives': {'amount': 0,\n",
    "                                                                             'entities': list()\n",
    "                                                                             },\n",
    "                                                         'false_negatives': {'amount': 0,\n",
    "                                                                             'entities': list()\n",
    "                                                                             }\n",
    "                                                         }\n",
    "\n",
    "    def __load_output(self, dir_output_path, is_tagger_output):\n",
    "        \"\"\"\n",
    "        Loads output files from a given directory in to corresponding dictionary. Second argument indicates if it is\n",
    "        the true output or the output to be evaluated.\n",
    "        \"\"\"\n",
    "        output_paths = os.listdir(dir_output_path)\n",
    "        for pubannotation_file_name in output_paths:\n",
    "            if pubannotation_file_name == '.DS_Store':  # For MacOS users skip .DS_Store-file\n",
    "                continue                                # generated.\n",
    "            full_path = dir_output_path + pubannotation_file_name\n",
    "            with open(full_path) as pubannotation_obj:\n",
    "                pubannotation_dict = json.loads(pubannotation_obj.read())\n",
    "                pubannotation_dict.update({'is_checked': False})\n",
    "                if is_tagger_output:\n",
    "                    self.tagger_output_dicts.update({pubannotation_file_name: pubannotation_dict})\n",
    "                else:\n",
    "                    self.true_output_dicts.update({pubannotation_file_name: pubannotation_dict})\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluates outputs compared to true outputs.\n",
    "        \"\"\"\n",
    "        self.__compare_outputs()\n",
    "        self.__evaluate_word_class()\n",
    "        self.__calculate_micro()\n",
    "        self.__print_result('MICRO')\n",
    "        self.__calculate_macro()\n",
    "        self.__print_result('MACRO')\n",
    "        self.__calculate_harmonic_mean()\n",
    "\n",
    "    def __compare_outputs(self):\n",
    "        \"\"\"\n",
    "        Iterates through all outputs to be compared to through output and compare output denotations.\n",
    "        \"\"\"\n",
    "        for cord_uid in self.tagger_output_dicts:\n",
    "            print_progress(self.iteration_nbr, self.processes_total)\n",
    "            tagger_pubannotation = self.tagger_output_dicts[cord_uid]\n",
    "            if cord_uid in self.true_output_dicts:\n",
    "                true_pubannotation = self.true_output_dicts[cord_uid]\n",
    "                text = true_pubannotation['text']\n",
    "                word_classes_list = [denotations_list_element['id'] for denotations_list_element in\n",
    "                                     true_pubannotation['denotations']]\n",
    "                self.__compare_output(tagger_pubannotation,\n",
    "                                      true_pubannotation,\n",
    "                                      cord_uid,\n",
    "                                      word_classes_list,\n",
    "                                      text)\n",
    "                self.iteration_nbr += 1\n",
    "        print_progress(self.iteration_nbr, self.processes_total)\n",
    "\n",
    "    def __compare_output(self, tagger_pubannotation, true_pubannotation, cord_uid, word_classes_list, text):\n",
    "        \"\"\"\n",
    "        Compares denotations with true denotations, false negatives field are incremented if there is a an existing\n",
    "        match in true denotations that does not exist in denotations to be compared, and only for the word classes\n",
    "        existing in the list argument. When a denotation is checked the field 'is_checked' is set to True which\n",
    "        helps to calculate false positives and false negatives. If two denotations are matching in span true positives\n",
    "        filed will be incremented in the result dictionary.\n",
    "        \"\"\"\n",
    "        tagger_denotations = tagger_pubannotation['denotations']\n",
    "        for tagger_denotation in tagger_denotations:\n",
    "            tagger_denotation['is_checked'] = False\n",
    "        true_denotations = true_pubannotation['denotations']\n",
    "        for true_denotation in true_denotations:\n",
    "            true_denotation['is_checked'] = False\n",
    "\n",
    "        for tagger_denotation in tagger_denotations:\n",
    "            i = 0\n",
    "            for true_denotation in true_denotations:\n",
    "                tagger_denotation_span = (tagger_denotation['span']['begin'], tagger_denotation['span']['end'])\n",
    "                true_denotation_span = (true_denotation['span']['begin'], true_denotation['span']['end'])\n",
    "                if tagger_denotation_span == true_denotation_span:\n",
    "                    # Might want to change to a safer implementation where we don't depend on an ordered\n",
    "                    # word_classes_list. TODO\n",
    "\n",
    "                    if word_classes_list[i] in self.word_classes_set and tagger_denotation['id'] == true_denotation['id']:\n",
    "                        word_class = word_classes_list[i]\n",
    "                        self.word_classes_result_dict[word_class]['true_positives']['amount'] += 1\n",
    "                        self.word_classes_result_dict[word_class]['total']['amount'] += 1\n",
    "                        self.word_classes_result_dict[word_class]['true_positives']['entities'].append(\n",
    "                            f'id: {word_class}, '\n",
    "                            f'entity: {text[tagger_denotation_span[0]:tagger_denotation_span[1] + 1]}, '\n",
    "                            f'span: {tagger_denotation_span}')\n",
    "                        self.word_classes_result_dict[word_class]['total']['entities'].append(\n",
    "                            f'id: {word_class}, '\n",
    "                            f'entity: {text[tagger_denotation_span[0]:tagger_denotation_span[1] + 1]}, '\n",
    "                            f'span: {tagger_denotation_span}')\n",
    "                        true_denotation.update({'is_checked': True})\n",
    "                        tagger_denotation.update({'is_checked': True})\n",
    "                        break\n",
    "                i += 1\n",
    "\n",
    "        for tagger_denotation in tagger_denotations:\n",
    "            if not tagger_denotation['is_checked']:\n",
    "                word_class = tagger_denotation['id']\n",
    "                if word_class in self.word_classes_set:\n",
    "                    tagger_denotation_span = (tagger_denotation['span']['begin'], tagger_denotation['span']['end'])\n",
    "                    self.word_classes_result_dict[word_class]['false_positives']['amount'] += 1\n",
    "                    self.word_classes_result_dict[word_class]['false_positives']['entities'].append(\n",
    "                        f'id: {word_class}, '\n",
    "                        f'entity: {text[tagger_denotation_span[0]:tagger_denotation_span[1] + 1]}, '\n",
    "                        f'span: {tagger_denotation_span}')\n",
    "                tagger_denotation.update({'is_checked': True})\n",
    "        for true_denotation in true_denotations:\n",
    "            word_class = true_denotation['id']\n",
    "            if word_class in self.word_classes_set:\n",
    "                if not true_denotation['is_checked']:\n",
    "                    true_denotation_span = (true_denotation['span']['begin'], true_denotation['span']['end'])\n",
    "                    self.word_classes_result_dict[word_class]['false_negatives']['amount'] += 1\n",
    "                    self.word_classes_result_dict[word_class]['total']['amount'] += 1\n",
    "                    self.word_classes_result_dict[word_class]['false_negatives']['entities'].append(\n",
    "                        f'id: {word_class}, '\n",
    "                        f'entity: {text[true_denotation_span[0]:true_denotation_span[1] + 1]}, '\n",
    "                        f'span: {true_denotation_span}')\n",
    "                true_denotation.update({'is_checked': True})\n",
    "\n",
    "\n",
    "\n",
    "    def __evaluate_word_class(self):\n",
    "        \"\"\"\n",
    "        Evaluates results for each word class.\n",
    "        \"\"\"\n",
    "        for word_class in self.word_classes_result_dict:\n",
    "            print_progress(self.iteration_nbr, self.processes_total)\n",
    "            self.__precision(word_class)\n",
    "            self.__recall(word_class)\n",
    "            self.__print_result(word_class)\n",
    "            self.iteration_nbr += 1\n",
    "        print_progress(self.iteration_nbr, self.processes_total)\n",
    "\n",
    "    def __precision(self, word_class):\n",
    "        \"\"\"\n",
    "        Calculates precision figure.\n",
    "        \"\"\"\n",
    "        true_positives = self.word_classes_result_dict[word_class]['true_positives']['amount']\n",
    "        false_positives = self.word_classes_result_dict[word_class]['false_positives']['amount']\n",
    "        self.total_true_positives += true_positives\n",
    "        self.total_false_positives += false_positives\n",
    "        sum_value = true_positives + false_positives\n",
    "        if sum_value:\n",
    "            self.precision_value = true_positives / sum_value\n",
    "            self.precision_values.append(self.precision_value)\n",
    "        else:\n",
    "            print('########### WARNING ###########')\n",
    "            print(f'{word_class} found no match, the precision result can be misleading')\n",
    "            print(\"########### WARNING ###########\")\n",
    "            self.precision_value = 0\n",
    "\n",
    "    def __recall(self, word_class):\n",
    "        \"\"\"\n",
    "        Calculates recall figure.\n",
    "        \"\"\"\n",
    "        true_positives = self.word_classes_result_dict[word_class]['true_positives']['amount']\n",
    "        false_negatives = self.word_classes_result_dict[word_class]['false_negatives']['amount']\n",
    "        self.total_false_negatives += false_negatives\n",
    "        sum_value = true_positives + false_negatives\n",
    "        if sum_value:\n",
    "            self.recall_value = true_positives / sum_value\n",
    "            self.recall_values.append(self.recall_value)\n",
    "        else: \n",
    "            print('########### WARNING ###########')\n",
    "            print(f\"'{word_class}' found no match, the recall result can be misleading\")\n",
    "            print('########### WARNING ###########')\n",
    "            print('\\n')\n",
    "            self.precision_values.append(0)\n",
    "            self.recall_value = 0\n",
    "\n",
    "    def __calculate_micro(self):\n",
    "        \"\"\"\n",
    "        Calculates micro figure.\n",
    "        \"\"\"\n",
    "        sum_value = self.total_true_positives + self.total_false_positives\n",
    "        if sum_value:\n",
    "            self.precision_value = self.total_true_positives / (self.total_true_positives + self.total_false_positives)\n",
    "            self.recall_value = self.total_true_positives / (self.total_true_positives + self.total_false_negatives)\n",
    "        else:\n",
    "            self.precision_value = 0\n",
    "            self.recall_value = 0\n",
    "\n",
    "    def __calculate_macro(self):\n",
    "        \"\"\"\n",
    "        Calculates macro figure.\n",
    "        \"\"\"\n",
    "        self.precision_value = 0\n",
    "        self.recall_value = 0\n",
    "        for precision_value in self.precision_values:\n",
    "            self.precision_value += precision_value\n",
    "        for recall_value in self.recall_values:\n",
    "            self.recall_value += recall_value\n",
    "        if self.precision_value:\n",
    "            self.precision_value /= len(self.precision_values)\n",
    "            self.recall_value /= len(self.recall_values)\n",
    "\n",
    "    def __calculate_harmonic_mean(self):\n",
    "        \"\"\"\n",
    "        Calculates harmonic mean/F1 score figure.\n",
    "        \"\"\"\n",
    "        sum_value = self.precision_value + self.recall_value\n",
    "        harmonic_mean = 0\n",
    "        if sum_value:\n",
    "            harmonic_mean = (2*self.precision_value*self.recall_value) / sum_value\n",
    "        print(f'#########\\tHARMONIC MEAN RESULT:\\t###########')\n",
    "        print(f'Harmonic mean:\\t{harmonic_mean * 100:.0f}%')\n",
    "\n",
    "    def __print_result(self, word_class):\n",
    "        \"\"\"\n",
    "        Prints result for a given section/word class.\n",
    "        \"\"\"\n",
    "        print(f'\\n\\n#########\\t{word_class.upper()} PRECISION & RECALL RESULT:\\t###########')\n",
    "        print('\\n')\n",
    "        print(f'Precision:\\t{self.precision_value * 100:.0f}%')\n",
    "        print(f'Recall:\\t\\t{self.recall_value * 100:.0f}%')\n",
    "        print('\\n')\n",
    "\n",
    "    def get_total_entities(self, word_class):\n",
    "        \"\"\"\n",
    "        Returns a list of total entities for a word class.\n",
    "        \"\"\"\n",
    "        return self.word_classes_result_dict[word_class]['total']['entities']\n",
    "\n",
    "    def get_true_positive_entities(self, word_class):\n",
    "        \"\"\"\n",
    "        Returns a list of entities marked as true positives for a word class.\n",
    "        \"\"\"\n",
    "        return self.word_classes_result_dict[word_class]['true_positives']['entities']\n",
    "\n",
    "    def get_false_positive_entities(self, word_class):\n",
    "        \"\"\"\n",
    "        Returns a list of entities marked as false positives for a word class.\n",
    "        \"\"\"\n",
    "        return self.word_classes_result_dict[word_class]['false_positives']['entities']\n",
    "\n",
    "    def get_false_negative_entities(self, word_class):\n",
    "        \"\"\"\n",
    "        Returns a list of entities marked as false negatives for a word class.\n",
    "        \"\"\"\n",
    "        return self.word_classes_result_dict[word_class]['false_negatives']['entities']\n",
    "\n",
    "    def get_total(self, word_class):\n",
    "        \"\"\"\n",
    "        Returns number of entities associated with a word class.\n",
    "        \"\"\"\n",
    "        return self.word_classes_result_dict[word_class]['total']['amount']\n",
    "\n",
    "    def get_true_positives(self, word_class):\n",
    "        \"\"\"\n",
    "        Returns number of true positives associated with a word class.\n",
    "        \"\"\"\n",
    "        return self.word_classes_result_dict[word_class]['true_positives']['amount']\n",
    "\n",
    "    def get_false_positives(self, word_class):\n",
    "        \"\"\"\n",
    "        Returns number of false positives associated with a word class.\n",
    "        \"\"\"\n",
    "        return self.word_classes_result_dict[word_class]['false_positives']['amount']\n",
    "\n",
    "    def get_false_negatives(self, word_class):\n",
    "        \"\"\"\n",
    "        Returns number of false negatives associated with a word class.\n",
    "        \"\"\"\n",
    "        return self.word_classes_result_dict[word_class]['false_negatives']['amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION PROGRESS\t |██████████████████████████████████████████████████████████████████████████████████████--------------| 87.0% COMPLETE\n",
      "\n",
      "#########\tDISEASE PRECISION & RECALL RESULT:\t###########\n",
      "\n",
      "\n",
      "Precision:\t24%\n",
      "Recall:\t\t50%\n",
      "\n",
      "\n",
      "EVALUATION PROGRESS\t |███████████████████████████████████████████████████████████████████████████████████████████---------| 91.3% COMPLETE\n",
      "\n",
      "#########\tSPECIES PRECISION & RECALL RESULT:\t###########\n",
      "\n",
      "\n",
      "Precision:\t13%\n",
      "Recall:\t\t16%\n",
      "\n",
      "\n",
      "EVALUATION PROGRESS\t |███████████████████████████████████████████████████████████████████████████████████████████████-----| 95.7% COMPLETE\n",
      "\n",
      "#########\tPROTEIN PRECISION & RECALL RESULT:\t###########\n",
      "\n",
      "\n",
      "Precision:\t29%\n",
      "Recall:\t\t91%\n",
      "\n",
      "\n",
      "EVALUATION PROGRESS\t |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% COMPLETE\n",
      "\n",
      "\n",
      "#########\tMICRO PRECISION & RECALL RESULT:\t###########\n",
      "\n",
      "\n",
      "Precision:\t20%\n",
      "Recall:\t\t33%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#########\tMACRO PRECISION & RECALL RESULT:\t###########\n",
      "\n",
      "\n",
      "Precision:\t22%\n",
      "Recall:\t\t52%\n",
      "\n",
      "\n",
      "#########\tHARMONIC MEAN RESULT:\t###########\n",
      "Harmonic mean:\t31%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(\"-------------- \\n\\nVALUES -----------------------\" )\\nprint(\\'False positives: \\' + str(evaluator.get_false_positives(\\'SPECIES\\')) + \\'\\n\\')\\nprint(\\'False negatives: \\' + str(evaluator.get_false_negatives(\\'SPECIES\\')) + \\'\\n\\')\\nprint(\\'True positives: \\' + str(evaluator.get_true_positives(\\'SPECIES\\')) + \\'\\n\\')\\nprint(\\'Total: \\' + str(evaluator.get_total(\\'SPECIES\\')) + \\'\\n\\')\\nprint(\\'False positive entities: \\' + str(evaluator.get_false_positive_entities(\\'SPECIES\\')) + \\'\\n\\')\\nprint(\\'False negative entities: \\' + str(evaluator.get_false_negative_entities(\\'SPECIES\\')) + \\'\\n\\')\\nprint(\\'True positive entities: \\' + str(evaluator.get_true_positive_entities(\\'SPECIES\\')) + \\'\\n\\')\\nprint(\\'Total entities: \\' + str(evaluator.get_total_entities(\\'SPECIES\\')) + \\'\\n\\')'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = 'unified_unique'\n",
    "tagger_output_dir_path = path_out + model + '/'\n",
    "true_output_dir_path = path_true\n",
    "evaluator = PubannotationEvaluator(tagger_output_dir_path, true_output_dir_path, ['DISEASE', 'SPECIES', 'PROTEIN']) #classes_dict[model])\n",
    "evaluator.evaluate()\n",
    "\n",
    "\n",
    "'''model = 'bionlpg13cg'\n",
    "tagger_output_dir_path = path_out + model + '/'\n",
    "true_output_dir_path = path_true\n",
    "evaluator = PubannotationEvaluator(tagger_output_dir_path, true_output_dir_path, classes_set)\n",
    "evaluator.evaluate()'''\n",
    "\n",
    "\n",
    "'''print(\"-------------- \\n\\nVALUES -----------------------\" )\n",
    "print('False positives: ' + str(evaluator.get_false_positives('SPECIES')) + '\\n')\n",
    "print('False negatives: ' + str(evaluator.get_false_negatives('SPECIES')) + '\\n')\n",
    "print('True positives: ' + str(evaluator.get_true_positives('SPECIES')) + '\\n')\n",
    "print('Total: ' + str(evaluator.get_total('SPECIES')) + '\\n')\n",
    "print('False positive entities: ' + str(evaluator.get_false_positive_entities('SPECIES')) + '\\n')\n",
    "print('False negative entities: ' + str(evaluator.get_false_negative_entities('SPECIES')) + '\\n')\n",
    "print('True positive entities: ' + str(evaluator.get_true_positive_entities('SPECIES')) + '\\n')\n",
    "print('Total entities: ' + str(evaluator.get_total_entities('SPECIES')) + '\\n')'''\n",
    "\n",
    "#print(evaluator.get_true_positives('SPECIES'))\n",
    "#for i in range(len(evaluator.get_total_entities('SPECIES'))):\n",
    "  #  print(evaluator.get_total_entities('SPECIES')[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
