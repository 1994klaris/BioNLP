{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docria in c:\\users\\elsar\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: msgpack-python>=0.5.6 in c:\\users\\elsar\\anaconda3\\lib\\site-packages (from docria) (0.5.6)\n",
      "Collecting regex\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/2f/f586e982712ffee5681ca149d54480dbb04ff533e9e4638c5e28ae76bdb5/regex-2019.08.19-cp37-none-win_amd64.whl (325kB)\n",
      "Installing collected packages: regex\n",
      "Successfully installed regex-2019.8.19\n"
     ]
    }
   ],
   "source": [
    "#install docria\n",
    "!pip install docria \n",
    "#install regular expressions\n",
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries \n",
    "from docria import Document, DataTypes as T, NodeSpan, set_large_screen, MsgpackCodec\n",
    "from docria.storage import MsgpackDocumentIO, MsgpackDocumentReader, MsgpackDocumentWriter\n",
    "from docria.codec import MsgpackDocument #MsgpackDocument must be imported from docria.codec\n",
    "from lxml import etree #approx the same thing but better than import xml.etree.ElementTree as ET\n",
    "import regex as re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'sh'\n"
     ]
    }
   ],
   "source": [
    "#IPython magic commands are prefixed by \"%\" -aim: succinctly solve various common problems in standard data analysis.\n",
    "#cell magics are denoted by a double %% prefix\n",
    "\n",
    "%%sh #invoke system shell (shell = user interface)\n",
    "zcat pubmed_mini/pubmed19n0080.xml.gz | head -n 100 \n",
    "#zcat prints whole document, pipe head - n 100 prints first 100 lines of document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error reading file 'pubmed_mini/pubmed19n0080.xml.gz': failed to load external entity \"pubmed_mini/pubmed19n0080.xml.gz\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-24c1bb3bf284>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpubmed0080\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pubmed_mini/pubmed19n0080.xml.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree.parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._parseDocument\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._parseDocumentFromURL\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._parseDocFromFile\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._BaseParser._parseDocFromFile\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._ParserContext._handleParseResultDoc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._handleParseResult\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._raiseParseError\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Error reading file 'pubmed_mini/pubmed19n0080.xml.gz': failed to load external entity \"pubmed_mini/pubmed19n0080.xml.gz\""
     ]
    }
   ],
   "source": [
    "pubmed0080 = etree.parse(\"pubmed_mini/pubmed19n0080.xml.gz\") #parses document, stored in memory (doesn't print it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pubmed0080.iterfind(\"PubmedArticle\") #finds PubmedArticle in document, stores it as variable articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = next(articles) #finds next I assume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(etree.tounicode(article)) \n",
    "# \"Serialize an element to the Python unicode representation of its XML tree.\" - https://lxml.de/api/lxml.etree-module.html\n",
    "#\"Serialization is the process of converting an object into a form that can be readily transported. \n",
    "# For example, you can serialize an object and transport it over the Internet using HTTP between a client and a server. \n",
    "# On the other end, deserialization reconstructs the object from the stream.\" - https://lxml.de/api/lxml.etree-module.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.find(\".//Article/ArticleTitle\").text #finds first ArticleTitle I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.find(\".//PMID\").text #finds first PubMed ID I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = article.find(\".//Abstract\") #finds first abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(etree.tounicode(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descedants = list(abstract.iterdescendants())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descedants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_text = \"\".join(node.text for node in descedants if node.text is not None).strip()\n",
    "print(abstract_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the regex at https://www.regex101.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex: \\symbol finds that symbol, otherwise it is interpreted as a command\n",
    "#\"?\" makes the preceding token in the regular expression optional. colou?r matches colour or color.\n",
    "#\">\" \n",
    "\n",
    "last = 0\n",
    "for m in re.finditer(r\"(?>[\\.\\?\\!])(?:\\s*(?=\\p{Lu})|$)\", abstract_text): #regex finds new sentence (doesn't find end of last sentence)\n",
    "    print(\"SENT:\", abstract_text[last:m.start()+1])\n",
    "    last = m.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_large_screen() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.props[\"pmid\"] = article.find(\".//PMID\").text #sets PubMed ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.maintext = abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.add_layer(\"token\", text=T.span(), partOfSpeech=T.string, namedEntity=T.string(\"O\"), indx=T.int32) \n",
    "doc.add_layer(\"sentence\", text=T.span(), tokens=T.nodespan(\"token\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_layer = doc[\"sentence\"]\n",
    "token_layer = doc[\"token\"]\n",
    "last = 0\n",
    "for m in re.finditer(r\"(?>[\\.\\?\\!])(?:\\s*(?=\\p{Lu})|$)\", abstract_text): #for each new sentence in abstract?\n",
    "    tokens = []\n",
    "    text = doc.maintext[last:m.start()+1]\n",
    "    offset = last\n",
    "    for tok_i, tok_m in enumerate(re.finditer(r\"\\p{L}+|\\p{N}+(\\.\\p{N}+)?|[\\-\\/%():,\\.;+&#=!?@_<>]\", str(text))):\n",
    "        tokens.append(token_layer.add(indx=tok_i+1, text=doc.maintext[tok_m.start()+offset:tok_m.end()+offset]))\n",
    "    \n",
    "    if len(tokens) > 0:\n",
    "        sentence = sentence_layer.add(text=text)\n",
    "        sentence[\"tokens\"] = NodeSpan(tokens[0], tokens[-1])\n",
    "\n",
    "    last = m.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[\"sentence\"][\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0afc81a348f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"token\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"token\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"of\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "doc[\"token\"][doc[\"token\"][\"text\"] == \"of\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docria.algorithm import group_by_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_span?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The statement below works because, 'text' is the default name of a span, if it is not called 'text', either group_span_field or layer_span_field = {'token': name} must be set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of_tokens = doc[\"token\"][doc[\"token\"][\"text\"] == \"of\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_span(group_nodes=doc[\"sentence\"], layer_nodes={\"token\": of_tokens})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale it up, processing many documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(doc):\n",
    "    token_layer = doc.add_layer(\"token\", text=T.span(), partOfSpeech=T.string, namedEntity=T.string(\"O\"), indx=T.int32)\n",
    "    sentence_layer = doc.add_layer(\"sentence\", text=T.span(), tokens=T.nodespan(\"token\"))\n",
    "    \n",
    "    pubmed_abstract = doc.texts[\"main\"]\n",
    "    sentence_layer = doc[\"sentence\"]\n",
    "    token_layer = doc[\"token\"]\n",
    "    \n",
    "    last = 0\n",
    "    for m in re.finditer(r\"(?>[\\.\\?\\!])(?:\\s*(?=\\p{Lu})|$)\", str(pubmed_abstract)):\n",
    "        tokens = []\n",
    "        text = pubmed_abstract[last:m.start()+1]\n",
    "        \n",
    "        offset = last\n",
    "        for tok_i, tok_m in enumerate(re.finditer(r\"\\p{L}+|\\p{N}+(\\.\\p{N}+)?|[\\-\\/%():,\\.;+&#=!?@_<>]\", str(text))):\n",
    "            tokens.append(token_layer.add(indx=tok_i+1, text=doc.maintext[tok_m.start()+offset:tok_m.end()+offset]))\n",
    "\n",
    "        if len(tokens) > 0:\n",
    "            sentence = sentence_layer.add(text=text)\n",
    "            sentence[\"tokens\"] = NodeSpan(tokens[0], tokens[-1])\n",
    "\n",
    "        last = m.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pubmed(articles):\n",
    "    for article in articles:\n",
    "        title = article.find(\".//Article/ArticleTitle\").text\n",
    "        pmid = article.find(\".//PMID\").text\n",
    "        abstract = article.find(\".//Abstract\")\n",
    "        if abstract is None: #because all articles don't have abstract\n",
    "            continue\n",
    "        \n",
    "        abstract_text = \"\".join(node.text for node in abstract.iterdescendants() if node.text is not None).strip()\n",
    "        \n",
    "        assert pmid is not None\n",
    "        assert title is not None\n",
    "        assert abstract is not None\n",
    "    \n",
    "        doc = Document()\n",
    "        doc.props[\"pmid\"] = pmid\n",
    "        doc.props[\"title\"] = title\n",
    "        doc.maintext = abstract_text\n",
    "        \n",
    "        segment(doc)\n",
    "        yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(inputfile, outputfile):\n",
    "    pubmedxml = etree.parse(inputfile)\n",
    "    with open(outputfile, \"wb\") as fout, MsgpackDocumentWriter(fout) as writer:\n",
    "        for doc in process_pubmed(pubmedxml.iterfind(\"PubmedArticle\")):\n",
    "            writer.write(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process(\"pubmed_mini/pubmed19n0080.xml.gz\", \"pubmed00080.docria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = MsgpackDocumentReader(open(\"pubmed00080.docria\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids = []\n",
    "titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in tqdm(reader):\n",
    "    props = doc.properties()\n",
    "    pmids.append(props[\"pmid\"])\n",
    "    titles.append(props[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = MsgpackDocumentReader(open(\"pubmed00080.docria\", \"rb\"))\n",
    "for mdoc in tqdm(reader):\n",
    "    doc = mdoc.document()\n",
    "    sentences.extend([str(sent[\"text\"]) for sent in doc[\"sentence\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count #library lets you run many steps in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #allows you to interface with the underlying operating system that Python is running on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfiles = list(\n",
    "    map(lambda fname: os.path.join(\"pubmed_mini\", fname),\n",
    "        filter(lambda fname: fname.endswith(\".xml.gz\"), os.listdir(\"pubmed_mini\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfiles = [os.path.join(\"pubmed_mini\", os.path.basename(fname) + \".docria\") for fname in inputfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genwork(inputfiles, outputfiles):\n",
    "    for i, o in zip(inputfiles, outputfiles):\n",
    "        yield {\"inputfile\": i , \"outputfile\": o}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work(args):\n",
    "    inputfile = args[\"inputfile\"]\n",
    "    outputfile = args[\"outputfile\"]\n",
    "    process(inputfile, outputfile)\n",
    "    return outputfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(cpu_count()) #use all kernels on computer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.imap_unordered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for completed in tqdm(pool.imap_unordered(work, genwork(inputfiles, outputfiles))):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
