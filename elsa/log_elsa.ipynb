{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November \n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* \n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November \n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* \n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 14th\n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Made ridiculously extensive hastags in code for creating dictionaries, to be able to use it in the thesis\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 13th\n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Made table about conversions\n",
    "* Gave temporary headings to all parts of what I've written so far, to get an overview of what I've written where. Used this to restructure some parts of the thesis\n",
    "* Wrote\n",
    "* Had a job interview \n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 12th\n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Worked on disease API implementation \n",
    "* Had meeting with new students and Sonja\n",
    "* Finished yeast parts \n",
    "* Wrote\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses API from http://www.disgenet.org/api/\n",
    "#prints output in output and in new file\n",
    "#looks like a lot of iterations, but seems to actually just be a lot of information about the genes\n",
    "#output is a mess, needs some cleaning up, maybe with .split\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "with open(r\"Disgenet_API.json\", \"w\") as nf, open(\"test3.txt\") as ipf:\n",
    "    for line in ipf:\n",
    "        uniprotID = line.split()[0]\n",
    "        response = requests.get(\"http://www.disgenet.org/api/gda/gene/uniprot/\" + uniprotID)\n",
    "        print(response.content)\n",
    "        output = str(response.content)\n",
    "#        output_split = str(response.content).split(\",\")\n",
    "        print(output, file = nf)\n",
    "#        print(\"and splitted:\")\n",
    "#        print(output_split)\n",
    "\n",
    "# uniprotIDS in test3:\n",
    "# Q92934 BAD\n",
    "# P04637 P53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 11th\n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Marcus helped with code for overlap, solution was set(open())\n",
    "* Coded protein-counter (without help from supervisor, yay)\n",
    "* Wrote\n",
    "\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Wait for Sonja to answer how to pose questions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints numbers of lines of files in current folder \n",
    "import os\n",
    "\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    with open(filename) as f:\n",
    "        print(filename)\n",
    "        lines = len(f.readlines())\n",
    "        print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### same code but with a lot of explanations (for non-coding to understand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints numbers of lines of files in current folder\n",
    "import os\n",
    "\n",
    "for filename in os.listdir(os.getcwd()): #For each file in this folder\n",
    "    with open(filename) as f: #the code opens the file and gives it the variable f\n",
    "        print(filename) #writes the file name in the output of the code\n",
    "        lines = len(f.readlines()) #counts how many lines the document has\n",
    "        print(lines) #writes the number of lines in the output of the code, which corresponds to the number of proteins, since there is 1 protein on each line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writes overlap between file1 and file2 in newFile\n",
    "file1 = open(r\"path to file1\")\n",
    "file2 = set(open(r\"path to file2\"))\n",
    "newFile = open(r\"Overlap category of file1 and category of file2 - species\",\"w\")\n",
    "\n",
    "for line in file1:\n",
    "    if line in file1:\n",
    "        newFile.write(line)\n",
    "\n",
    "newFile.close()\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing but as explained as much as possible, for med students to understand\n",
    "\n",
    "# Writes overlap between file1 and file2 in newFile\n",
    "\n",
    "file1 = open(r\"path to file1\")\n",
    "# Replace \"path to file1\" with an actual path to a certain file, for example a file with a list of proteins involved in autophagy. The code opens the file in that path and gives it the variable file1.\n",
    "file2 = set(open(r\"path to file2\"))\n",
    "# Replace \"path to file1\" with an actual path to a certain file, for example a file with a list of proteins involved in cell death. The code opens the file in that path and gives it the variable file2. The set()function is used to ensure that the computer can iterate over that file.\n",
    "newFile = open(r\"Overlap category of file1 and category of file2 - species\",\"w\")\n",
    "# Replace \"categories of file1 and 2 with the right category, in this example \"autophagy and cell death\". Replace species with the right species, fore example h sapiens. The code creates a new file with the entered file name, in this example \"Overlap autophagy and cell death - h sapiens\", and the variable newFile.\n",
    "\n",
    "for line in file1:\n",
    "# The code iterates over each line, corresponding to a protein, in file1\n",
    "    if line in file2:\n",
    "    # The code identifies the lines (proteins) in file1 which are also found in file2 (ie the duplicates) and...\n",
    "        newFile.write(line)\n",
    "        # ... writes those lines in the new file.\n",
    "\n",
    "newFile.close()\n",
    "# Closes the file, which is necessary after writing in a file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 8th\n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Almost wrote program for overlap\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Wait for answers from Marcus and Sonja\n",
    "* Overlaps\n",
    "* Venn diagrams - maybe the tool I've been using so far isn't ideal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints overlap of f1 and f2\n",
    "f1 = open(r\"C:\\Users\\elsar\\PycharmProjects\\make_dictionaries\\test3.txt\")\n",
    "f2 = open(r\"C:\\Users\\elsar\\PycharmProjects\\make_dictionaries\\testtext\")\n",
    "nf = open(\"newfile.txt\",\"w+\")\n",
    "\n",
    "for line in f1:\n",
    "    if line in f2:\n",
    "        nf.write(line)\n",
    "\n",
    "nf.close()\n",
    "f1.close()\n",
    "f2.close()\n",
    "\n",
    "#prints hejsan if I write that in both documents, but doesn't print proteins, have emailed Marcus about it\n",
    "f1 = open(r\"C:\\Users\\elsar\\PycharmProjects\\make_dictionaries\\Overlaps\\autophagy all species.txt\")\n",
    "f2 = open(r\"C:\\Users\\elsar\\PycharmProjects\\make_dictionaries\\Overlaps\\cell death all species.txt\")\n",
    "nf = open(\"newfile.txt\",\"w+\")\n",
    "\n",
    "for line in f1:\n",
    "    if line in f2:\n",
    "        nf.write(line)\n",
    "\n",
    "nf.close()\n",
    "f1.close()\n",
    "f2.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 7th\n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Tried making API work, got surprisingly far but only managed to get the API to run on last line of a list instead of all the lines \n",
    "* Made files \n",
    "* Wrote \n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Make the rest of easy files at home (bc have mouse and 2 screens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use API from http://www.disgenet.org/api/, this one only gets last uniprotID unfortunately\n",
    "import requests\n",
    "with open (\"test3.txt\") as mf:\n",
    "    for line in mf:\n",
    "        uniprotID = line\n",
    "        response = requests.get(\"http://www.disgenet.org/api/gda/gene/uniprot/\" + uniprotID)\n",
    "        print(response.content)\n",
    "\n",
    "# uniprotIDS in test3:\n",
    "# Q92934 BAD\n",
    "# P04637 P53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 6th\n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Didn't know how to make progress on project, took the day off \n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 5th\n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Wrote\n",
    "* Read about predictions of protein function\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Continue with essay when Sonja has answered\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 4th\n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Created files with lists of proteins\n",
    "* Made Venn diagrams for all species and human because these were the IDs I already had and are the most relevant\n",
    "* Wrote draft for ethical reflections\n",
    "* Improved discussion\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Continue writing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 1st \n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Made table for protein databases \n",
    "* Learnt about APIs https://www.dataquest.io/blog/python-api-tutorial/ and tried using drug-gene interaction API (see below)\n",
    "    * print(response.json()) was successful (with chaotic output) but print(response_interactions.json()) gave {'status': 500, 'error': 'Internal Server Error'} even though I tried to insert information from the database itself \n",
    "* Tried doing drugs/diseases steps and asked a lot questions to Sonja about drugs/diseases steps\n",
    "\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Need help from Sonja with drug/disease parts \n",
    "* Meanwhile can be occupied with making files for the report and venn diagrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matchedTerms': [{'searchTerm': 'IBUPROFEN', 'drugName': 'IBUPROFEN', 'chemblId': 'CHEMBL521', 'interactions': [{'interactionId': '59e74cb5-92b8-4776-83e9-2bbfccef149a', 'interactionTypes': ['inhibitor'], 'geneName': 'PTGS1', 'geneLongName': 'PROSTAGLANDIN-ENDOPEROXIDE SYNTHASE 1', 'geneEntrezId': 5742, 'sources': ['TdgClinicalTrial', 'GuideToPharmacologyInteractions', 'ChemblInteractions', 'TEND'], 'pmids': [], 'score': 4}, {'interactionId': '83927294-666d-4356-86ef-754151d00757', 'interactionTypes': ['inhibitor'], 'geneName': 'CPT2', 'geneLongName': 'CARNITINE PALMITOYLTRANSFERASE 2', 'geneEntrezId': 1376, 'sources': ['GuideToPharmacologyInteractions'], 'pmids': [], 'score': 1}, {'interactionId': '0f4c5a88-f3a6-4365-9795-bdd2f15f5e05', 'interactionTypes': ['inhibitor'], 'geneName': 'SLC5A8', 'geneLongName': 'SOLUTE CARRIER FAMILY 5 MEMBER 8', 'geneEntrezId': 160728, 'sources': ['GuideToPharmacologyInteractions'], 'pmids': [], 'score': 1}, {'interactionId': '7567234e-050d-4b03-b205-baec7b3df326', 'interactionTypes': ['inhibitor'], 'geneName': 'PTGS2', 'geneLongName': 'PROSTAGLANDIN-ENDOPEROXIDE SYNTHASE 2', 'geneEntrezId': 5743, 'sources': ['TdgClinicalTrial', 'ChemblInteractions', 'TEND', 'TTD'], 'pmids': [], 'score': 4}, {'interactionId': '52891fdc-7cda-4a58-b66a-283916be19ee', 'interactionTypes': ['channel blocker'], 'geneName': 'ASIC1', 'geneLongName': 'ACID SENSING ION CHANNEL SUBUNIT 1', 'geneEntrezId': 41, 'sources': ['GuideToPharmacologyInteractions'], 'pmids': [], 'score': 1}, {'interactionId': 'e98c4471-8bb0-46f8-92d6-9e3610a5a43a', 'interactionTypes': [], 'geneName': 'DDIT3', 'geneLongName': 'DNA DAMAGE INDUCIBLE TRANSCRIPT 3', 'geneEntrezId': 1649, 'sources': ['NCI'], 'pmids': [15131590], 'score': 2}, {'interactionId': 'ca87c1a5-5c81-4dd1-918c-508678a56a3e', 'interactionTypes': [], 'geneName': 'PTGER4', 'geneLongName': 'PROSTAGLANDIN E RECEPTOR 4', 'geneEntrezId': 5734, 'sources': ['NCI'], 'pmids': [9118476], 'score': 2}, {'interactionId': '0cc4dcdb-5810-4b99-ae8b-7256979c6042', 'interactionTypes': [], 'geneName': 'APC', 'geneLongName': 'APC, WNT SIGNALING PATHWAY REGULATOR', 'geneEntrezId': 324, 'sources': ['CKB'], 'pmids': [17909047], 'score': 2}, {'interactionId': 'e1c9f321-8c3d-425e-bdb9-c69861376517', 'interactionTypes': [], 'geneName': 'ABCB1', 'geneLongName': 'ATP BINDING CASSETTE SUBFAMILY B MEMBER 1', 'geneEntrezId': 5243, 'sources': ['NCI'], 'pmids': [11583291], 'score': 2}, {'interactionId': 'd4b81bb8-3451-4be4-a507-d1684b71b789', 'interactionTypes': [], 'geneName': 'CYP1A2', 'geneLongName': 'CYTOCHROME P450 FAMILY 1 SUBFAMILY A MEMBER 2', 'geneEntrezId': 1544, 'sources': ['NCI'], 'pmids': [12580991], 'score': 2}, {'interactionId': 'c2766bbd-6fde-44e5-b52d-e76086d8ed30', 'interactionTypes': [], 'geneName': 'VHL', 'geneLongName': 'VON HIPPEL-LINDAU TUMOR SUPPRESSOR', 'geneEntrezId': 7428, 'sources': ['NCI'], 'pmids': [15217953], 'score': 2}, {'interactionId': 'eaec39cd-a404-4fe1-ac28-d231d6013c35', 'interactionTypes': [], 'geneName': 'IFNG', 'geneLongName': 'INTERFERON GAMMA', 'geneEntrezId': 3458, 'sources': ['NCI'], 'pmids': [8931897], 'score': 2}, {'interactionId': 'cffd47c8-8d21-4570-836a-7e9e96d00dbc', 'interactionTypes': [], 'geneName': 'HPGD', 'geneLongName': '15-HYDROXYPROSTAGLANDIN DEHYDROGENASE', 'geneEntrezId': 3248, 'sources': ['NCI'], 'pmids': [16997128], 'score': 2}, {'interactionId': '236b900a-a833-4c29-b272-0ab9f0f33840', 'interactionTypes': [], 'geneName': 'PTGER1', 'geneLongName': 'PROSTAGLANDIN E RECEPTOR 1', 'geneEntrezId': 5731, 'sources': ['NCI'], 'pmids': [8870119, 9118476], 'score': 3}, {'interactionId': 'c6dc8f28-dfa2-48a0-9364-8bd9d0e9872e', 'interactionTypes': [], 'geneName': 'CYP19A1', 'geneLongName': 'CYTOCHROME P450 FAMILY 19 SUBFAMILY A MEMBER 1', 'geneEntrezId': 1588, 'sources': ['NCI'], 'pmids': [15964185], 'score': 2}]}], 'ambiguousTerms': [], 'unmatchedTerms': []}\n",
      "<Response [500]>\n"
     ]
    }
   ],
   "source": [
    "#use API from http://www.dgidb.org/api_v1\n",
    "import requests\n",
    "import json\n",
    "#insert gene names or entrez ID in genes=gene1,gene2 etc for example genes=100271715,10994,123688,164045,22898,23074,23078,23240,253012\n",
    "#insert drug names in drugs=drug1,drug2 etc for example drugs=ibuprofen\n",
    "#matchedterms is what it found\n",
    "response = requests.get(\"http://dgidb.org/api/v2/interactions.json?drugs=ibuprofen&drugs=ibuprofen&nteraction_sources=TALC\")\n",
    "print(response.json())\n",
    "\n",
    "response_interactions = requests.get(\"http://dgidb.org/api/v2/interactions.json?genes=CYP2E1&drugs=ISOFLAVONE&interaction_sources=TALC\")\n",
    "print(response_interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 31st\n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Improved introduction and populärvetenskaplig sammanfattning\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Protein interactions and drug/disease parts \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 30th\n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Learnt a lot about autophagy from taking notes from https://www.nature.com/articles/s41580-018-0003-4#Abs1\n",
    "* Wrote, mostly about protein/gene databases\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 29th\n",
    " \n",
    "## Aims:\n",
    "* \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Wrote and read a lot about protein databases \n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Finish writing/reading about protein databases\n",
    "* See Sonja's mail\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 28th\n",
    " \n",
    "## Aims:\n",
    "* Try applying HGNC-dictionary\n",
    "* Do disease part of project\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Met with Marcus\n",
    "    * Got code up and running that I can insert dictionary and text that I want annotated \n",
    "* Looked into disease and drug part of project, downloaded files\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\elsar\\\\PycharmProjects\\\\make_dictionaries\\\\uniprot_sprot.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-eeeacd377fa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Byt ut din path mot korrekt, jag läser filen komprimmerad, därav gzip.open\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mraw_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\elsar\\PycharmProjects\\make_dictionaries\\uniprot_sprot.xml\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'{http://uniprot.org/uniprot}'\u001b[0m \u001b[1;31m#namespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# tag är attributet för att välja vilken topp-nivå nod, då uniprot använder sig av XML namespaces måste jag lägga till {namespace} före noden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\elsar\\\\PycharmProjects\\\\make_dictionaries\\\\uniprot_sprot.xml'"
     ]
    }
   ],
   "source": [
    "# adds the protein and gene names to uniprot_dictionary by streaming document\n",
    "# which means reading it line by line instead of the whole thing at once, because then it crashes\n",
    "\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "import gzip\n",
    "# Denna metoden behövs för att lxml inte ska läcka minne.\n",
    "def fast_iter(context):\n",
    "    \"\"\"\n",
    "    http://lxml.de/parsing.html#modifying-the-tree\n",
    "    Based on Liza Daly's fast_iter\n",
    "    http://www.ibm.com/developerworks/xml/library/x-hiperfparse/\n",
    "    See also http://effbot.org/zone/element-iterparse.htm\n",
    "    \"\"\"\n",
    "    for event, elem in context:\n",
    "        yield elem\n",
    "        # It's safe to call clear() here because no descendants will be\n",
    "        # accessed\n",
    "        elem.clear()\n",
    "        # Also eliminate now-empty references from the root node to elem\n",
    "        for ancestor in elem.xpath('ancestor-or-self::*'):\n",
    "            while ancestor.getprevious() is not None:\n",
    "                del ancestor.getparent()[0]\n",
    "    del context\n",
    "\n",
    "# Byt ut din path mot korrekt, jag läser filen komprimmerad, därav gzip.open\n",
    "raw_in = open(r\"C:\\Users\\elsar\\PycharmProjects\\make_dictionaries\\uniprot_sprot.xml\", mode=\"rb\")\n",
    "ns='{http://uniprot.org/uniprot}' #namespace\n",
    "# tag är attributet för att välja vilken topp-nivå nod, då uniprot använder sig av XML namespaces måste jag lägga till {namespace} före noden\n",
    "# Du kan se vilket namespace genom xmlns attributet för en rad:\n",
    "# <entry dataset=\"Swiss-Prot\" created=\"2011-06-28\" modified=\"2019-06-05\" version=\"28\" xmlns=\"http://uniprot.org/uniprot\">\n",
    "#\n",
    "# Nedan väljer alla entry element och allt som finns under det.\n",
    "# {http://uniprot.org/uniprot}entry\n",
    "reader = etree.iterparse(raw_in, tag=\"{http://uniprot.org/uniprot}entry\")\n",
    "with open('uniprot_dictionary.txt', 'w') as wf: #get full protein names\n",
    "    for entry in tqdm(fast_iter(reader)):\n",
    "        for protein_name in entry.iterfind(\".//\" + ns + \"fullName\"): #get full protein names\n",
    "            wf.write(protein_name.text + \"\\n\")\n",
    "        for gene_name in entry.iterfind(ns + \"gene\"): #get full gene names\n",
    "            for name in gene_name.iterfind(ns+\"name\"):\n",
    "                wf.write(name.text + \"\\n\")\n",
    "\n",
    "proteins = []\n",
    "for line in open(\"uniprot_dictionary.txt\", \"r\").readlines():\n",
    "    line = line.strip() #remove garbage at the end of lines\n",
    "    if len(line) > 0:\n",
    "        proteins.append(line)\n",
    "proteins = list(set(proteins)) #remove duplicates\n",
    "proteins.sort()\n",
    "\n",
    "with open('uniprot_dictionary_processed.txt', 'w') as wf:\n",
    "    for entry in tqdm(proteins):\n",
    "        wf.write(entry + \"\\n\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 25th\n",
    " \n",
    "## Aims:\n",
    "* Prepare for meeting on Monday\n",
    "* Continue writing\n",
    "* Maybe keep on copy-pasting\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Ran make uniprot dictionary code\n",
    "    * Computer completely crashed\n",
    "    * Tried it in colab\n",
    "        * File too big -> compressed it, uploaded it, unzipped it with !unzip /content/uniprot_sprot_copy.zip\n",
    "    * It crashed (this time only the session, not the whole computer)\n",
    "    * Emailed Sonja and Marcus to try to get some help\n",
    "* Wrote\n",
    "* Looked into different ways to create venn diagram, http://bioinformatics.psb.ugent.be/webtools/Venn/ is the winner\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* create venn diagrams with http://bioinformatics.psb.ugent.be/cgi-bin/liste/Venn/calculate_venn.htpl\n",
    "    * Change colors in Paint afterwards to be consistent\n",
    "* write\n",
    "* disease and drug part of project\n",
    "* find good protein-protein interaction thing\n",
    "    * string max 2 000 - can run overlap\n",
    "    * downloadable files really big, do sample files to figure out which ones are good \n",
    "    * when loading many entries, string forces you to chose organism. try what happens if a wrong organism is chosen. \n",
    "        * right protein IDs and wrong organism\n",
    "        * right proteins but different organisms \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 24th \n",
    " \n",
    "## Aims:\n",
    "* Do stuff that aren't a lot easier with 2 screens for example don't convert IDs\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Wrote key words in methods and discussion section of thesis\n",
    "* Continued copy/pasteing/converting in Excel sheets\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Get Endnote refs for all databases\n",
    "* Write keywords -> full sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 23rd\n",
    " \n",
    "## Aims:\n",
    "* Continue to work on prior knowledge part \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Spent a lot of time converting different sorts of IDs to UniProt ID for the amigo document\n",
    "* Met with Pierre, Marcus, Sonja and Dennis\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Write on essay, database mining\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 22nd\n",
    " \n",
    "## Aims:\n",
    "* See yesterday\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Hashtagged on Marcus's docria file\n",
    "* Went through comments Sonja gave at yesterday's meeting \n",
    "\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* To fully understand it: would need to download a good example file and actually run the code \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 21st\n",
    " \n",
    "## Aims:\n",
    "* Go through questions re lists with Sonja on Monday\n",
    "* Comment on Marcus's notebook - explain cells\n",
    "* Continue to recover from cold - make this a short day \n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Met Sonja\n",
    "* Started commenting Marcus's notebook\n",
    "* Learnt about regex https://www.regular-expressions.info/quickstart.html \n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 18th\n",
    " \n",
    "## Aims:\n",
    "*  Continue making protein/gene lists\n",
    "\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Processed database lists\n",
    "    * Spent a lot of time copy-pasting stuff in HeLa subcell loc document, to make \"batch predictions\" for 500 proteins at the time \n",
    "* Met with Marcus and Sonja\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Go through questions re lists with Sonja on Monday\n",
    "* Comment on Marcus's colab - explain cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 17th\n",
    " \n",
    "## Aims:\n",
    "*  Make progress on database lists \n",
    "\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Finished article about databases\n",
    "* Downloaded lists and removed irrelevant stuff\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Same as today\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 16th\n",
    " \n",
    "## Aims:\n",
    "*  \n",
    "\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* E-science conference \n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 15th\n",
    " \n",
    "## Aims:\n",
    "*  Make plan \n",
    "\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Met Sonja\n",
    "* E-science conference \n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 14th\n",
    " \n",
    "## Aims:\n",
    "* Get better understanding of what steps the projects need\n",
    "\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Read last part of previous student's report\n",
    "* Downloaded and looked at bioinfer corpus\n",
    "* Emailed Johan\n",
    "* Talked to Sonja\n",
    "    * Decided to make a simpler version of the project\n",
    "        * Use already existing taggers/databases etc \n",
    "        * Write about AI approaches in the discussion\n",
    "        * If time: start executing AI approaches \n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Meet Sonja tmw - make plan\n",
    "* Read review article \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 11th\n",
    " \n",
    "## Aims:\n",
    "* Get geniatagger running on test file\n",
    "* Get Jensen's tagger running on test file \n",
    "* If time: see how to make the output match the golden standard \n",
    "* Learn about doctria at Marcus's defence\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Yay, now geniatagger works, thanks to Johan's advice, see code below\n",
    "https://colab.research.google.com/drive/1PcFg3nNijNe9jxsvPlLzTqQATrcAoFTM#scrollTo=w4D33JwFfgH1\n",
    "    * downside is that the output doesn't match the golden standard, see pictures below\n",
    "* Yay, Johan made Jensen tagger work, he even made an evaluater work https://colab.research.google.com/drive/18kAosTvYiYXh_Ottdn_eOcHP1mQ_4g7Q#scrollTo=2ylqSrdAvtZd\n",
    "* I tried on some different abstracts\n",
    "    * Precision seems to be good and recall pretty bad. F-score around 0,15. \n",
    "    * Some times there the tagger and evaluater mean the same thing, but annotate slightly different - leading to a false negative and a false positive - when the entity is actually recognized. Example:\n",
    "        * Jensen identified: \t12453616\tGalpha\t76\t82 - counts as false positive\n",
    "        * Evaluater identified: 12453616\tGalpha(o)\t76\t85 - counts as false negative (as if it hasn't recognized it)\n",
    "* Johan has also made GNormPlus running \n",
    "    * I don't understand how to change the file, but for the one Johan has inputted - the F-score is 0,6 which is much better than Jensen \n",
    "* Read previous students' reports (again)\n",
    "    * Understood the purpose of everthing they did now (last time I didn't undertand much), but not entirely how they did stuff\n",
    "* Went to Marcus's defence\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* x finish reading 3Berntsson - from methods \n",
    "* see how to make the output match the golden standard for geniatagger \n",
    "* if we want to use Jensen tagger (have asked Sonja about this): probably count score together for all the abstracts and fix problem above with <i> false </i> false positives/negatives \n",
    "* / consider GNormPlus (see Johan's colab/mail)\n",
    "* x Meet Sonja\n",
    "* x Prepare for e-science conference \n",
    "* x Take a closer look at bioinfer corpus - it is fully tagged and can therefore be used easier for relationship extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make geniatagger run on uploaded file\n",
    "# Måsvingarna behövs för att göra om en python-variabel ('line') till något som bakomliggande shell förstår (allt som körs med ! först körs utanför python)\n",
    "# Fnuttarna för att det är text\n",
    "\n",
    "with open (\"/content/test3.txt\") as myfile: # file should have one sentence per line - can be done with code in log_elsa \n",
    "  for line in myfile: \n",
    "    !echo \"{line}\" | ./geniatagger # not optimal - restarts geniatagger for each line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output geniatagger:\n",
    "(click on cell to see linebreaks)\n",
    "\n",
    "loading morphdic...done.\n",
    "loading pos_models................done.\n",
    "loading chunk_models....done.\n",
    "loading named_entity_models..done.\n",
    "10022127|t|TIF1gamma\t10022127|t|TIF1gamma\tNN\tB-NP\tB-protein\n",
    ",\t,\t,\tO\tO\n",
    "a\ta\tDT\tB-NP\tO\n",
    "novel\tnovel\tJJ\tI-NP\tO\n",
    "member\tmember\tNN\tI-NP\tO\n",
    "of\tof\tIN\tB-PP\tO\n",
    "the\tthe\tDT\tB-NP\tO\n",
    "transcriptional\ttranscriptional\tJJ\tI-NP\tO\n",
    "intermediary\tintermediary\tJJ\tI-NP\tO\n",
    "factor\tfactor\tNN\tI-NP\tO\n",
    "1\t1\tCD\tI-NP\tO\n",
    "family.\tfamily.\tNN\tI-NP\tO\n",
    "\n",
    "\n",
    "loading morphdic...done.\n",
    "loading pos_models................done.\n",
    "loading chunk_models....done.\n",
    "loading named_entity_models..done.\n",
    "\n",
    "\n",
    "loading morphdic...done.\n",
    "loading pos_models................done.\n",
    "loading chunk_models....done.\n",
    "loading named_entity_models..done.\n",
    "10022127|a|We\t10022127|a|We\tPRP\tB-NP\tO\n",
    "report\treport\tVBP\tB-VP\tO\n",
    "the\tthe\tDT\tB-NP\tO\n",
    "cloning\tcloning\tNN\tI-NP\tO\n",
    "and\tand\tCC\tI-NP\tO\n",
    "characterization\tcharacterization\tNN\tI-NP\tO\n",
    "of\tof\tIN\tB-PP\tO\n",
    "a\ta\tDT\tB-NP\tO\n",
    "novel\tnovel\tJJ\tI-NP\tO\n",
    "member\tmember\tNN\tI-NP\tO\n",
    "of\tof\tIN\tB-PP\tO\n",
    "the\tthe\tDT\tB-NP\tO\n",
    "Transcriptional\tTranscriptional\tNNP\tI-NP\tB-DNA\n",
    "Intermediary\tIntermediary\tNNP\tI-NP\tI-DNA\n",
    "Factor\tFactor\tNNP\tI-NP\tI-DNA\n",
    "1\t1\tCD\tI-NP\tI-DNA\n",
    "(\t(\t(\tO\tI-DNA\n",
    "TIF1\tTIF1\tNN\tB-NP\tI-DNA\n",
    ")\t)\t)\tO\tI-DNA\n",
    "gene\tgene\tNN\tB-NP\tI-DNA\n",
    "family\tfamily\tNN\tI-NP\tI-DNA\n",
    ",\t,\t,\tO\tO\n",
    "human\thuman\tJJ\tB-NP\tO\n",
    "TIF1gamma\tTIF1gamma\tNN\tI-NP\tO\n",
    "\n",
    "\n",
    "## corresponding in golden standard:\n",
    "10022127\t0\t9\tTIF1gamma\tGene\t51592\n",
    "10022127\t33\t70\ttranscriptional intermediary factor 1\tFamilyName\n",
    "10022127\t147\t184\tTranscriptional Intermediary Factor 1\tFamilyName\n",
    "10022127\t186\t190\tTIF1\tFamilyName\n",
    "10022127\t211\t220\tTIF1gamma\tGene\t51592\n",
    "\n",
    "## similarities and differences: \n",
    "- They have taggad pretty much the same entities\n",
    "- Geniatagger tags everything\n",
    "- Geniatagger tags beginning of entity and in entity on different rows \n",
    "\n",
    "## notes from Johan's email and own notes:\n",
    "Några för- och nackdelar med\n",
    "\n",
    "### Jensen\n",
    "-använder eg inte maskininlärning, utan bara listor\n",
    "-gör en del fel som beror på fel ord och några som verkar bero på att texten 'förskjuts'?\n",
    "+möjligen enklare att förbättra, eftersom man kan lägga till egna listor\n",
    "\n",
    "\n",
    "### PubTator/GNormPlus\n",
    "-kan inte påverka/förbättra den\n",
    "+ingen installation behövs\n",
    "+verkar bättre\n",
    "\n",
    "### Geniatagger\n",
    "* Geniatagger is trained on the NLPBA data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 10th\n",
    " \n",
    "## Aims:\n",
    "* Learn basics of colab - maybe that can help me understand why running the taggers in colab didn't work as I wanted \n",
    "* Continue to try to get geniatagger running on abstracts from corpora \n",
    "\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Looked at https://www.youtube.com/watch?v=wQ8BIBpya2k&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN about basic tensorflow and colab. Have watched it approx 6 months ago, but now I understood why he made the different steps\n",
    "* Thought about how I could use geniatagger (requires one sentence per line) - try:\n",
    "    * extract abstracts from file with annotated abstracts in new file - later let genia tagger annotate this, and see how well it corresponds to the manual annotations\n",
    "    * split with spacy or just based on periods \n",
    "    * try to make genia tagger run a txt-file\n",
    "* read a bit more about Marcus disputation \n",
    "* learnt some random python for fun\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* With help from Johan's email, make Jensen tagger work \n",
    "* try to make geniatagger run a txt-file in colab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract abstracts and titles from file with annotated titles and abstracts\n",
    "with open (\"BC2GNtest.PubTator.txt\") as rf: #input file\n",
    "    with open(\"BC2GN_abstracts_and_titles\", \"w\") as wf: # output file\n",
    "        for line in rf:\n",
    "            if \"|a|\" in line: # |a| denotes abstract\n",
    "                wf.write(line)\n",
    "            if \"|t|\" in line:  # |t| denotes title\n",
    "                wf.write(line)\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning how .split works\n",
    "myfile = (\"Rad 1. Rad 2. Rad 3.\")\n",
    "print(myfile.split('. '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract abstracts and titles from file with annotated titles and abstracts\n",
    "with open (\"two_annotated_abstracts.txt\") as rf: #input file\n",
    "    with open(\"two_abstracts_and_titles.txt\", \"w\") as wf: # output file\n",
    "        for line in rf:\n",
    "            if \"|a|\" in line: # |a| denotes abstract\n",
    "                wf.write(line)\n",
    "            if \"|t|\" in line:  # |t| denotes title\n",
    "                wf.write(line)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "# make period write stuff on new line\n",
    "sentence = []\n",
    "with open (\"two_abstracts_and_titles.txt\") as rf: #input file\n",
    "    with open(\"test3.txt\", 'w') as wf:\n",
    "        for line in rf:\n",
    "            sentence.append(line.split('. '))\n",
    "        for item in sentence:\n",
    "            for subitem in item:\n",
    "                wf.write(\"%s\\n\" % subitem) # no idea what's inside those brackets but it works...\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 9th \n",
    " \n",
    "## Aims:\n",
    "*  Actually get started with tagging\n",
    "\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Read about ML/tagging and looked through documents in folder of taggers - got frightened by Java code\n",
    "* Met with Johan - tried 3 different taggers than can't run on my computer because: \n",
    "    * NCBI's GNormPlus - needs 10 GB RAM\n",
    "    * Jensen's tagger - needs Linux or Mac\n",
    "        * Johan has tried it successfully in colab \n",
    "        * Is dictionary based, no ML involved which is what we're looking for \n",
    "    * GeniaTagger - needs Linux\n",
    "* Can maybe work with colab \n",
    "    * Didn't get Jensen's to work\n",
    "    * GeniaTagger sort of worked, at least got some output\n",
    "* Informed Sonja about project not going very well\n",
    "\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Talk to Sonja - can we change anything? More supervision or more guidelines what to learn?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 8th\n",
    " \n",
    "## Aims:\n",
    "*  Finish introduction\n",
    "\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Wrote on ethtics/methods/discussion - when the methods is more clear to me, I think it will be more clear what information should be where \n",
    "* Read some inspirational (non-scientific) articles: https://lionbridge.ai/articles/12-best-ai-and-machine-learning-articles-of-2018/, that I can't use as references but gave me some ideas of how to explain parts of ML and dieas for the discussion of possible future applications.\n",
    "* Finished first version of introduction\n",
    "* Read articles from endnote group that seemed relevant\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Prepare for meeting with Johan - make document from biocreative where it is clear to me what information we want a tagger to extract\n",
    "* With Johan: get at tagger to work and evaluate it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 7th\n",
    " \n",
    "## Aims:\n",
    "* get at least 1 tagger to work \n",
    "\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* found https://biocreative.bioinformatics.udel.edu/resources/ through former students' log - annotated corpora\n",
    "    * ? there seemed to be more recognized entities than in the actual abstract\n",
    "* tried EXTRACT - biomedical tagger from Jensenlab https://extract.jensenlab.org/ \n",
    "    * can only be used on HTML. Still tested it, to get a basic understanding of what it is doing\n",
    "* looked at GNormPlus from NCBI\n",
    "    * ? didn't understand how to install it (using java)\n",
    "* felt lost, emailed Johan\n",
    "* :( wanted to try scispacy, didn't work even though Salma successfully installed it for me a while ago\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* get GNormPlus to work \n",
    "* compare to annotated corpus \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 4th\n",
    " \n",
    "## Aims:\n",
    "* compare libraries  \n",
    "\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* (worked med school)\n",
    "* compared synonyms in databases we'll use. \n",
    "    * used code below to see if entries were unique \n",
    "* Johan answered how to make uniprot proteins on new rows, works on the sample from uniprot \n",
    "* met with Pierre \n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* Try at least 1 already existing tagger on annotated corpora \n",
    "* Meet with Johan and evaluate tagger \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listpubchem = []\n",
    "listdrugbank = []\n",
    "listchebi =[]\n",
    "\n",
    "with open (\"pubchem_ex\") as pubchem:\n",
    "    for line in pubchem:\n",
    "        listpubchem.append(line)\n",
    "\n",
    "with open(\"drugbank_ex\") as drugbank:\n",
    "    for line in drugbank:\n",
    "        listdrugbank.append(line)\n",
    "\n",
    "llistchebi = [item.lower() for item in listchebi] #to not care about lower or uppercase \n",
    "llistpubchem = [item.lower() for item in listpubchem]\n",
    "llistdrugbank = [item.lower() for item in listdrugbank]\n",
    "\n",
    "for row in llistchebi:\n",
    "    if row not in llistdrugbank:\n",
    "        print(row)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get protein name and gene name from uniprot \n",
    "#now the code gets everything on one row each\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(\"uniprot_sprot.xml\") #choose input document here\n",
    "root = tree.getroot()\n",
    "\n",
    "ns='{http://uniprot.org/uniprot}' #namespace\n",
    "\n",
    "# get full protein names\n",
    "li_prot = [] # skapa listan\n",
    "with open('uniprot_dictionary.txt', 'w') as wf: #because this is the first thing that happens\n",
    "    for fullName in root.iter(ns+\"fullName\"):\n",
    "        li_prot.append(fullName.text)  # bygg upp med varje träff\n",
    "    s = '\\n'\n",
    "    s = s.join(li_prot)\n",
    "    wf.write(s)\n",
    "\n",
    "# get full protein names\n",
    "li_gene = [] # skapa listan\n",
    "with open('uniprot_dictionary.txt', 'a') as af: #because this is the first thing that happens\n",
    "    for name in root.iter(ns+\"gene\"):\n",
    "        li_gene.append(name.find(ns+'name').text)  # bygg upp med varje träff\n",
    "    s = '\\n'\n",
    "    s = s.join(li_gene)\n",
    "    af.write('\\n')\n",
    "    af.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 3rd \n",
    " \n",
    "## Aims:\n",
    "* See how to download the databases Sonja selected \n",
    "\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Created excel file: Selected NLP resources to save information about downloading different databases and track own progress\n",
    "* (Half of day taught at med school)\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* See October 2nd\n",
    "* Meet with Pierre \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 2nd\n",
    " \n",
    "## Aims:\n",
    "*  Finish dictionary for uniprot\n",
    "\n",
    "\n",
    "\n",
    "## Activities:\n",
    "\n",
    "* Emailed Sonja\n",
    "* Email correspondance with Johan - made some progress on uniprot dictionary, see below\n",
    "* Wrote a bit on report\n",
    "* (Half of day taught at med school)\n",
    "\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "* HGNC dictionary: finish it (first wait for Sonja's response)\n",
    "* Uniprot dictionary: get each name on own row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to get Python to store each protein on a new row, meanwhile at least I can print them on different rows:\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(\"sample_from_uniprot.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "ns='{http://uniprot.org/uniprot}'\n",
    "\n",
    "# print full protein name\n",
    "for fullName in root.iter(ns+\"fullName\"):\n",
    "    print(fullName.text)\n",
    "\n",
    "# print gene, name type\n",
    "for name in root.iter(ns+'gene'): #because name  is a subelemnt of gene  \n",
    "   print(name.find(ns+'name').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds the protein and gene names to uniprot_dictionary, unfortunately on one row\n",
    "# tried .join, but didn't get it to work\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(\"three_entries_from_uniprot.xml\") #choose input document here\n",
    "root = tree.getroot()\n",
    "\n",
    "ns='{http://uniprot.org/uniprot}' #namespace\n",
    "\n",
    "# get full protein names\n",
    "with open('uniprot_dictionary.txt', 'w') as wf: #because this is the first thing that happens\n",
    "    for fullName in root.iter(ns+\"fullName\"):\n",
    "        wf.write(fullName.text)\n",
    "\n",
    "# get gene names\n",
    "with open('uniprot_dictionary.txt', 'a') as af: #a to append, not overwrite\n",
    "    for name in root.iter(ns+'gene'):\n",
    "        af.write(name.find(ns+'name').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when I try to get full protein names on own rows, instead each character appears on its own row\n",
    "with open('uniprot_dictionary.txt', 'w') as wf: #because this is the first thing that happens\n",
    "    for fullName in root.iter(ns+\"fullName\"):\n",
    "        list = (fullName.text)\n",
    "        s = '\\n'\n",
    "        s = s.join(list)\n",
    "        wf.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 1st\n",
    "## Aims:\n",
    "* Correct some flaws of the code below - make it:\n",
    "    * Remove blank lines (happens when the cell of the original text-file is empty)\n",
    "    * Replace \"|a|b|c\" with a \\n b \\n c (happens when the cell contains synonyms)\n",
    "* Make it work on actual file\n",
    "<br><br>\n",
    "## Activities:<br>\n",
    "* Got synonyms to individual rows using .split\n",
    "* Removing blank lines is harder than I expected. Would probably work directly in Python if I'd know more about regex, but instead I found a very simple guide https://www.computerhope.com/issues/ch000924.htm, using Notepad++ to replace all \"^\\r\\n\" with nothing, using search mode regular expressions\n",
    "* Sent mandatory email to med school about my progress \n",
    "* Extract the lines that contain synonyms, and remove quotations marks\n",
    "* Can remove the lines containing '\"' by using Notepad++ to replace all '.*\".*' with nothing, using search mode regular expressions\n",
    "* Didn't manage to make it into one nice piece of code because I don't know how to store all the output that I print and I took some short cuts using Notepad++. Hence, to create a dictionary from HGNC:\n",
    "1) Get file, delete top row (headers)\n",
    "2) Run code for getting output into nice rows (insert into input), copy output into desired_file (update: after all this work, I just realised this could be done by copying the text-file into excel, and copying the columns manually...)\n",
    "3) Extract lines starting with '\"' and remove the '\"', copy output into new file\n",
    "4) Run code for getting synonyms to individual rows \n",
    "5) Copy those new rows into desired_file\n",
    "6) I notepad: remove blank lines and lines containing '\"' (first make sure there's no instances this shouldn't be deleted)\n",
    "* It is not pretty, but it works on sample file, yay! So I did it with the real deal: \n",
    "1) \n",
    "2) Threw an errow:\n",
    "Traceback (most recent call last):\n",
    "  File \"C:/Users/elsar/PycharmProjects/test/test1.py\", line 3, in <module>\n",
    "    for line in myfile:\n",
    "  File \"C:\\Users\\elsar\\AppData\\Local\\Programs\\Python\\Python37\\lib\\encodings\\cp1252.py\", line 23, in decode\n",
    "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
    "UnicodeDecodeError: 'charmap' codec can't decode byte 0x90 in position 729: character maps to <undefined>\n",
    "\n",
    "So I copied it manually instead with excel as a mellanlandning\n",
    "3) Same error. Solved error with: file = open(filename, <b>encoding=\"utf8\"</b>). Got back to 2). However, when I tried searching for some samples that should've been in the document, all weren't there. Therefor did the manual copy process instead\n",
    "3) Just noticed that, when copied into excel, the '\"' are removed automatically. Skipped this step\n",
    "6) into \n",
    "4) \n",
    "5) Works for the last part of it\n",
    "<br><br>\n",
    "## Next steps:<br>\n",
    "* Get an output with the whole dictionary on individual rows, not just the last part of it\n",
    "* Try Johan's edits for the uniprot code \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get synonyms to individual rows\n",
    "input = \"NCRNA00181|A1BGAS|A1BG-AS\"\n",
    "list =[]\n",
    "list.append(input.split('|'))\n",
    "for sublist in list:\n",
    "    for element in sublist: \n",
    "        print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting lines starting with \"'\" and removing '\"' from beginning and end of line\n",
    "\n",
    "with open('hgnc_sample_output1.txt', encoding=\"utf8\") as f: #encoding=\"utf8\" to avoid UnicodeDecodeError\n",
    "    mylist = [line.rstrip('\\n') for line in f] #needs to be in string format to use .startswith\n",
    "for line in mylist:\n",
    "    if line.startswith('\"'): #extracting lines starting with '\"'\n",
    "        print(line[1:-1]) #not printing first and last character, meaning the double quotes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 30th\n",
    "## Aims:\n",
    "* Improve dictionary\n",
    "<br><br>\n",
    "## Activities:<br>\n",
    "* Learnt to extract information from txt-file through https://www.computerhope.com/issues/ch001721.htm\n",
    "* Wrote a code that would extract the correct information from sample document\n",
    "<br><br>\n",
    "## Next steps:<br>\n",
    "* Figure out how to make it work on actual file\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made a code that works on the sample file (but got error on actual file)\n",
    "with open(\"hgnc_sample.txt\") as myfile:\n",
    "    list =[]\n",
    "    for line in myfile:\n",
    "        list.append(line.split('\\t')[1:3]) #split by tab and choose columns here\n",
    "        list.append(line.split('\\t')[8:12]) #choose rest of columns here \n",
    "for sublist in list:\n",
    "    for element in sublist:\n",
    "        print(element)\n",
    "\n",
    "\n",
    "# Jon wrote the beginning of a code that would be able to write the elements sorted by header, it needs some debugging though\n",
    "with open(\"hgnc_sample.txt\",\"r\") as myfile:\n",
    "    dt = {}\n",
    "    ls = []\n",
    "    for line in myfile:\n",
    "        ls.append(line)\n",
    "    header = ls.pop(0)\n",
    "    counter = 0\n",
    "    for key in header.split('\\t')[counter]:\n",
    "        temp = []\n",
    "        for row in ls:\n",
    "            temp.append(row.split('\\t')[counter])\n",
    "        dt[key] = temp\n",
    "        counter += 1\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 27th\n",
    "## Aims:\n",
    "* Figure out next steps\n",
    "* Figure out good protein source\n",
    "<br>\n",
    "## Activities:<br>\n",
    "* Compared sources for protein infomation\n",
    "* Met with Sonja\n",
    "* (Taught at med school, hence didn't have much time)\n",
    "<br>\n",
    "## Next steps:<br>\n",
    "* Dictionary\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 26th\n",
    "## Aims:\n",
    "* Figure out how to bulk download fulltext in XML from pubmed, or some other way to text mine from there \n",
    "<br>\n",
    "## Activities:<br>\n",
    "* Googled and took notes \n",
    "* Main findings/ideas:\n",
    "    * \n",
    "* Wrote down some presentation ideas I didn't want to forget \n",
    "<br>\n",
    "## Next steps:<br>\n",
    "* \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 25th\n",
    "## Aims:\n",
    "* Remake dictionary (including finish list of what should be included and excluded\n",
    "<br>\n",
    "## Activities:\n",
    "* Read through https://docs.python.org/2/library/xml.etree.elementtree.html#xml-tree-and-elements and understood it better this time\n",
    "* Didn't get code to work, emailed supervisors\n",
    "* Attended NBIS drop in\n",
    "    * Asked about additional databases, apart from UniProt. They didn't know any straight away. They thought UniProt would be enough, could start with swiss part and then the parts with worse quality. \n",
    "    * Talked about stuff I'll google (string database for example)\n",
    "<br>\n",
    "## Next steps:\n",
    "* Actually manage to run code to remake dictionary\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 24th\n",
    "## Aims:\n",
    "* Quality check my dictionary (turns out, quality was super bad)\n",
    "<br><br>\n",
    "## Activities:<br>\n",
    "* Tried several XML file opener programs. Finally, XML ValidatorBuddy managed to open the uniprot_sprot.xml (= the original file from uniprot). The others had a problem with the file being big (6,7 gb). \n",
    "* Both uniprot_sprot (from uniprot) and proteins_names_uniprot (from Anna and Eric) contain names of proteins, not included in the protein_names_short, that the code I ran yesterday generated. \n",
    "    * In their log, it says that the code generated a list of > 200 000 proteins, whereas my file only has ~ 48 000 lines. \n",
    "    * Tried running the code below -> xml.etree.ElementTree.ParseError: syntax error: line 1, column 0 \n",
    "* When reading the uniprot_sprot.xml, it seems like the interesting information is in the tags, maybe it is possible to extract information using that (instead of the length of the tag, which the previous students seem to have done):\n",
    "<u> this list is not completed yet </u>\n",
    "* including \n",
    "    * <name> \n",
    "        * including:\n",
    "            * <recommendedName> which contains:\n",
    "                * <fullName>\n",
    "                * <ecNumber> (= Enzyme Commission number = numerical classification scheme for enzymes)\n",
    "            *<name type=\"ORF\"> - (= open reading frame = the part of a reading frame that has the ability to be translated) \n",
    "        * exluding: \n",
    "            * <name type=\"scientific\"> - always exlude - is only used for organism/host/\n",
    "            * name type=\"common\"> - always exlude - is only used for organism/host/\n",
    "            * <name type=\"synonym\">\n",
    "    * also excluding <person name= xxx> - subelement of <authorList>, <citation type, <reference \n",
    "    * <dbReference type=\"GeneID\" id=\"2947773\"/>\n",
    "    * <dbReference type=\"KEGG\" id=\"vg:2947773\"/>\n",
    "* excluding:\n",
    "       * <dbReference type=\"NCBI Taxonomy\" id=\"654924\"/> - is ID for the source organism\n",
    "* Tried to make a test file for learning to apply ElementTree to the first 100 lines of the uniprot document, but got error message: file isn't valid (even tho I tried to close all tags)\n",
    "* read about XML format https://www.w3schools.com/xml/xml_syntax.asp and followed tutorial https://docs.python.org/2/library/xml.etree.elementtree.html#xml-tree-and-elements \n",
    "    * Learned how to create and process XML\n",
    "    * Created <b> sample_from_uniprot </b> with the first ~100 lines of the uniprot document, to be able to test parsing and extracting information in small scale\n",
    "<br>\n",
    "    \n",
    "## Next steps:<br>\n",
    "* Do all steps of https://docs.python.org/2/library/xml.etree.elementtree.html#xml-tree-and-elements on country file \n",
    "* Extract everything that's necessary from sample_from_uniprot\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "# in protein_names.txt\n",
    "name = 'long_no_anno'\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "filename = r'C:\\Users\\elsar\\OneDrive\\Documents\\GitHub\\BioNLP\\anna_eric\\protein_name\\protein_names_uniprot.txt'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28  # length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'fullName' == tag:  # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "\n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'w+') as f:\n",
    "    f.write('\\n'.join(proteins))\n",
    "\n",
    "# READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "\n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'ANNOTATION' and 'ALTGENE' not in s:\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "\n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'a', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(proteins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 23rd\n",
    "Aims:\n",
    "* Get closer to testing applying /sci/scacy <br> <br>\n",
    "Activities:\n",
    "* Finished first 3 chapters of spacy course \n",
    "    * It started to get complicated in the end and I wasn't completely sure if I'll actually use spaCy, so I decided to stop. I'm happy that I made it that far tho, because I feel like I got a better understanding of how NLP works in practice.\n",
    "    * Thinking spaCy might not be good for extracting information, but for <b>dependencies</b>\n",
    "* Couldn't open original xml file because it was too big\n",
    "* Found Anna and Erics list of proteins (based on uniprot), tried running their code (se below), which generated a file \"<b>protein_names_short</b>\" that contains what looks like > 48 000 protein names \n",
    "    * For comparison: the document from Anna and Eric \"protein_names_uniprot\" has > 700 000 lines\n",
    "* read about docria https://docria.readthedocs.io/en/latest/#module-docria.storage since it was mentioned in their report, but didn't understand how to apply it... <br>\n",
    "Next steps:\n",
    "* Understand why my files has a lot fewer rows than Anna and Eric's file\n",
    "* Sonja's message re dictionaries and bulk downloading\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "#in protein_names.txt\n",
    "from xml.etree import ElementTree as ET\n",
    "filename=r\"C:\\Users\\elsar\\Dropbox\\T10\\Uniprot\\uniprot_sprot.xml\" #(moved it out from my dropbox afterwards, too big)\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'shortName' in tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear() # The clear() method removes all items from the dictionary.\n",
    "f = open('protein_names_short.txt', 'w')\n",
    "f.write('\\n'.join(proteins))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 20th\n",
    "Aims:\n",
    "* Start trying things in small scale <br><br>\n",
    "Activities:\n",
    "* Read https://www.hindawi.com/journals/mpe/2015/942435/, about how a very big text mining project was performed \n",
    "* Started learning spaCy from https://course.spacy.io/chapter1\n",
    "* Really understand for loops and functions (Not just knowing what it is, but knowing how and when to apply them.)<br>\n",
    "Next steps:\n",
    "* Finish spacy learning thing\n",
    "* Try /sci/spacy on article\n",
    "    * Including spacy matcher\n",
    "* Look through https://www.w3schools.com/python/default.asp for repeating basics of Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 19th\n",
    "Aims: \n",
    "\n",
    "•\tGet help with libraries and complete first step of old students’ log<br>\n",
    "  •\tGet started with jupyter notebook <br><br>\n",
    "Activities:<br>\n",
    "* Finished week 6 coursera ML\n",
    "* Met Johan \n",
    "    * got help to install spacy and ElementTree (and XML-files)\n",
    "    * got advice on how to complete first step of old students' log\n",
    "* writing in jupyter notebook from now on\n",
    "* got help from Salma to install scispacy\n",
    "* tried to learn basics about ElementTree from https://www.datacamp.com/community/tutorials/python-xml-elementtree, didn't get it to work\n",
    "* downloaded uniprot-protein list file from [https://www.uniprot.org/downloads](http://url)<br>\n",
    "* applied scispacy to random article (see below), got some sort of result <br>\n",
    "  Next steps:\n",
    "* Learn about XML-files from pubmed \n",
    "* Learn about spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "text = \"\"\"\n",
    "Summary\n",
    "Lysosomes serve as the cellular recycling centre and are filled with numerous hydrolases that can degrade most cellular macromolecules. Lysosomal membrane permeabilization and the consequent leakage of the lysosomal content into the cytosol leads to so-called “lysosomal cell death”. This form of cell death is mainly carried out by the lysosomal cathepsin proteases and can have necrotic, apoptotic or apoptosis-like features depending on the extent of the leakage and the cellular context. This article summarizes our current knowledge on lysosomal cell death with an emphasis on the upstream mechanisms that lead to lysosomal membrane permeabilization.\n",
    "\n",
    "Introduction\n",
    "The concept of lysosomal cell death (LCD) was first presented by Christian de Duve, who was awarded the Nobel Prize in 1974 for his discovery and characterization of lysosomes as cellular ‘recycling bins’. Owing to the potent hydrolytic capacity of lysosomal enzymes, he also defined lysosomes as ‘suicide bags’ that can cause cell and tissue autolysis upon rupture (de Duve, 1983). Even though lysosomal rupture was recognized back in the 1970s as a powerful way to kill cells (Firestone et al., 1979), the interest in LCD faded during the following decades. This was largely due to the lack of methods to differentiate lysosomal rupture that causes cell death from post-death alterations in autolytic cells. Furthermore, lysosomal involvement in cell death was commonly overlooked because lysosomal membrane permeabilization (LMP) does not necessarily change the ultrastructure of lysosomes (Brunk and Ericsson, 1972) and because the ability of methyl-ketone-based protease inhibitors (e.g. zVAD-fmk) to inhibit cell death was generally considered as proof for caspase-mediated apoptotic cell death, even though such compounds also inhibit lysosomal cysteine cathepsins (Schotte et al., 1999). Thus, the interest in LCD was revived only recently when more advanced assays to study LMP were developed and emerging genetic data corroborated the role of cathepsins as evolutionarily conserved executors of cell death (Tables 1 and 2). This article and the accompanying poster briefly summarize the molecular mechanisms of LCD.\n",
    "\n",
    "Figure1\n",
    "Download figureOpen in new tabDownload powerpoint\n",
    "View inlineView popupDownload powerpoint\n",
    "Table 1.\n",
    "Examples of genetically confirmed cellular models for lysosomal cell death\n",
    "View inlineView popupDownload powerpoint\n",
    "Table 2.\n",
    "Examples of in vivo models of lysosomal cell death\n",
    "Induction of LMP\n",
    "Most, if not all, cell death pathways eventually lead to LMP (Vanden Berghe et al., 2010). To define LCD, it is thus important to differentiate between LMP that is required for cell death and LMP that is a consequence of it. Tables 1 and 2 list experimental systems in which the role of lysosomes in causing cell death has been confirmed. Additionally, numerous other stimuli, including most known inducers of apoptosis, can trigger LMP that either initiates or amplifies the cell death program (Groth-Pedersen and Jaattela, 2010; Johansson et al., 2010). Except for lysosomotropic detergents (detergents that accumulate in lysosomes) and pore-forming toxins, the mechanisms underlying LMP are largely ambiguous, possibly reflecting multiple means to permeabilize the lysosomal membrane, as discussed below.\n",
    "\n",
    "Lysosomotropic detergents\n",
    "Lysosomotropic detergents damage the lysosomal membrane owing to their detergent-like properties (de Duve et al., 1974; Firestone et al., 1979). They are weak bases that diffuse across membranes and become trapped in the acidic lysosomes after protonation (de Duve et al., 1974). Examples of lysosomotropic detergents include amines with hydrophobic side-chains (e.g. imidazole and morpholine) (Firestone et al., 1979), ciprofloxacin (Boya et al., 2003), o-methyl-serine dodecylamide hydrochloride (Li et al., 2000), sphingosine (Kågedal et al., 2001) and siramesine (Ostenfeld et al., 2008), all of which are potent inducers of LMP. Although most lysosomotropic detergents are likely to be cytotoxic to all lysosome-bearing cells (Firestone et al., 1979), the transformation-associated sensitization to some of them (e.g. siramesine) opens possibilities for their use in cancer therapy (Ostenfeld et al., 2005). In addition, l-leucyl-l-leucine methyl ester (Leu-Leu-OMe) is under development for the treatment of graft-versus-host disease owing to its pronounced effect on cytotoxic lymphocytes. The increased sensitivity of these cells depends on their high level of cathepsin C, which is required to convert Leu-Leu-OMe into the detergent (Leu-Leu)n-OMe (n>3) after its delivery to the lysosomes by receptor-mediated endocytosis (Uchimoto et al., 1999).\n",
    "\n",
    "Viral proteins\n",
    "Virus infection requires the delivery of viral genes into the cell, which mostly occurs by penetrating the endolysosomal membranes with viral entry proteins that become active in the acidic environment (Lozach et al., 2011; Vázquez-Calvo et al., 2012). The penetration of non-enveloped viruses is typically achieved by endolysosomal membrane rupture (e.g. adenovirus and rhinovirus HRV14) or pore formation (e.g. rhinovirus HRV2 and poliovirus) (Prchla et al., 1995), which also releases lysosomal content into the cytosol. Adenovirus membrane lytic protein VI ruptures the membrane by causing membrane curvature stress (Maier et al., 2010; Wiethoff et al., 2005), but membrane rupture can also be caused by vesicular swelling beyond the retaining capacity of the membrane. Alternatively, viral capsid proteins of HRV2 and poliovirus insert directly into the endolysosomal membrane and form size-selective pores (Fuchs and Blaas, 2010; Tosteson and Chow, 1997). By contrast, parvovirus H-1 induces lethal LMP in glioma cells that is not directly related to the viral entry process but instead results from a dramatic downregulation of cytosolic cysteine cathepsin inhibitors, which sensitizes the cells to otherwise non-lethal cathepsin release (Di Piazza et al., 2007).\n",
    "\n",
    "The entry of enveloped viruses has not been associated with LMP, possibly owing to the ability of the viral envelope to seal the endolysosomal membrane. Nevertheless, proteins of these viruses that are not involved in the entry process can induce lethal LMP. HIV-1 Nef causes LMP when expressed in high amounts in the cytosol, which might contribute to the massive destruction of CD4-positive T cells upon HIV-1 infection (Laforge et al., 2007). In addition, viral cationic peptides (e.g. HIV-1 Tat peptide), which, upon protonation in the acidic environment, acquire detergent-like properties, might damage lysosomes (Meade and Dowdy, 2007; Ziegler et al., 2005).\n",
    "\n",
    "Bacterial, fungal and snake toxins\n",
    "In a manner similar to viral entry proteins, many bacterial toxins form pores after undergoing conformational changes at low pH (Kagan et al., 1981; Sandvig and van Deurs, 2005). Accordingly, many of them strongly induce LCD, including Bacillus anthracis toxin (Newman et al., 2009), Streptomyces hygroscopicus nigericin (Hentze et al., 2003), Pseudomonas aeruginosa pyocyanin (Prince et al., 2008) and Aggregatibacter actinomycetemcomitans leukotoxin (DiFranco et al., 2012). Similarly, the cytotoxicity of enniatin mycotoxins (Ivanova et al., 2012), and venom toxins from cobra (Feofanov et al., 2005) and South American rattlesnake (Hayashi et al., 2008), have been connected with LMP. Additionally, Vibrio parahaemolyticus VepA was recently identified as a new type of LMP-inducing protein (Matsuda et al., 2012). After inoculation, VepA binds to the cytoplasmic tail of the channel-forming subunit c of vacuolar H+-ATPase and triggers leakage of lysosomal hydrolases into the cytosol in a manner that depends on the subunit c. It will be of great interest to investigate whether VepA causes the widening of the ATPase channel and whether other LMP-inducing stimuli utilize a similar mechanism.\n",
    "\n",
    "Reactive oxygen species\n",
    "Reactive oxygen species (ROS) contribute to LMP that is induced by a wide range of oxidative stimuli (e.g. drugs, heavy metals and ionizing radiation) and conditions (e.g. ischemia–reperfusion injury, inflammation and neurodegenerative disorders) (Kurz et al., 2008a). Upon oxidative stress, excess H2O2 diffuses into lysosomes, where it reacts with redox-active iron, resulting in the production of hydroxyl radicals in Fenton-type reactions (see Poster) (Kurz et al., 2008b). Hydroxyl radicals are highly reactive and can destabilize the lysosomal membrane by causing lipid peroxidation and damaging lysosomal membrane proteins. Additionally, ROS might contribute to LMP by activating lysosomal Ca2+ channels (Sumoza-Toledo and Penner, 2011) or altering the activity of lysosomal enzymes such as phospholipase A2 (PLA2). In concordance with the lysosome-destabilizing effect of ROS, various antioxidants and redox regulators as well as iron-binding proteins confer protection against oxidative-stress-induced LMP (Kurz et al., 2008a; Kurz et al., 2008b).\n",
    "\n",
    "Proteases\n",
    "Cathepsins are mainly considered to be downstream mediators of LCD, but they can apparently also initiate LMP. Supporting this hypothesis, lack of cathepsin B prevents LMP in hepatocytes treated with tumor necrosis factor (TNF) or sphingosine (Werneburg et al., 2002). Furthermore, sensitization to LMP upon oncogene-driven transformation and several models of LCD (e.g. mammary gland involution and death induced by cytoskeletal disruption) are associated with increased cysteine cathepsin activity (Fehrenbacher et al., 2008; Fehrenbacher et al., 2004; Kreuzaler et al., 2011; Groth-Pedersen et al., 2007; Groth-Pedersen et al., 2012). The LMP-promoting effect of cysteine cathepsins might be due to the intralysosomal degradation of highly glycosylated lysosome-associated membrane proteins, which form a protective glycocalyx shield on the inner lysosomal membrane (Eskelinen et al., 2003; Fehrenbacher et al., 2008). Alternatively, minor leakage of cathepsins could activate LMP by cleaving sphingosine kinase 1 or other cytosolic substrates that maintain lysosomal stability (Mora et al., 2010; Taha et al., 2005).\n",
    "\n",
    "Other proteases can also cause LMP. Cytosolic calpain proteases contribute to LMP upon ischemic and hypochlorous-acid-induced injury of neurons (Windelborn and Lipton, 2008; Yamashima et al., 1998; Yap et al., 2006). After deprivation of oxygen and glucose, μ-calpain localizes to lysosomes in hippocampal slices, suggesting a direct effect on the lysosomal membrane (Yamashima et al., 1996). Interestingly, heat shock protein 70 (Hsp70), which stabilizes lysosomes, has been proposed to be a target of calpain in this context (Yamashima, 2012).\n",
    "\n",
    "Finally, the activation of apoptotic caspases is frequently associated with secondary LMP that might speed up or amplify the death process. Often, such secondary LMP is initiated by caspase-9, which can be activated in the apoptososome or, in murine cells, by caspase-8-dependent cleavage (Gyrd-Hansen et al., 2006; Oberle et al., 2010). Furthermore, caspase-2 has been reported to cause LMP and subsequent activation of other caspases in tunicamycin-treated leukemia cells (Huang et al., 2009). The caspase targets that are responsible for LMP remain mostly speculative (Oberle et al., 2010). After TNF receptor internalization, cathepsin D release can result from a caspase-8 and -7-dependent cascade that activates acid sphingomyelinase (ASM&semi see below) see below) (Edelmann et al., 2011; Tchikov et al., 2011). Additionally, TNF-induced LMP in hepatocytes has been reported to be partially inhibited in the absence of the caspase-8 target Bid (Guicciardi et al., 2005; Werneburg et al., 2004), a BH3-only protein, whose truncated form (tBid) is essential for TNF-induced activation of pore-forming Bcl-2 proteins (Bax and Bak) and subsequent mitochondrial outer membrane permeabilization (MOMP) (Happo et al., 2012). It is unclear, however, whether tBid initiates LMP directly or whether it promotes LMP by means of MOMP (Happo et al., 2012).\n",
    "\n",
    "Lipids and their metabolites\n",
    "The sphingolipid metabolite sphingosine may act as an endogenous lysosomotropic detergent following treatments that induce its accumulation – for example, through the activation of lysosomal ASM and acid ceramidase in TNF-treated rat hepatocytes (Ullio et al., 2012). ASM might also enhance the LCD pathway through ceramide-mediated activation and release of cathepsin D (Heinrich et al., 2004; Heinrich et al., 1999). By contrast, ASM activity protects cells against photooxidation-induced LMP, and this might explain the potent lysosome-stabilizing effect of Hsp70, which enhances ASM activity by promoting its binding to lysosomal membranes (Kirkegaard et al., 2010; Nylandsted et al., 2004). Notably, LMP is also triggered by inhibition of sphingosine kinase 1, which converts sphingosine to sphingosine-1-phosphate (S1P) (Mora et al., 2010). In this case, however, loss of S1P, rather than accumulation of sphingosine, damages the lysosomes by hindering lysosomal recycling. Interestingly, sphingosine kinase 1 is a cathepsin B substrate (Taha et al., 2005), whose degradation might contribute to the amplification of LMP. Overall, lysosomal sphingomyelin catabolism controls lysosomal stability by multiple means, with the excess of either sphingomyelin or sphingosine having a destabilizing effect and S1P preserving normal lysosomal function.\n",
    "\n",
    "LMP can also be caused by phospholipase A2 (PLA2), which has been shown to destabilize purified lysosomes (Zhao et al., 2003). Based on studies with semi-selective pharmacological PLA2 inhibitors, cytosolic PLA2 has been implicated in LCD induced by low concentrations of H2O2 (Zhao et al., 2001), neuronal ischemia (Windelborn and Lipton, 2008) and TNF (Wissing et al., 1997), whereas secretory PLA2 has been associated with LCD induced by heavy metals and environmental pollutants (Marchi et al., 2004). These effects might be mediated by arachidonic acid, a lipid metabolite generated by PLA2, which displays detergent-like properties and increases lysosomal permeability to K+ and H+, thereby enhancing lysosomal osmotic sensitivity (Zhang et al., 2006). Thus, PLA2 activity could contribute to LMP in several ways, but more research is required to clarify how different PLA2 enzymes promote LMP.\n",
    "\n",
    "Loss of cholesterol might also increase lysosomal permeability to K+ and H+ and thereby destabilize the lysosomes (Johansson et al., 2010), but this effect is still poorly understood.\n",
    "\n",
    "p53\n",
    "Even though LMP can occur in the absence of cellular tumor antigen p53 (Erdal et al., 2005; Nylandsted et al., 2000; Ostenfeld et al., 2005), emerging evidence supports the notion that p53 can trigger LMP. For example, in myeloid leukemia cells, the activation of temperature-sensitive p53 is sufficient to cause LMP that precedes MOMP (Yuan et al., 2002). Furthermore, early LMP in TNF-treated fibrosarcoma cells (Li et al., 2007), embelin-treated colon cancer cells (Joy et al., 2010), as well as in cortical neurons exposed to Δ9-tetrahydrocannabinol or β-amyloid (Fogarty et al., 2010; Gowran and Campbell, 2008) depends on p53 and is associated with the localization of phospho-Ser15-p53 to the lysosomal membrane. The recruitment of phospho-Ser15-p53 to the lysosomes depends on LAPF (LMP-inducing lysosome-associated apoptosis-inducing protein containing PH and FYVE domains) (Li et al., 2007). It will be of great interest to reveal the mechanism of action of these proteins and to investigate whether p53 and/or LAPF link other cellular signals to LMP.\n",
    "\n",
    "Proapoptotic Bcl-2 family members\n",
    "Proapoptotic Bcl-2 family members are not essential for the induction of LMP, as demonstrated by the failure of Bcl-2 overexpression or Bax–Bak double-deficiency to prevent LMP after various stimuli (Boya et al., 2003; Gonzalez et al., 2012; Gyrd-Hansen et al., 2006; Nylandsted et al., 2000; Ostenfeld et al., 2005; Rammer et al., 2010). They might, however, contribute to LMP in some model systems, as discussed above for the role of tBid in TNF-treated hepatocytes. Besides, it has been suggested that Bim recruits activated Bax to the lysosomes and thereby promotes LMP in hepatocytes treated with TRAIL (TNF-related apoptosis-inducing factor) (Werneburg et al., 2012; Werneburg et al., 2007). Even though Bax can form pores in isolated lysosomes in vitro (Kågedal et al., 2005), its lysosomal localization and direct involvement in LMP remains, however, controversial (Oberle et al., 2010; Repnik et al., 2012). Moreover, a recent report has revealed an unexpected role for Bim in lysosomal acidification (Ruppert et al., 2012), which might indirectly contribute to LMP.\n",
    "\n",
    "Other regulators\n",
    "The disruption of cytoskeleton and cellular trafficking by microtubule-targeting drugs (Bröker et al., 2004; Groth-Pedersen et al., 2007) or by depletion of cytoskeleton-associated motor proteins (Groth-Pedersen et al., 2012) also induces LMP. However, the underlying mechanisms are poorly understood. In addition, many other molecules regulate LMP, as reviewed elsewhere (Boya and Kroemer, 2008; Kirkegaard and Jäättelä, 2009; Kroemer and Jäättelä, 2005; Repnik et al., 2012).\n",
    "\n",
    "Overall, a large number of stimuli and mediators have been implicated in LMP, but future work is likely to connect many of them to a lesser number of signalling pathways that converge on even fewer mechanisms actually causing LMP.\n",
    "\n",
    "Consequences of LMP\n",
    "It is unclear whether the entire lysosomal population is equally prone to LMP or whether a subpopulation of lysosomes is specifically targeted by LMP-inducing stimuli. It is, however, clear that the extent of LMP determines the morphological features of cell death. Extensive LMP results in uncontrolled necrosis with rapid plasma membrane permeabilization, whereas limited LMP can activate the intrinsic apoptosis pathway in apoptosis-competent cells (Kågedal et al., 2001) or caspase-independent death with apoptosis-like morphology in cells with defective apoptosis (Kirkegaard and Jäättelä, 2009).\n",
    "\n",
    "In the case of extensive LMP, most lysosomal content leaks into the cytosol, and specific inhibitors of lysosomal hydrolases fail to attenuate cell death. By contrast, inhibition of cathepsins – especially cysteine cathepsins B and L and aspartyl cathepsin D – by genetic or pharmacological targeting or by overexpression of cytosolic cathepsin inhibitors (e.g. cystatin A or serine protease inhibitor 2A) can confer significant protection against cell death following limited LMP (Tables 1 and 2). The role of cathepsins as executors of LMP-induced apoptosis and apoptosis-like cell death is further supported by the ability of microinjected cathepsin B or D to trigger MOMP and apoptosis (Bivik et al., 2006; Roberg et al., 2002) as well as the capability of cathepsin B to induce apoptotic morphology in isolated nuclei (Vancompernolle et al., 1998).\n",
    "\n",
    "LMP-induced apoptosis is usually activated through MOMP, which can be brought about by cathepsin-mediated activating cleavage of pro-apoptotic (Bid) or inhibiting cleavage of anti-apoptotic Bcl-2 proteins (Bcl-2, Bcl-XL and Mcl-1) (Appelqvist et al., 2012; Cirman et al., 2004; Droga-Mazovec et al., 2008). Furthermore, cytosolic cathepsins can activate apoptotic caspases by cleaving either them or their inhibitor E3 ubiquitin-protein ligase XIAP (Conus et al., 2008; Droga-Mazovec et al., 2008; Vancompernolle et al., 1998; Zhou and Salvesen, 1997). The activated caspases can then enhance either MOMP-dependent or -independent apoptotic death.\n",
    "\n",
    "Notably, LMP can also cause cell death with little or no caspase activation – for example, in response to hypochlorous acid (Yap et al., 2006), depletion of Hsp70 (Nylandsted et al., 2000), antibodies to CD3 (Michallet et al., 2004) or siramesine (Ostenfeld et al., 2005) – and, even when caspases are activated, their inhibition does not necessarily reduce cell death (Di Piazza et al., 2007; Nylandsted et al., 2004). Instead, cathepsins themselves can cleave many cellular proteins and take over the role of ‘death-executing proteases’ (Turk et al., 2012). So far, only a few cell death-promoting cathepsin substrates have been identified (Turk et al., 2012). As discussed above, sphingosine kinase 1 might be one of them. Additionally, cathepsins can cleave the caspase substrate PARP (Gobeil et al., 2001) and cell adhesion molecules such as membrane-associated guanylate kinases (MAGUKs), thereby inducing cellular detachment (Ivanova et al., 2011).\n",
    "\n",
    "It should be emphasized that, even though cathepsins are important executors of LCD, their inhibition provides only partial protection from LCD. Thus, more studies are clearly needed to define the roles of other lysosomal hydrolases (e.g. lipases and phosphatases), lysosome-derived second messengers (e.g. Ca2+, H+ and ROS) and LMP-associated lysosomal dysfunction in LCD.\n",
    "\n",
    "Perspectives\n",
    "LCD has long been overlooked as a mode of regulated cell death. Nevertheless, its regulation and tight links to other cell death pathways are finally beginning to emerge. As discussed above and reviewed elsewhere (Boya and Kroemer, 2008; Česen et al., 2012; Kirkegaard and Jäättelä, 2009; Yamashima and Oikawa, 2009), LCD has important physiological functions, and it contributes to numerous degenerative and infectious diseases (see Poster). Nevertheless, it might provide an alternative strategy for the treatment of apoptosis- and multidrug-resistant cancers (Groth-Pedersen and Jaattela, 2010; Kallunki et al., 2012; Kreuzaler and Watson, 2012). However, a great amount of basic research is still needed to bring our knowledge of the complex regulation of lysosomal stability up to a level that allows the optimal design of LCD-targeting therapies.\n",
    "\"\"\"\n",
    "doc = nlp(text)\n",
    "#print(list(doc.sents))\n",
    "print(doc.ents)\n",
    "\n",
    "\n",
    "# We can also visualise dependency parses\n",
    "# (This renders automatically inside a jupyter notebook!):\n",
    "from spacy import displacy\n",
    "displacy.render(next(doc.sents), style='dep', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 18th\n",
    "Aims: start using jupyter and github<br>\n",
    "Activities:<br>\n",
    "•\tGave feedback on cell hunter text <br>\n",
    "•\tGot stuck trying to run code from previous students’ notebooks<br>\n",
    "o\tSeems like Salma or Johan can help me<br>\n",
    "•\tContinued with week 6 coursera course machine learning<br>\n",
    "•\tSonja uploaded a folder for me on github<br>\n",
    "Next steps: meet Johan \n",
    "\n",
    "# September 17th\n",
    "Aims: get a better understanding of what I should focus on learning \n",
    "Activities: \n",
    "•\tCareer planning with Sonja\n",
    "o\tGoals for the fall: \n",
    "\tsee if research seems to be something for me, in that case maybe continue with it for PhD. \n",
    "\tLearn programming/maths/related stuff \n",
    "o\tLook for external ways (courses, seminars etc) to get knowledge, inspiration and network \n",
    "•\tMeet Johan and Sonja\n",
    "o\tInformation about project\n",
    "o\tSet up some next steps \n",
    "•\tTried to understand the report from Anna and Eric\n",
    "o\tThey used a library called ElementTree for creating a dictionary. Got help from Salma and Jon to install spacy and ElementTree.\n",
    "Next steps: \n",
    "•\tNew personal goal: spend 30 minutes per day for learning new things that are related to the project, but not as closely as for example reading articles that can be used for the project. Mostly thinking of online courses for programming\n",
    "\n",
    "# September 16th\n",
    "Aims: prepare for introduction of thesis \n",
    "Activities:\n",
    "•\tRead articles about lysosomes\n",
    "•\tWrote in introduction about lysosomes\n",
    "•\tStarted reading abstracts in text mining shared group \n",
    "Next steps:\n",
    "•\tRead more NLP articles \n",
    "\n",
    "# September 15th\n",
    "Aims: get knowledge and inspiration for programming \n",
    "Activities:\n",
    "•\tPink programming = programming event in Malmö for women, all levels of programming \n",
    "o\tListened to inspiring lectures\n",
    "o\tMet women who code\n",
    "o\tTried to get help to install spaCy, didn’t succeed \n",
    "\n",
    "\n",
    "# September 13th\n",
    "Aims: \n",
    "•\tMake non-programming progress (since I’m feeling a bit lost in that area)\n",
    "Activities:\n",
    "•\tRead and summarized a bunch of articles related to LCD\n",
    "o\tLater: Use for introduction of thesis \n",
    "Next steps:\n",
    "•\tAsk about spaCy and relevant next steps on Sunday\n",
    "\n",
    "# September 12th\n",
    "Aim: prepare for meeting with Sonja about far-reaching aims for career development and with Johan about project\n",
    "Activities:\n",
    "•\tRead documents from Sonja’s email and wrote down answers to questions I found relevant\n",
    "•\tLearned about Python library spaCy through https://spacy.io/usage/spacy-101 \n",
    "o\tTroubles installing it \n",
    "Next steps:  \n",
    "•\tMeet Sonja tomorrow\n",
    "•\tManage to install and try spaCy\n",
    "\n",
    "# September 11th\n",
    "Main aim: learn more about math relevant for ML\n",
    "Activities: \n",
    "•\tFinished Coursera’s Mathematics for machine learning: Linear algebra \n",
    "•\tDid not find the introduction to deep learning course very helpful\n",
    "•\tStarted NLP course, seems more promising\n",
    "•\tWorked a bit on the NLP resources list\n",
    "Next steps: talk to Sonja about whether it is most relevant to learn more math or if I’m ready for NLP\n",
    "\n",
    "# September 10th\n",
    "Aim: learn basic math\n",
    "•\tFinished week 2 and 3 of lin alg course\n",
    "o\tNot very good at solving matrix equations\n",
    "\n",
    "# September 9th\n",
    "•\tAttended lecture about statistics \n",
    "•\tContinued online course about lin alg\n",
    "\n",
    "\n",
    "# September 8th\n",
    "•\tAimed for starting a NLP course. Instead, followed their recommendations and started online course about linear algebra to prepare for that.\n",
    "\n",
    "# September 6th \n",
    "•\tLearnt that shared groups online work with the online version of EndNote \n",
    "•\tCoursera course: understanding less of the maths for each course week. Finished (but didn’t entirely understand) week 5, started on week 6\n",
    "•\tWorked on NLP resources document\n",
    "•\tRead 3 presentations from the previous students\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 5th \n",
    "•\tE-mailed about EndNote problems\n",
    "•\tFinished Coursera week 3 and 4\n",
    "•\tInstalled SPSS for statistical lecture September 9th \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 4th \n",
    "•\tAttended lecture about how to search for information for a scientific project\n",
    "•\tLearnt basics of EndNote\n",
    "o\tHaving problems esp with the desktop version, which apparently also Salma and John do. Seems like desktop version is needed for getting citation easily into word.\n",
    "•\tGot access card \n",
    "•\tFinished week 2 of Coursera/Stanford course about ML \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 3rd\n",
    "Got familiar with:\n",
    "•\tThe project\n",
    "o\tResources such as LU box, git hub\n",
    "o\tPlanned the next steps\n",
    "•\tColleagues \n",
    "•\tThe work spaces\n",
    "Started reading\n",
    "•\tMade flash cards for basic things worth memorizing\n",
    "•\tReport from former student \n",
    "•\tStarted reading articles från och med endnote-library \n",
    "\n",
    "Thoughts: \n",
    "Many terms in the articles and report are unfamiliar to me, I need more basic knowledge about ML and will prioritize the Coursera course before reading more articles about ML/text mining.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
