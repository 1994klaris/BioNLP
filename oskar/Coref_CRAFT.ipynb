{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coref_CRAFT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKsIN3H3QPBl"
      },
      "source": [
        "### System Installation\n",
        "Installing the coref tool from mandarjoshi90 along with tensorflow.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g07Sx5jqQbPw",
        "outputId": "248ceba3-09e8-43c1-9744-2854886c7a23"
      },
      "source": [
        "! git clone https://github.com/mandarjoshi90/coref.git\n",
        "%cd coref"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'coref' already exists and is not an empty directory.\n",
            "/content/coref\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4mHoDEKQdLu"
      },
      "source": [
        "! sed 's/MarkupSafe==1.0/MarkupSafe==1.1.1/; s/scikit-learn==0.19.1/scikit-learn==0.21/; s/scipy==1.0.0/scipy==1.6.2/' < requirements.txt > tmp\n",
        "! mv tmp requirements.txt\n",
        "\n",
        "! sed 's/.D.GLIBCXX.USE.CXX11.ABI.0//' < setup_all.sh  > tmp\n",
        "! mv tmp setup_all.sh \n",
        "! chmod u+x setup_all.sh "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cABghgMnQgkw",
        "outputId": "7f1ba40e-5195-4106-99af-52df60286b82"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "! pip uninstall -y tensorflow\n",
        "! pip install -r requirements.txt --log install-log.txt -q\n",
        "! ./setup_all.sh"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Successfully uninstalled tensorflow-2.4.1\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 28.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 25.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6MB 19.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 32.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 4.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 18.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 31.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 24.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 32.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 26.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.3MB 2.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1MB 37.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 30.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 430kB 39.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 5.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 256kB 40.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 4.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 4.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 44.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 512kB 45.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 5.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.7MB 22.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 27.4MB 82kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 38.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 491kB 53.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 377.1MB 32kB/s \n",
            "\u001b[K     |████████████████████████████████| 748.9MB 21kB/s \n",
            "\u001b[K     |████████████████████████████████| 8.8MB 36.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 327kB 53.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 256kB 55.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.1MB 44.4MB/s \n",
            "\u001b[?25h  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for h5py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for JPype1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mmh3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for msgpack-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for psycopg2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pycparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyhocon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-metadata 0.29.0 has requirement absl-py<0.13,>=0.9, but you'll have absl-py 0.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyasn1-modules 0.2.8 has requirement pyasn1<0.5.0,>=0.4.6, but you'll have pyasn1 0.4.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.5 has requirement python-dateutil>=2.7.3, but you'll have python-dateutil 2.6.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: googleapis-common-protos 1.53.0 has requirement protobuf>=3.12.0, but you'll have protobuf 3.9.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement astor~=0.8.1, but you'll have astor 0.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.3 has requirement protobuf>=3.12.0, but you'll have protobuf 3.9.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.3 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: flask 1.1.2 has requirement Jinja2>=2.10.1, but you'll have jinja2 2.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: flask 1.1.2 has requirement Werkzeug>=0.15, but you'll have werkzeug 0.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement python-dateutil>=2.8.0, but you'll have python-dateutil 2.6.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: bokeh 2.3.1 has requirement pillow>=7.1.0, but you'll have pillow 6.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtaUoOBJkNDw"
      },
      "source": [
        "### Specifying Input and Training\n",
        "\n",
        "Input and Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuq5EkiiqkpW"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5cxKv28qsRf"
      },
      "source": [
        "#/content/gdrive/MyDrive/CRAFT-txt/dev/17194222.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT-y2vv2QHyy"
      },
      "source": [
        "filename = \"/content/gdrive/MyDrive/CRAFT-txt/dev/17194222.txt\"\n",
        "\n",
        "text = [\n",
        "\"Firefly is an American space Western drama television series which ran from 2002-2003, created by writer and director Joss Whedon, under his Mutant Enemy Productions label.\",\n",
        "\"Whedon served as an executive producer, along with Tim Minear.\",\n",
        "\"The series is set in the year 2517, after the arrival of humans in a new star system and follows the adventures of the renegade crew of Serenity, a 'Firefly-class' spaceship.\",\n",
        "\"The ensemble cast portrays the nine characters who live on Serenity.\",\n",
        "\"Whedon pitched the show as 'nine people looking into the blackness of space and seeing nine different things.'\",\n",
        "\"The show explores the lives of a group of people, some of whom fought on the losing side of a civil war, who make a living on the fringes of society as part of the pioneer culture of their star system.\",\n",
        "\"In this future, the only two surviving superpowers, the United States and China, fused to form the central federal government, called the Alliance, resulting in the fusion of the two cultures.\",\n",
        "\"According to Whedon's vision, 'nothing will change in the future: technology will advance, but we will still have the same political, moral, and ethical problems as today.'\",\n",
        "\"Firefly premiered in the U.S. on the Fox network on September 20, 2002.\",\n",
        "\"By mid-December, Firefly had averaged 4.7 million viewers per episode and was 98th in Nielsen ratings.\",\n",
        "\"It was canceled after 11 of the 14 produced episodes were aired.\",\n",
        "\"Despite the relatively short life span of the series, it received strong sales when it was released on DVD and has large fan support campaigns.\",\n",
        "\"It won a Primetime Emmy Award in 2003 for Outstanding Special Visual Effects for a Series.\",\n",
        "\"TV Guide ranked the series at No. 5 on their 2013 list of 60 shows that were 'Cancelled Too Soon.'\",\n",
        "\"The post-airing success of the show led Whedon and Universal Pictures to produce Serenity, a 2005 film which continues from the story of the series, and the Firefly franchise expanded to other media, including comics and a role-playing game.\",\n",
        "]\n",
        "\n",
        "if filename != \"optional-change-to-your-file.txt\":\n",
        "    data = [l.strip() for l in open(filename).readlines()]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaFGGbm2QN7r"
      },
      "source": [
        "genre = \"nw\"\n",
        "# The Ontonotes data for training the model contains text from several sources\n",
        "# of very different styles. You need to specify the most suitable one out of:\n",
        "# \"bc\": broadcast conversation\n",
        "# \"bn\": broadcast news\n",
        "# \"mz\": magazine\n",
        "# \"nw\": newswire\n",
        "# \"pt\": Bible text\n",
        "# \"tc\": telephone conversation\n",
        "# \"wb\": web data\n",
        "\n",
        "model_name = \"spanbert_base\"\n",
        "# The fine-tuned model to use. Options are:\n",
        "# bert_base\n",
        "# spanbert_base\n",
        "# bert_large\n",
        "# spanbert_large"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ163Z2uQevj"
      },
      "source": [
        "import os\n",
        "os.environ['data_dir'] = \".\"\n",
        "os.environ['CHOSEN_MODEL'] = model_name"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi0-5VN-kr6N"
      },
      "source": [
        "Downloading the selected model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klQ48SSkQiWG",
        "outputId": "6966e3c4-ad0f-49ac-e920-e7e84fc2787e"
      },
      "source": [
        "! ./download_pretrained.sh $CHOSEN_MODEL"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading spanbert_base\n",
            "--2021-05-04 18:55:46--  http://nlp.cs.washington.edu/pair2vec/spanbert_base.tar.gz\n",
            "Resolving nlp.cs.washington.edu (nlp.cs.washington.edu)... 128.208.3.120, 2607:4000:200:12::78\n",
            "Connecting to nlp.cs.washington.edu (nlp.cs.washington.edu)|128.208.3.120|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1633726311 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘./spanbert_base.tar.gz’\n",
            "\n",
            "spanbert_base.tar.g 100%[===================>]   1.52G  42.7MB/s    in 38s     \n",
            "\n",
            "2021-05-04 18:56:24 (41.4 MB/s) - ‘./spanbert_base.tar.gz’ saved [1633726311/1633726311]\n",
            "\n",
            "spanbert_base/\n",
            "spanbert_base/checkpoint\n",
            "spanbert_base/model.max.ckpt.index\n",
            "spanbert_base/stdout.log\n",
            "spanbert_base/bert_config.json\n",
            "spanbert_base/vocab.txt\n",
            "spanbert_base/model.max.ckpt.data-00000-of-00001\n",
            "spanbert_base/events.out.tfevents.1561596094.learnfair1413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDl_lF2EkyCx"
      },
      "source": [
        "Process the data to be in the required input format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "O-rCm36PQlaB",
        "outputId": "2a6f301d-8853-462b-c03d-c58309e8385f"
      },
      "source": [
        "from bert import tokenization\n",
        "import json\n",
        "\n",
        "data = {\n",
        "    'doc_key': genre,\n",
        "    'sentences': [[\"[CLS]\"]],\n",
        "    'speakers': [[\"[SPL]\"]],\n",
        "    'clusters': [],\n",
        "    'sentence_map': [0],\n",
        "    'subtoken_map': [0],\n",
        "}\n",
        "\n",
        "# Determine Max Segment\n",
        "max_segment = None\n",
        "for line in open('experiments.conf'):\n",
        "    if line.startswith(model_name):\n",
        "        max_segment = True\n",
        "    elif line.strip().startswith(\"max_segment_len\"):\n",
        "        if max_segment:\n",
        "            max_segment = int(line.strip().split()[-1])\n",
        "            break\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=\"cased_config_vocab/vocab.txt\", do_lower_case=False)\n",
        "subtoken_num = 0\n",
        "for sent_num, line in enumerate(text):\n",
        "    raw_tokens = line.split()\n",
        "    tokens = tokenizer.tokenize(line)\n",
        "    if len(tokens) + len(data['sentences'][-1]) >= max_segment:\n",
        "        data['sentences'][-1].append(\"[SEP]\")\n",
        "        data['sentences'].append([\"[CLS]\"])\n",
        "        data['speakers'][-1].append(\"[SPL]\")\n",
        "        data['speakers'].append([\"[SPL]\"])\n",
        "        data['sentence_map'].append(sent_num - 1)\n",
        "        data['subtoken_map'].append(subtoken_num - 1)\n",
        "        data['sentence_map'].append(sent_num)\n",
        "        data['subtoken_map'].append(subtoken_num)\n",
        "\n",
        "    ctoken = raw_tokens[0]\n",
        "    cpos = 0\n",
        "    for token in tokens:\n",
        "        data['sentences'][-1].append(token)\n",
        "        data['speakers'][-1].append(\"-\")\n",
        "        data['sentence_map'].append(sent_num)\n",
        "        data['subtoken_map'].append(subtoken_num)\n",
        "        \n",
        "        if token.startswith(\"##\"):\n",
        "            token = token[2:]\n",
        "        if len(ctoken) == len(token):\n",
        "            subtoken_num += 1\n",
        "            cpos += 1\n",
        "            if cpos < len(raw_tokens):\n",
        "                ctoken = raw_tokens[cpos]\n",
        "        else:\n",
        "            ctoken = ctoken[len(token):]\n",
        "\n",
        "data['sentences'][-1].append(\"[SEP]\")\n",
        "data['speakers'][-1].append(\"[SPL]\")\n",
        "data['sentence_map'].append(sent_num - 1)\n",
        "data['subtoken_map'].append(subtoken_num - 1)\n",
        "\n",
        "with open(\"sample.in.json\", 'w') as out:\n",
        "    json.dump(data, out, sort_keys=True)\n",
        "\n",
        "! cat sample.in.json"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-aa053216a826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subtoken_map'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtoken_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mctoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mcpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuC9mlsWkTuP"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zniQuqNGQnOk",
        "outputId": "8d8f24cc-46fe-4318-9ad2-ee5f980f2ea3"
      },
      "source": [
        "! GPU=0 python predict.py $CHOSEN_MODEL sample.in.json sample.out.txt"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0504 19:16:09.621827 140043079042944 deprecation_wrapper.py:119] From /content/coref/coref_ops.py:11: The name tf.NotDifferentiable is deprecated. Please use tf.no_gradient instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  DeprecationWarning)\n",
            "W0504 19:16:09.687411 140043079042944 deprecation_wrapper.py:119] From /content/coref/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0504 19:16:10.990808 140043079042944 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Setting CUDA_VISIBLE_DEVICES to: 0\n",
            "Running experiment: spanbert_base\n",
            "data_dir = \"/sdb/data/new_coref\"\n",
            "model_type = \"independent\"\n",
            "max_top_antecedents = 50\n",
            "max_training_sentences = 3\n",
            "top_span_ratio = 0.4\n",
            "max_num_speakers = 20\n",
            "max_segment_len = 384\n",
            "bert_learning_rate = 2e-05\n",
            "task_learning_rate = 0.0001\n",
            "num_docs = 2802\n",
            "dropout_rate = 0.3\n",
            "ffnn_size = 3000\n",
            "ffnn_depth = 1\n",
            "num_epochs = 20\n",
            "feature_size = 20\n",
            "max_span_width = 30\n",
            "use_metadata = true\n",
            "use_features = true\n",
            "use_segment_distance = true\n",
            "model_heads = true\n",
            "coref_depth = 2\n",
            "coarse_to_fine = true\n",
            "fine_grained = true\n",
            "use_prior = true\n",
            "train_path = \"./train.english.384.jsonlines\"\n",
            "eval_path = \"./dev.english.384.jsonlines\"\n",
            "conll_eval_path = \"./dev.english.v4_gold_conll\"\n",
            "single_example = true\n",
            "genres = [\n",
            "  \"bc\"\n",
            "  \"bn\"\n",
            "  \"mz\"\n",
            "  \"nw\"\n",
            "  \"pt\"\n",
            "  \"tc\"\n",
            "  \"wb\"\n",
            "]\n",
            "eval_frequency = 1000\n",
            "report_frequency = 100\n",
            "log_root = \".\"\n",
            "adam_eps = 1e-06\n",
            "task_optimizer = \"adam\"\n",
            "bert_config_file = \"./spanbert_base/bert_config.json\"\n",
            "vocab_file = \"./spanbert_base/vocab.txt\"\n",
            "tf_checkpoint = \"./spanbert_base/model.max.ckpt\"\n",
            "init_checkpoint = \"./spanbert_base/model.max.ckpt\"\n",
            "log_dir = \"./spanbert_base\"\n",
            "W0504 19:16:11.069346 140043079042944 deprecation_wrapper.py:119] From /content/coref/bert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0504 19:16:11.161837 140043079042944 deprecation_wrapper.py:119] From /content/coref/independent.py:48: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0504 19:16:11.170141 140043079042944 deprecation_wrapper.py:119] From /content/coref/independent.py:50: The name tf.PaddingFIFOQueue is deprecated. Please use tf.queue.PaddingFIFOQueue instead.\n",
            "\n",
            "W0504 19:16:11.173970 140043079042944 deprecation.py:323] From /content/coref/bert/modeling.py:158: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0504 19:16:11.191356 140043079042944 deprecation_wrapper.py:119] From /content/coref/bert/modeling.py:175: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0504 19:16:11.191607 140043079042944 deprecation_wrapper.py:119] From /content/coref/bert/modeling.py:175: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0504 19:16:11.289899 140043079042944 deprecation.py:506] From /content/coref/bert/modeling.py:362: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0504 19:16:11.339846 140043079042944 deprecation.py:323] From /content/coref/bert/modeling.py:676: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0504 19:16:14.884306 140043079042944 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0504 19:16:14.986084 140043079042944 deprecation.py:323] From /content/coref/independent.py:221: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0504 19:16:15.059911 140043079042944 deprecation_wrapper.py:119] From /content/coref/util.py:117: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
            "\n",
            "W0504 19:16:15.816519 140043079042944 deprecation_wrapper.py:119] From /content/coref/independent.py:60: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "**** Trainable Variables ****\n",
            "  name = bert/embeddings/word_embeddings:0, shape = (28996, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = span_width_embeddings:0, shape = (30, 20), *INIT_FROM_CKPT*\n",
            "  name = mention_word_attn/output_weights:0, shape = (768, 1), *INIT_FROM_CKPT*\n",
            "  name = mention_word_attn/output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = mention_scores/hidden_weights_0:0, shape = (2324, 3000), *INIT_FROM_CKPT*\n",
            "  name = mention_scores/hidden_bias_0:0, shape = (3000,), *INIT_FROM_CKPT*\n",
            "  name = mention_scores/output_weights:0, shape = (3000, 1), *INIT_FROM_CKPT*\n",
            "  name = mention_scores/output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = span_width_prior_embeddings:0, shape = (30, 20), *INIT_FROM_CKPT*\n",
            "  name = width_scores/hidden_weights_0:0, shape = (20, 3000), *INIT_FROM_CKPT*\n",
            "  name = width_scores/hidden_bias_0:0, shape = (3000,), *INIT_FROM_CKPT*\n",
            "  name = width_scores/output_weights:0, shape = (3000, 1), *INIT_FROM_CKPT*\n",
            "  name = width_scores/output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = genre_embeddings:0, shape = (7, 20), *INIT_FROM_CKPT*\n",
            "  name = src_projection/output_weights:0, shape = (2324, 2324), *INIT_FROM_CKPT*\n",
            "  name = src_projection/output_bias:0, shape = (2324,), *INIT_FROM_CKPT*\n",
            "  name = antecedent_distance_emb:0, shape = (10, 20), *INIT_FROM_CKPT*\n",
            "  name = output_weights:0, shape = (20, 1), *INIT_FROM_CKPT*\n",
            "  name = output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/same_speaker_emb:0, shape = (2, 20), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/antecedent_distance_emb:0, shape = (10, 20), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/segment_distance/segment_distance_embeddings:0, shape = (3, 20), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/slow_antecedent_scores/hidden_weights_0:0, shape = (7052, 3000), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/slow_antecedent_scores/hidden_bias_0:0, shape = (3000,), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/slow_antecedent_scores/output_weights:0, shape = (3000, 1), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/slow_antecedent_scores/output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/f/output_weights:0, shape = (4648, 2324), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/f/output_bias:0, shape = (2324,), *INIT_FROM_CKPT*\n",
            "W0504 19:16:16.860119 140043079042944 deprecation_wrapper.py:119] From /content/coref/independent.py:74: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0504 19:16:16.867286 140043079042944 deprecation_wrapper.py:119] From /content/coref/optimization.py:13: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0504 19:16:16.872271 140043079042944 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0504 19:16:16.898278 140043079042944 deprecation_wrapper.py:119] From /content/coref/optimization.py:64: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "bert:task 199 27\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "2021-05-04 19:16:29.486322: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2021-05-04 19:16:29.498361: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-05-04 19:16:29.498439: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (4a158eb5595f): /proc/driver/nvidia/version does not exist\n",
            "2021-05-04 19:16:29.498912: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-05-04 19:16:29.505008: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200140000 Hz\n",
            "2021-05-04 19:16:29.505278: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5585a44e3640 executing computations on platform Host. Devices:\n",
            "2021-05-04 19:16:29.505324: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "Restoring from ./spanbert_base/model.max.ckpt\n",
            "2021-05-04 19:16:36.277994: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "W0504 19:16:41.284320 140043079042944 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Decoded 1 examples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI6OkNG-kXIQ"
      },
      "source": [
        "## Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXpA3sW5QpSA",
        "outputId": "7bfd9052-a5c1-421b-8277-8aff92f26027"
      },
      "source": [
        "output = json.load(open(\"sample.out.txt\"))\n",
        "\n",
        "comb_text = [word for sentence in output['sentences'] for word in sentence]\n",
        "\n",
        "def convert_mention(mention):\n",
        "    start = output['subtoken_map'][mention[0]]\n",
        "    end = output['subtoken_map'][mention[1]] + 1\n",
        "    nmention = (start, end)\n",
        "    mtext = ''.join(' '.join(comb_text[mention[0]:mention[1]+1]).split(\" ##\"))\n",
        "    return (nmention, mtext)\n",
        "\n",
        "seen = set()\n",
        "print('Clusters:')\n",
        "for cluster in output['predicted_clusters']:\n",
        "    mapped = []\n",
        "    for mention in cluster:\n",
        "        seen.add(tuple(mention))\n",
        "        mapped.append(convert_mention(mention))\n",
        "    print(mapped, end=\",\\n\")\n",
        "\n",
        "print('\\nMentions:')\n",
        "for mention in output['top_spans']:\n",
        "    if tuple(mention) in seen:\n",
        "        continue\n",
        "    print(convert_mention(mention), end=\",\\n\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clusters:\n",
            "[((15, 20), 'writer and director Joss Whedon'), ((21, 22), 'his'), ((26, 27), 'Whedon'), ((78, 79), 'Whedon'), ((170, 171), \"Whedon ' s\"), ((304, 305), 'Whedon')],\n",
            "[((2, 9), 'an American space Western drama television series'), ((36, 38), 'The series'), ((80, 82), 'the show'), ((96, 98), 'The show'), ((195, 196), 'Firefly'), ((210, 211), 'Firefly'), ((224, 225), 'It'), ((243, 245), 'the series'), ((245, 246), 'it'), ((250, 251), 'it'), ((261, 262), 'It'), ((280, 282), 'the series'), ((301, 303), 'the show'), ((320, 322), 'the series'), ((324, 325), 'Firefly')],\n",
            "[((63, 67), \"Serenity , a ' Firefly - class ' spaceship\"), ((77, 78), 'Serenity')],\n",
            "[((50, 54), 'a new star system'), ((134, 137), 'their star system')],\n",
            "[((12, 13), '2002'), ((207, 208), '2002')],\n",
            "[((12, 13), '2003'), ((268, 269), '2003')],\n",
            "[((277, 279), 'TV Guide'), ((286, 287), 'their')],\n",
            "\n",
            "Mentions:\n",
            "((0, 1), 'Firefly'),\n",
            "((0, 2), 'Firefly is'),\n",
            "((2, 13), 'an American space Western drama television series which ran from 2002 - 2003'),\n",
            "((10, 11), 'ran'),\n",
            "((12, 13), '2002 - 2003'),\n",
            "((13, 14), 'created'),\n",
            "((15, 26), 'writer and director Joss Whedon , under his Mutant Enemy Productions label'),\n",
            "((21, 25), 'his Mutant Enemy Productions'),\n",
            "((21, 26), 'his Mutant Enemy Productions label'),\n",
            "((26, 36), 'Whedon served as an executive producer , along with Tim Minear'),\n",
            "((27, 28), 'served'),\n",
            "((34, 36), 'Tim Minear'),\n",
            "((36, 54), 'The series is set in the year 2517 , after the arrival of humans in a new star system'),\n",
            "((38, 39), 'is'),\n",
            "((39, 40), 'set'),\n",
            "((41, 44), 'the year 2517'),\n",
            "((41, 54), 'the year 2517 , after the arrival of humans in a new star system'),\n",
            "((45, 49), 'the arrival of humans'),\n",
            "((45, 54), 'the arrival of humans in a new star system'),\n",
            "((48, 49), 'humans'),\n",
            "((54, 56), 'and follows'),\n",
            "((55, 56), 'follows'),\n",
            "((56, 67), \"the adventures of the renegade crew of Serenity , a ' Firefly - class ' spaceship\"),\n",
            "((59, 67), \"the renegade crew of Serenity , a ' Firefly - class ' spaceship\"),\n",
            "((65, 66), 'Firefly'),\n",
            "((65, 66), 'Firefly -'),\n",
            "((65, 66), 'Firefly - class'),\n",
            "((65, 66), \"Firefly - class '\"),\n",
            "((65, 67), \"Firefly - class ' spaces\"),\n",
            "((67, 70), 'The ensemble cast'),\n",
            "((67, 78), 'The ensemble cast portrays the nine characters who live on Serenity'),\n",
            "((70, 71), 'portrays'),\n",
            "((71, 78), 'the nine characters who live on Serenity'),\n",
            "((78, 96), \"Whedon pitched the show as ' nine people looking into the blackness of space and seeing nine different things\"),\n",
            "((79, 80), 'pitched'),\n",
            "((80, 96), \"the show as ' nine people looking into the blackness of space and seeing nine different things\"),\n",
            "((83, 96), 'nine people looking into the blackness of space and seeing nine different things'),\n",
            "((87, 91), 'the blackness of space'),\n",
            "((90, 91), 'space'),\n",
            "((92, 93), 'seeing'),\n",
            "((93, 96), 'nine different things'),\n",
            "((95, 96), \"'\"),\n",
            "((98, 99), 'explores'),\n",
            "((106, 118), 'some of whom fought on the losing side of a civil war'),\n",
            "((111, 118), 'the losing side of a civil war'),\n",
            "((115, 118), 'a civil war'),\n",
            "((119, 120), 'make'),\n",
            "((123, 127), 'the fringes of society'),\n",
            "((126, 127), 'society'),\n",
            "((130, 137), 'the pioneer culture of their star system'),\n",
            "((134, 135), 'their'),\n",
            "((138, 140), 'this future'),\n",
            "((138, 150), 'this future , the only two surviving superpowers , the United States and China ,'),\n",
            "((140, 148), 'the only two surviving superpowers , the United States'),\n",
            "((140, 150), 'the only two surviving superpowers , the United States and China'),\n",
            "((140, 150), 'the only two surviving superpowers , the United States and China ,'),\n",
            "((145, 148), 'the United States'),\n",
            "((149, 150), 'China'),\n",
            "((150, 151), 'fused'),\n",
            "((150, 152), 'fused to'),\n",
            "((150, 153), 'fused to form'),\n",
            "((150, 160), 'fused to form the central federal government , called the Alliance'),\n",
            "((150, 160), 'fused to form the central federal government , called the Alliance ,'),\n",
            "((150, 161), 'fused to form the central federal government , called the Alliance , resulting'),\n",
            "((150, 162), 'fused to form the central federal government , called the Alliance , resulting in'),\n",
            "((150, 168), 'fused to form the central federal government , called the Alliance , resulting in the fusion of the two cultures'),\n",
            "((150, 168), 'fused to form the central federal government , called the Alliance , resulting in the fusion of the two cultures .'),\n",
            "((152, 153), 'form'),\n",
            "((153, 160), 'the central federal government , called the Alliance'),\n",
            "((162, 168), 'the fusion of the two cultures'),\n",
            "((165, 168), 'the two cultures'),\n",
            "((170, 172), \"Whedon ' s vision\"),\n",
            "((172, 173), \"'\"),\n",
            "((172, 195), \"' nothing will change in the future : technology will advance , but we will still have the same political , moral , and ethical problems as today\"),\n",
            "((176, 178), 'the future'),\n",
            "((176, 195), 'the future : technology will advance , but we will still have the same political , moral , and ethical problems as today'),\n",
            "((178, 179), 'technology'),\n",
            "((180, 181), 'advance'),\n",
            "((182, 183), 'we'),\n",
            "((182, 195), 'we will still have the same political , moral , and ethical problems as today'),\n",
            "((184, 186), 'still have'),\n",
            "((185, 186), 'have'),\n",
            "((186, 195), 'the same political , moral , and ethical problems as today'),\n",
            "((194, 195), 'today'),\n",
            "((195, 197), 'Firefly premiered'),\n",
            "((195, 200), 'Firefly premiered in the U . S .'),\n",
            "((195, 204), 'Firefly premiered in the U . S . on the Fox network'),\n",
            "((195, 208), 'Firefly premiered in the U . S . on the Fox network on September 20 , 2002'),\n",
            "((195, 208), 'Firefly premiered in the U . S . on the Fox network on September 20 , 2002 .'),\n",
            "((196, 197), 'premiered'),\n",
            "((198, 200), 'the U . S .'),\n",
            "((201, 204), 'the Fox network'),\n",
            "((205, 208), 'September 20 , 2002'),\n",
            "((210, 218), 'Firefly had averaged 4 . 7 million viewers per episode'),\n",
            "((212, 213), 'averaged'),\n",
            "((212, 218), 'averaged 4 . 7 million viewers per episode'),\n",
            "((213, 218), '4 . 7 million viewers per episode'),\n",
            "((217, 218), 'episode'),\n",
            "((222, 224), 'Nielsen ratings'),\n",
            "((224, 227), 'It was canceled'),\n",
            "((225, 227), 'was canceled'),\n",
            "((226, 227), 'canceled'),\n",
            "((228, 234), '11 of the 14 produced episodes'),\n",
            "((228, 236), '11 of the 14 produced episodes were aired'),\n",
            "((230, 234), 'the 14 produced episodes'),\n",
            "((234, 236), 'were aired'),\n",
            "((235, 236), 'aired'),\n",
            "((237, 245), 'the relatively short life span of the series'),\n",
            "((237, 255), 'the relatively short life span of the series , it received strong sales when it was released on DVD'),\n",
            "((245, 255), 'it received strong sales when it was released on DVD'),\n",
            "((250, 255), 'it was released on DVD'),\n",
            "((252, 253), 'released'),\n",
            "((254, 255), 'DVD'),\n",
            "((256, 257), 'has'),\n",
            "((257, 261), 'large fan support campaigns'),\n",
            "((261, 277), 'It won a Primetime Emmy Award in 2003 for Outstanding Special Visual Effects for a Series'),\n",
            "((262, 263), 'won'),\n",
            "((263, 267), 'a Primetime Emmy Award'),\n",
            "((263, 269), 'a Primetime Emmy Award in 2003'),\n",
            "((263, 277), 'a Primetime Emmy Award in 2003 for Outstanding Special Visual Effects for a Series'),\n",
            "((270, 277), 'Outstanding Special Visual Effects for a Series'),\n",
            "((275, 277), 'a Series'),\n",
            "((277, 297), \"TV Guide ranked the series at No . 5 on their 2013 list of 60 shows that were ' Cancelled Too Soon\"),\n",
            "((279, 280), 'ranked'),\n",
            "((280, 297), \"the series at No . 5 on their 2013 list of 60 shows that were ' Cancelled Too Soon\"),\n",
            "((286, 288), 'their 2013'),\n",
            "((286, 297), \"their 2013 list of 60 shows that were ' Cancelled Too Soon\"),\n",
            "((290, 297), \"60 shows that were ' Cancelled Too Soon\"),\n",
            "((296, 297), \"'\"),\n",
            "((301, 322), 'the show led Whedon and Universal Pictures to produce Serenity , a 2005 film which continues from the story of the series'),\n",
            "((304, 308), 'Whedon and Universal Pictures'),\n",
            "((304, 322), 'Whedon and Universal Pictures to produce Serenity , a 2005 film which continues from the story of the series'),\n",
            "((306, 308), 'Universal Pictures'),\n",
            "((309, 310), 'produce'),\n",
            "((310, 311), 'Serenity'),\n",
            "((310, 322), 'Serenity , a 2005 film which continues from the story of the series'),\n",
            "((312, 313), '2005'),\n",
            "((317, 322), 'the story of the series'),\n",
            "((323, 326), 'the Firefly franchise'),\n",
            "((323, 336), 'the Firefly franchise expanded to other media , including comics and a role - playing game'),\n",
            "((326, 327), 'expanded'),\n",
            "((328, 336), 'other media , including comics and a role - playing game'),\n",
            "((331, 332), 'comics'),\n",
            "((333, 336), 'a role - playing game'),\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}