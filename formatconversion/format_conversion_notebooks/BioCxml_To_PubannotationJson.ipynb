{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert BioCxml to Pubannotation Json\n",
    "## April 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         pmid        type   id       entity_type  \\\n",
      "0    31991541  annotation   A2       Virus_other   \n",
      "1    31991541  annotation   A3  Virus_SARS-CoV-2   \n",
      "2    31991541  annotation   A4  Virus_SARS-CoV-2   \n",
      "3    31991541  annotation   A5      Virus_family   \n",
      "4    31991541  annotation   A1  Virus_SARS-CoV-2   \n",
      "..        ...         ...  ...               ...   \n",
      "150  31996494  annotation  A60  Virus_SARS-CoV-2   \n",
      "151  31996494  annotation  A61      Virus_family   \n",
      "152  31996494  annotation  A62      Virus_family   \n",
      "153  31996494  annotation  A63  Virus_SARS-CoV-2   \n",
      "154  31996494  annotation  A64  Virus_SARS-CoV-2   \n",
      "\n",
      "                                          text location/reference  \\\n",
      "0                                     SARS-CoV              114:8   \n",
      "1    most recent emergent group 2B coronavirus             293:41   \n",
      "2                                    2019-nCoV               27:9   \n",
      "3                                  Coronavirus              14:11   \n",
      "4                            novel coronavirus              57:17   \n",
      "..                                         ...                ...   \n",
      "150                                  2019-nCoV              410:9   \n",
      "151                              coronaviruses             719:13   \n",
      "152                 human pathogen coronavirus             792:26   \n",
      "153                                  2019-nCoV             1160:9   \n",
      "154                                 2019- nCoV            1223:10   \n",
      "\n",
      "                 infon  \n",
      "0    NCBI:txid:694009   \n",
      "1    NCBI:txid:2697049  \n",
      "2    NCBI:txid:2697049  \n",
      "3     NCBI:txid:693995  \n",
      "4    NCBI:txid:2697049  \n",
      "..                 ...  \n",
      "150  NCBI:txid:2697049  \n",
      "151   NCBI:txid:693995  \n",
      "152                NaN  \n",
      "153  NCBI:txid:2697049  \n",
      "154  NCBI:txid:2697049  \n",
      "\n",
      "[155 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv ('../corona/manuscript/Supplemental_file6.csv')\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import sys\n",
    "import codecs\n",
    "from xml.dom.minidom import parse\n",
    "import xml.dom.minidom\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "with codecs.open(\"../corona/manuscript/Supplemental_file4.xml\", \"r\", \"utf-8\") as file:##gold standard\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, Cord19 papers are identified by \"sha\" and \"cord_id\", I extracted those columns from \"supplementary_file\" for gold papers.\n",
    "One of the papers did not exist at all in cord19 papers and one doesnot have neither \"pmcid\" nor \"sha\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm_cid = ['nan', 'PMC7003341', 'PMC7033720', 'PMC7159299', 'PMC7110798', 'PMC7094943', 'PMC6988272', 'PMC7054940', 'PMC7077245']\n",
      "pub_id =['32013309', '32029004', '32020836', '31986264', '32007643', '32015508', '31992388', '32036774', '31991541'] \n",
      "cord_id = ['0lsniwyv', 'hmy8fs3g', 'ea99j2qn', '0nhgxoim', 'drqnrwdl', 'qele28zk', '0rxtati9', '1qkwsh6a', 't8s4s0wo'] \n",
      "sha = ['nan', 'fd28e6d03eef27b0454f13ca539dc1498242a4c2', '52dfa9fbd6dcf7c51fac5cb85a88fb154506a722', 'ea646b114ea01f978d205ed97405e1f2afed2dfd', '637939a0f462b9e821ff62fc20e9fedfe78df73e', '5734e3b81e16fe1976a129c5a0872716f3dd50b8', '28223ad437aa22ac2285bd9dd775e1415a69411a', '7bc69471836fbf9cf28e57926d95fc48e6f45964', '253cfd411f93ef88f702accd0fa195f24d1d2925'] \n"
     ]
    }
   ],
   "source": [
    "pmc_id = []\n",
    "pub_id = []\n",
    "cord_id = []\n",
    "sha = []\n",
    "\n",
    "with open(\"../files/gold_standard_corpus/gold_corpus_ids.txt\",'r') as f:\n",
    "    first_line = f.readline()\n",
    "    for line in f:\n",
    "        #print(line)\n",
    "        pmc_id.append(line.strip().split(',')[2].replace('.xml.json',''))\n",
    "        pub_id.append(str(int(float(line.strip().split(',')[3]))))\n",
    "        cord_id.append(line.strip().split(',')[0])\n",
    "        sha.append(line.strip().split(',')[1].split(';')[0])\n",
    "        \n",
    "print(\"pm_cid = {}\".format(pmc_id))\n",
    "print(\"pub_id ={} \".format(pub_id))\n",
    "print(\"cord_id = {} \".format(cord_id))\n",
    "print(\"sha = {} \".format(sha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to extract title and abstract of the gold papers from kaggle database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_id: PMC7003341\n",
      "paper_id: PMC7033720\n",
      "paper_id: PMC7159299\n",
      "paper_id: PMC7110798\n",
      "paper_id: PMC7094943\n",
      "paper_id: PMC6988272\n",
      "paper_id: PMC7054940\n",
      "paper_id: PMC7077245\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for file in sha:\n",
    "    if file!='nan':\n",
    "        data = json.load(open(\"../Kaggle_data/gold_corpus_json/\"+file+'.json'))\n",
    "        jtopy=json.dumps(data,indent=4) #json.dumps take a dictionary as input and returns a string as output.\n",
    "        dict_json=json.loads(jtopy) # json.loads take a string as input and returns a dictionary as output.\n",
    "        index_sh = sha.index(dict_json[\"paper_id\"])\n",
    "        if 'abstract'in dict_json.keys():\n",
    "            print(\"paper_id:\",pmc_id[index_sh])\n",
    "            #print(\"title:\",dict_json[\"metadata\"][\"title\"])\n",
    "            #print(\"abstract: \", dict_json[\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PMC7077245', 'PMC7077245', '32013309', '32013309', 'PMC7110798', 'PMC7110798', 'PMC7054940', 'PMC7054940', 'PMC6988272', 'PMC6988272', 'PMC7033720', 'PMC7033720', 'PMC7094943', 'PMC7094943', 'PMC7159299', 'PMC7159299', 'PMC7003341', 'PMC7003341', '31996494', '31996494']\n",
      "['PMC', 'PMC', 'PubMed', 'PubMed', 'PMC', 'PMC', 'PMC', 'PMC', 'PMC', 'PMC', 'PMC', 'PMC', 'PMC', 'PMC', 'PMC', 'PMC', 'PMC', 'PMC', 'PubMed', 'PubMed']\n",
      "['t8s4s0wo', 't8s4s0wo', '0lsniwyv', '0lsniwyv', 'drqnrwdl', 'drqnrwdl', '1qkwsh6a', '1qkwsh6a', '0rxtati9', '0rxtati9', 'ea99j2qn', 'ea99j2qn', 'qele28zk', 'qele28zk', '0nhgxoim', '0nhgxoim', 'hmy8fs3g', 'hmy8fs3g', ' ', ' ']\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "from xml.etree import ElementTree\n",
    "\n",
    "with open('../corona/manuscript/Supplemental_file4.xml', 'rt') as f:\n",
    "    tree = ElementTree.parse(f)\n",
    "\n",
    "source_id = []\n",
    "source_db = []\n",
    "text = []\n",
    "denotation_array = []\n",
    "annotation_id = []\n",
    "begin = []\n",
    "end = []\n",
    "\n",
    "\n",
    "for node in tree.iter():\n",
    "\n",
    "    if node.tag == \"id\":\n",
    "        source_id.append(node.text)\n",
    "        source_id.append(node.text)\n",
    "    if node.tag ==\"infon\" and node.attrib['key']=='BQ_FROM':\n",
    "        source_db.append(node.text)\n",
    "        source_db.append(node.text)\n",
    "    if node.tag == 'annotation':\n",
    "        annotation_id.append(node.attrib.get('id'))\n",
    "    if node.tag=='location':\n",
    "        span_begin = int(node.attrib.get('offset'))\n",
    "        span_end   = int(span_begin) + int(node.attrib.get('length'))\n",
    "        begin.append(span_begin)\n",
    "        end.append(span_end)\n",
    "\n",
    "###############################################################################\n",
    "#print(text)\n",
    "cord_uid = []\n",
    "pmc_source_id = source_id\n",
    "for pb in source_id:\n",
    "    if pb in pub_id:\n",
    "        index_s =  source_id.index(pb)\n",
    "        index_p =  pub_id.index(pb)\n",
    "        cord_uid.append(cord_id[index_p])\n",
    "        if pmc_id[index_p] !='nan':\n",
    "            pmc_source_id[index_s] = pmc_id[index_p]\n",
    "            source_db[index_s] = \"PMC\"\n",
    "        \n",
    "    else:\n",
    "        cord_uid.append(\" \")\n",
    "        \n",
    "print(source_id)\n",
    "print(source_db)\n",
    "print(cord_uid)\n",
    "        \n",
    "#############################################################################\n",
    "##This will write two json objects of title and abstract in one json file in Pubannotation format \n",
    "\n",
    "print('done!')    \n",
    "\n",
    "ann_id_ctr = 0 \n",
    "general_ctr = 0\n",
    "documents = soup.find_all('document')\n",
    "    #print(documents[1])\n",
    "for i in documents:\n",
    "    passages = i.find_all('passage')\n",
    "    for k in passages:\n",
    "        offset = int(k.find('offset').text)\n",
    "        passage_text = k.find('text').text\n",
    "        #print(passage_text)\n",
    "        objects = k.find_all('annotation')\n",
    "        denotation_array = []\n",
    "        for obj in objects:\n",
    "            obj_id = annotation_id[ann_id_ctr]\n",
    "            info = obj.find_all('infon')\n",
    "            for ann_info in info:\n",
    "                if \"key=\\\"type\\\"\" in str(ann_info):\n",
    "                    obj_description = str(ann_info.text)#+':'\n",
    "                   # if \"NCBI:txid\" in str(ann_info):\n",
    "                   #    obj_description = obj_description + str(ann_info.text)\n",
    "            span_begin = begin[ann_id_ctr]-offset\n",
    "            span_end   = end[ann_id_ctr]-offset\n",
    "            ann_id_ctr = ann_id_ctr+1\n",
    "            denotation_array.append({\"id\": obj_id, \"span\": {\"begin\": span_begin, \"end\": span_end}, \"obj\": obj_description})                                \n",
    "        if general_ctr<20:\n",
    "            with open('../files/gold_standard_corpus/'+source_id[general_ctr]+\".json\",'a') as out_file:\n",
    "                #print(passage_text)\n",
    "                tmp_dict = {\n",
    "                    \"cord_uid\": cord_uid[general_ctr],\n",
    "                    \"sourcedb\": source_db[general_ctr],\n",
    "                    \"sourceid\": pmc_source_id[general_ctr],\n",
    "                    \"text\": passage_text, # either abstract or title\n",
    "                    \"denotations\": denotation_array  # all annotations\n",
    "                    }\n",
    "                out_file.write(json.dumps(tmp_dict,indent=4))\n",
    "                out_file.close()\n",
    "                general_ctr = general_ctr + 1\n",
    "                #print(general_ctr)\n",
    "                \n",
    "        #\n",
    "                                         \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with codecs.open(\"../files/Updated_Gold_std.json\", \"r\", \"utf-8\") as file:\n",
    "    soupjson = BeautifulSoup(file, \"html.parser\")\n",
    "#print(soupjson.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "JsonList = []\n",
    "#print(\"Started Reading JSON file which contains multiple JSON document\")\n",
    "with open('../files/Updated_Gold_std.json') as json_file:\n",
    "       for j in json_file:\n",
    "            x = json.loads(j)\n",
    "            JsonList.append(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert gold standard papers to Cord19 format\n",
    "### 29 of April"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "JsonList = []\n",
    "with open('Updated_gold_standard.json', 'rt') as j_f:\n",
    "    for j in j_f:\n",
    "        x = json.loads(j)\n",
    "        JsonList.append(x)\n",
    "        \n",
    "    for j_ctr in range(len(JsonList)):\n",
    "        if not j_ctr%2:\n",
    "            paper_id = JsonList[j_ctr][\"sourceid\"]\n",
    "            paper_title = JsonList[j_ctr][\"text\"]\n",
    "        else:\n",
    "            paper_abstract = JsonList[j_ctr][\"text\"]\n",
    "            with open (\"../Kaggle_data/gold_corpus_json/gold_papers/\"+paper_id+\".json\",'w') as f:\n",
    "                temp_dict = {\n",
    "                    \"paper_id\" : paper_id,\n",
    "                    \"metadata\" : {\n",
    "                        \"title\":paper_title,\n",
    "                        \"authors\":[]\n",
    "                    },\n",
    "                    \"abstract\":[\n",
    "                       { \n",
    "                        \"text\":paper_abstract,\n",
    "                        \"cite_spans\": [],\n",
    "                        \"ref_spans\": [],\n",
    "                        \"section\": \"Abstract\"\n",
    "\n",
    "                       }\n",
    "                    ],\n",
    "                    \"body_text\": []\n",
    "                     \n",
    "                }\n",
    "                \n",
    "                f.write(json.dumps(temp_dict,indent=4))\n",
    "                f.close()\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
