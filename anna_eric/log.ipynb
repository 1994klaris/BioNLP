{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log for project in Computer Science\n",
    "\n",
    "By Anna and Eric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1, 4/4 - 2019\n",
    "\n",
    "Today the goal is to understand the project and be prepared for mondays meeting.\n",
    "\n",
    "Step \n",
    "\n",
    "     1) Download UniProt and check out the dataset\n",
    "\n",
    "     2) Create the dictionary\n",
    "     \n",
    "     3) Get tool from Marcus\n",
    "     \n",
    "     4) Train a tagger with machine learning\n",
    "     \n",
    "So we would like to finish step 1 and start reseach on 2.\n",
    "\n",
    "---\n",
    "Today we sat from 13-16. We started by creating a private git repo with this file in it. Then we downloaded the uniprot dataset in xml form. We proceded to try and parse it with BeautifulSoup. The file was too big for that method.\n",
    "\n",
    "After that we tried with ElementTree instead which can go through each element one by one. This worked and we eventually got a list of proteins (over 200 000).\n",
    "\n",
    "We got stuck at which dictionary we want to create. Do we want one that's protein -> gene1 gene2 gene3 and thus also mine genes from the xml file. Or is the dictionary something else? For next time this needs to be cleared up in order to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "#in protein_names.txt\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'shortName' in tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "f = open('protein_names_short.txt', 'w')\n",
    "f.write('\\n'.join(proteins))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 2, 8/4 - 2019\n",
    "\n",
    "Today we had our second meeting with Pierre and Sonja. We got clarification about the biology bit of the project but also some tips going forward. Now we have a Slack and are to get an invite to the Github repo. Also, an invite to Endnote.\n",
    "\n",
    "TODO: Invite Sonja, Pierre to Overleaf document\n",
    "\n",
    "Tips:\n",
    "\n",
    "    1) Search PubMed for 'biomedical text-mining' and 'bio NLP' (Jensen)\n",
    "    \n",
    "    2) Get annotation from Hannes\n",
    "\n",
    "So this week we wish to get the infrastructure up and running.\n",
    "\n",
    "---\n",
    "We started by looking at Marcus email and getting his program to work. After a lot of problems with installation of the various packages and programs we finally got it to work, almost 2h later.\n",
    "\n",
    "Question: What is the file 'disease_index.fst' and how is it created? Should we create this?\n",
    "\n",
    "---\n",
    "We now have the basic infrastructure up and running. We have done some tests on running it with our dataset of proteins from UniProt. Also we started looking at BioCreative's datasets, but we'll leave that for next time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions to start JVM (need Marcus repo):\n",
    "\n",
    "    1) cd [katalog]/mention-index-py4j\n",
    "\n",
    "    2) mvn package\n",
    "\n",
    "    3) cd target\n",
    "\n",
    "    4) java -jar mentions-index-py4j-1.0-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install docria\n",
    "#!{sys.executable} -m pip install py4j\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    # Here we do the py4j equivalent for new java.io.File(path)\n",
    "    return jvm.java.io.File(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app.buildIndex(get_java_file(\"protein_name/protein_names_long.txt\"), get_java_file(\"protein_name/protein_names_long.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_long.fst\"))\n",
    "\n",
    "doc = Document()\n",
    "doc.add_text(\"main\",\"Peroxydase reaction stains were negative, chloroacetate esterase were strongly positive.\")\n",
    "binary_doc = MsgpackCodec.encode(doc) \n",
    "search_binary_doc = app.search(indx, binary_doc)\n",
    "doc = MsgpackCodec.decode(search_binary_doc)\n",
    "\n",
    "tuls = []\n",
    "for term in doc[\"matches\"]:\n",
    "    tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    #print(str(term[\"text\"]), \"-\", \"%d:%d\" % (term[\"text\"].start, term[\"text\"].stop))\n",
    "print(tuls)\n",
    "dominant_right(tuls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 3, 9/4-2019\n",
    "We have had some questions about the purpose and think we have finally figured it out. To identify the entities 'protein' and 'diseases' like cell death. To start with we will identify proteins by checking a pubmed article and using our uniprot list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Goes through the file pubmed19n0651.xml and saves the abstract text as one textfile\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='pubmed19n0651.xml'\n",
    "out_name = 'abstract_text19n0651.txt'\n",
    "parser = ET.iterparse(filename)\n",
    "f=open(out_name, \"a+\", encoding=\"utf-8\")\n",
    "for event, element in parser:\n",
    "    if element.tag == 'AbstractText':\n",
    "        if element.text:\n",
    "            f.write(element.text)\n",
    "    element.clear()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Prints all occurences of proteins in the abstract file.\n",
    "app.buildIndex(get_java_file(\"protein_names_long.txt\"), get_java_file(\"protein_names_long.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"protein_names_long.fst\"))\n",
    "data = ''\n",
    "with open('abstract_text19n0651.txt', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('.')\n",
    "for s in sp:\n",
    "    hits = indx.search(s)\n",
    "    for hit in hits:\n",
    "        print(hit.start, hit.end, \" \".join([term.raw for term in hit.terms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prints all occurences of cell deaths in the abstract file.\n",
    "app.buildIndex(get_java_file(\"cell_death_names.txt\"), get_java_file(\"cell_name_index.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"cell_name_index.fst\"))\n",
    "data = ''\n",
    "with open('abstract_text19n0651.txt', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('.')\n",
    "for s in sp:\n",
    "    hits = indx.search(s)\n",
    "    for hit in hits:\n",
    "        print(hit.start, hit.end, \" \".join([term.raw for term in hit.terms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tried to use the model that https://aclweb.org/anthology/W16-5104 implemented at github: https://github.com/withtwist/medical-ner. After transpiling the code to Python 2, changing the encoding of the data etc. we still got compilation errors. It seemed too big of a job so we scraped it. Since we have been able to identify proteins from the abstract and where they are located, we are now interested in finding a way to create the tracer with machine learning. We found the article *Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts. \n",
    "\n",
    "\n",
    "Note: We noticed that the indx.search is case sensitive, which needs to be fixed. (e.g. toLowerCase or similar). Check w Marcus!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion of todays work: We have fixed the dictionaries and spent some time making sure the xml reader for uniprot included the edgecases and included 'shortName'. We also started tinkering with neural networks by downloading existing repos but we could not make it work. We started reading a couple of papers to get inspiration for our tagger. \n",
    "\n",
    "For next time, we will look deeper into the neural networks and use the tips Marcus sent us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 4, 14/4-2019\n",
    "We prepared for tomorrows meeting by looking at Marcus tips that he sent us. Also we started looking at more journals and papers and found a really good one: [A Novel Approach for Protein-Named Entity Recognition and Protein-Protein Interaction Extraction](https://www.hindawi.com/journals/mpe/2015/942435/). They describe step-by-step a method with SVM to extract the entities. If they agree on tomorrows meeting we will follow this guide next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 5, 15/4-2019\n",
    "Today after the meeting we have a clearer understanding how to move forward. The process follows:\n",
    "    1. Add list of corpus with summarization of each content and size to github\n",
    "    2. [x] Fix java to ignore case\n",
    "    3. [x] Complete a working model with the dictionary model\n",
    "        i. [x] ~~Fix reader and use Element tree instead.~~ Also read in 'pmid' and change output to 'pmid \\t abstract_text \\n'.\n",
    "        ii. [x] Match output to corpus annotation\n",
    "    4. [ ] Create evaluator for dictionary model\n",
    "        i. Use [genetag](https://www.ncbi.nlm.nih.gov/pubmed/15960837) for evaluation or chemner? or both?\n",
    "        ii. Compare with e.g recall\n",
    "    5. Add more to dictionary and run evaluator to see if it improves. If ambigueties use dominant write.\n",
    "    \n",
    "After this is completed we will start on the machine learning model.\n",
    "    1. Use POS? Pre-processing?\n",
    "    2. Sentence -> Tokens -> Tagger -> Phrases -> ML (BioLSTM and Bioascembedding(spelling?))\n",
    "    3. Check out page [nlpprogress](http://nlpprogress.com/english/named_entity_recognition.html) for NER datasets\n",
    "   \n",
    "\n",
    "Möte med Sonja: https://biocreative.bioinformatics.udel.edu/resources/corpora/biocreative-iii-corpus/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indx = app.loadIndex(get_java_file(\"protein_names_long.fst\"))\n",
    "\n",
    "data = ''\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    #If it is an excerpt\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ''.join(line.split(': ')[1:])\n",
    "        dictionary[id] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use indx.search to find all occurrences of proteins in abstracts from genetag. \n",
    "indx = app.loadIndex(get_java_file(\"protein_name_index.fst\"))\n",
    "\n",
    "d = dictionary\n",
    "out = []\n",
    "for key in d:\n",
    "    value = d[key]\n",
    "    hits = indx.search(value)    \n",
    "    for hit in hits:\n",
    "        # term.raw for term in hit.terms prints lists of protein names. Each list is protein name\n",
    "        out.append('|'.join([key, str(hit.start) + ' ' + str(hit.end), \" \".join([term.raw for term in hit.terms])]))\n",
    "file = open('genetag.out', 'w+', encoding=\"utf-8\", errors='ignore')\n",
    "file.write('\\n'.join(out))\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "Today we sat for quite some time with merge conflicts in jupyter notebooks as it would not compile after a while. Also we tried to implement 2 models for evaluation; genetag with perl and another one with python 2.5. The gentag could have worked, but we got such a low score that it probably checks line by line simmilarity or something like it. This means we only got 8 correct matches which seems very low. We will look further into this next time.\n",
    "\n",
    "We succesfully changed the tokenizers in java by removing 2 of them and adding one to ignore cases. We became aware of how many small words were in the 'shortName' tag from chemprot. So we got protein names like 'AM, OR, AND' etc. We thus created a textfile 'protein_names_long' which contains only the long names. This meant we got a more expected result, but we are also missing out on some of the real names. TODO: Think about how to include the short names - do we want to set a limit of min 2 characters?\n",
    "\n",
    "Finally we created an out file to match the evaluation file in genetag. It reads from the database and using our proteinnames builds the same style file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 6, 17/4-2019\n",
    "We started by creating another file 'protein_names_short', and discussed whether to use this in combination with long names or only long names. For now we will only use long.\n",
    "\n",
    "Today's goal is to create/find/adapt an evaluator that works on our data.\n",
    "\n",
    "We can run the perl evaluator from genetag and get the following result:\n",
    "```\n",
    "TP: 2\n",
    "FP: 18211\n",
    "FN: 406\n",
    "Precision: 0.000109811672980838\n",
    "Recall: 0.00490196078431373\n",
    "```\n",
    "Which is very bad. we think this is because the java tokenizer changes words like \"h-55\" to \"h - 55\". So it matches both 'h' and '55'. We need to change such that the hyphen does not get split in this way. There are probably more errors as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "p_type = 'long'\n",
    "\n",
    "#app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(p_type)), get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "#indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    #If it is an excerpt\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        dictionary[id] = text\n",
    "    elif lines[0] == 'ANNOTATION':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        if 'ALTGENE' in lines[1]:\n",
    "            continue\n",
    "        for line in lines[2:]:\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "print(len(proteins))\n",
    "with open('protein_name/protein_names_{}.txt'.format(p_type), 'a', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(proteins))\n",
    "\n",
    "app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(p_type)), get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import re\n",
    "d = dictionary\n",
    "out = []\n",
    "\n",
    "for key in d:\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", d[key])    \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    print(tuls)    \n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    \n",
    "    for m in matches:\n",
    "        #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "            out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "file = open('genetag/genetag_{}.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore')\n",
    "file.write('\\n'.join(out))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We chatted with Marcus for quite a bit today. As we wanted to add dominant_right we first tried to do it in Java but after a lot of trial-and-error it did not work and we switched to the python version. We also had to switch the way wee send in data as we needed to use docria instead of python.\n",
    "\n",
    "Additionally we started getting back to the filters in java that we originally had. We also tried to work on the evaluator, but we are suspecting that something is weird in it as we get an unexpected nbr of matches (only ~27 TruePositive). \n",
    "\n",
    "For next time: add stopwords and fix min_length of matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run the perl evaluator\n",
    "- go to medtag/genetag\n",
    "- run `perl alt_eval.perl ../[PATH]/genetag_short.out Gold.format > answer.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Day 7 6/5-2019\n",
    "Today's meeting: For next week we want a functioning demo. Want something to show eventhough it is not that accurate. GUI not priority but Pierre would like to do everything with the push of a button.\n",
    "\n",
    "Keep 2 layers, one fuzzy and remove the matches from fuzzy if it overlaps with the exact layer.\n",
    "\n",
    "TODO: Fix the Gold.format file. Right now it does not have the correct offset and thus gives us the terrible score. Second, we need to play around with the filters and keep building on 2 different analyzers for the short and long words. Finally tweak so that we can present a solution next time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Fix gold format file\n",
    "with open('../../medtag/genetag/Gold.format', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "\n",
    "prot = {}\n",
    "fixed = {} \n",
    "for line in data.split('\\n'):\n",
    "    l = line.split('|')\n",
    "    if len(l) > 2:\n",
    "        id = l[0]\n",
    "        [i1, i2] = l[1].split()\n",
    "        p = l[2]\n",
    "        prot.setdefault(id, []).append(p)\n",
    "\n",
    "for key in dictionary:\n",
    "    if key in prot:\n",
    "        proteins = prot[key]\n",
    "        start_i = 0\n",
    "        previous = ''\n",
    "        for p in proteins:\n",
    "            if p == previous:\n",
    "                start_i += len(previous)\n",
    "            match_start = start_i + dictionary[key][start_i:].index(p)\n",
    "            match_end = match_start + len(p)\n",
    "            start_i = match_start\n",
    "            fixed.setdefault(key, []).append('{}|{} {}|{}'.format(key, match_start, match_end, p))\n",
    "            previous = p\n",
    "\n",
    "elems = []\n",
    "for key in sorted(fixed.keys()):\n",
    "    for e in fixed[key]:\n",
    "        elems.append(e)\n",
    "\n",
    "with open('Gold2.format', 'w+', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(elems))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new Gold.format we got:\n",
    "\n",
    "genetag_long: ```Precision: 0.517406105864265 Recall: 0.350740713169061```. Gives: F1 = 0.42\n",
    "\n",
    "genetag_short: ```Precision: 0.11398451655411 Recall: 0.309527359475175```\n",
    "\n",
    "We changed the filters for genetag short (only minimum length = 2 and split on whitespace) and recieved a better result:\n",
    "\n",
    "genetag_short: ```Precision: 0.522418879056047 Recall: 0.291714709273596```. Gives: F1 = 0.37\n",
    "\n",
    "Now we also need to find a way to combine genetag_short and long and score them together. One idea is to use dominant right for each abstract. Another is to simply smack them together. As the evaluator is not dependent on the order of the id's, we will try the simple solution of stacking them together first.\n",
    "\n",
    "genetag: ```Precision: 0.288077969174977 Recall: 0.488497227255257```\n",
    "\n",
    "When stacking them together we don't get TP(short)+TP(long), so there seems to be an overlap of ~4000 proteins. So we must use something like dominant right to fix this overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build together long and short\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 8 7/5-2019\n",
    "We have consulted Marcus on what todo going forward with adding genetag_short and genetag_long. He said we have 2 options:\n",
    "    1. Add them both to one layer and do Dominant Right\n",
    "    2. Prioritize one detection method over the other, i.e. find all the intersections, remove the overlaps and then join the layers.\n",
    "\n",
    "So for today, our todo is as follows:\n",
    "    1. Remove numbers from the genetag lists (pure numbers like 555)\n",
    "    2. Join genetag long and short using one of the methods above\n",
    "    3. Evaluate and find possible improvements\n",
    "    4. Add list of cell death synonyms\n",
    "    5. Fix demo that we can show\n",
    "    \n",
    "---\n",
    "\n",
    "We will first look at the filters in java to see if there is one ready to use. There wasn't one. We will then do some postprocessing and remove those matches. We did this ```if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers``` which will suffice for now.\n",
    "\n",
    "genetag_short: ```Precision: 0.523598384077249 Recall: 0.291769615110086``` Without the only number ones. So slightly improved (by ~3 decimals).\n",
    "\n",
    "genetag_long: ```Precision: 0.336532053845129 Recall: 0.415911711414923``` So this got worse recall? What happened? \n",
    "\n",
    "(genetag_long: ```Precision: 0.336532053845129 Recall: 0.415911711414923``` F1 = 0.37. When we swiched ICUFoldingFilter to toLowerCaseFilter. So we'll go with ICU instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove numbers from the genetag lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying different filters for genetag_long:\n",
    "\n",
    "| Activated filter | Precision | Recall | F1 |\n",
    "|----------------------------------------|-----------|--------|----|\n",
    "| Mention, ICUNorm, Diacratic, Snowball  | 0.623715592889822 | 0.456596936254324 | 0.527 |\n",
    "| Mention, ICUNorm, Diacratic, Snowball, ICUFolding | 0.350802099229538 | 0.517487508922198 | 0.418 |\n",
    "| Mention, ICUNorm, Diacratic, Snowball, StopFilter | 0.615680473372781 | 0.457036182946247 | 0.524 |\n",
    "| Mention, ICUNorm, Diacratic, Snowball, LengthFilter | 0.287200716043858 | 0.422829846812716 | 0.342 |\n",
    "| Mention, ICUNorm, Diacratic, Snowball, toLowerCase | 0.350802099229538 | 0.517487508922198 | 0.418 |\n",
    "| Mention, ICUNorm, Diacratic | 0.651159123166693 | 0.45341239773788 | 0.535 |\n",
    "| Mention, ICUNorm |  0.651159123166693 | 0.45341239773788 | 0.535 |\n",
    "| Mention |  0.651159123166693 | 0.45341239773788 | 0.535 |\n",
    "| Mention (without removing numbers) |  0.651107782070488 | 0.45341239773788 | 0.535 |\n",
    "\n",
    "So for genetag_long the best solution seemed to be to not really add anything.\n",
    "\n",
    "---\n",
    "\n",
    "Different filters for genetag_short:\n",
    "\n",
    "| Activated filter | Precision | Recall | F1 |\n",
    "|----------------------------------------|-----------|--------|----|\n",
    "| PatternTokenizerFactory | 0.61393618220769 | 0.298962279690331 | 0.402 |\n",
    "| Mention | 0.629489888318744 | 0.458024488003075 | 0.530 |\n",
    "| Mention, LengthFilter | 0.324438202247191 | 0.418547191566464 | 0.366 |\n",
    "| Mention (without removing numbers) | 0.629489888318744 | 0.458024488003075 | 0.530 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Join genetag long and short using one of the methods above\n",
    "data_long = ''\n",
    "data_short = ''\n",
    "\n",
    "out = []\n",
    "\n",
    "with open('genetag/genetag_long.out', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    data_long = file.read().split('\\n')\n",
    "with open('genetag/genetag_short.out', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    data_short = file.read().split('\\n')\n",
    "\n",
    "def read(data, dic):\n",
    "    for line in data:\n",
    "        tul = line.split('|')\n",
    "        key = tul[0]\n",
    "        line_nbr = tul[1].split()\n",
    "        start = line_nbr[0]\n",
    "        end = line_nbr[1]\n",
    "        word = tul[2]\n",
    "        dic.setdefault(key, []).append((int(start), int(end), word))\n",
    "    return dic\n",
    "\n",
    "dic = read(data_short, read(data_long, {}))\n",
    "\n",
    "for key in dic:\n",
    "    tuls = dic[key]\n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    matches = list(set(matches))\n",
    "    \n",
    "    for m in matches:\n",
    "        out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "with open('genetag/genetag.out', 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined genetag_long and genetag_short, i.e. genetag.out: ```0.592672838290329 Recall: 0.495634985999012```\n",
    "F1 = 0.540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prototype\n",
    "text = 'Peroxydase reaction stains were negative, chloroacetate esterase were strongly positive.'\n",
    "file_name = 'protein_name/protein_names_long'\n",
    "\n",
    "\n",
    "def get_proteins(s, f):\n",
    "    from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "    import os.path\n",
    "    from docria.printout import options\n",
    "    from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "    set_large_screen()\n",
    "    def connect_jvm(port):\n",
    "        from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "        gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "        gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "        app = gateway.entry_point\n",
    "        return gateway, gateway.jvm, app\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    def get_java_file(path):\n",
    "        # Here we do the py4j equivalent for new java.io.File(path)\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "    app.buildIndex(get_java_file(\"{}.txt\".format(f)), get_java_file(\"{}.fst\".format(f)))\n",
    "    indx = app.loadIndex(get_java_file(\"{}.fst\".format(f)))\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", s)\n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "        #print(str(term[\"text\"]), \"-\", \"%d:%d\" % (term[\"text\"].start, term[\"text\"].stop))\n",
    "    print(tuls)\n",
    "    return dominant_right(tuls)\n",
    "\n",
    "matches = get_proteins(text, file_name)\n",
    "print('--------')\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do everything for both short and long names\n",
    "name = 'protein_name/protein_names_all';\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "with open('{}.txt'.format(name), 'w') as f:\n",
    "    for event, element in parser:\n",
    "        tag = element.tag[length:]\n",
    "        if 'shortName' == tag or 'fullName' == tag:\n",
    "            proteins.add(element.text)  \n",
    "        element.clear()\n",
    "    f.write('\\n'.join(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "############################################################################\n",
    "\n",
    "app.buildIndex(get_java_file('{}.txt'.format(name)), get_java_file('{}.fst'.format(name)))\n",
    "indx = app.loadIndex(get_java_file('{}.fst'.format(name)))\n",
    "\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    #If it is an excerpt\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        dictionary[id] = text\n",
    "    elif lines[0] == 'ANNOTATION':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "\n",
    "with open('{}.txt'.format(name), 'a', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(proteins))\n",
    "file.close()\n",
    "\n",
    "app.buildIndex(get_java_file('{}.txt'.format(name)), get_java_file('{}.fst'.format(name)))\n",
    "indx = app.loadIndex(get_java_file('{}.fst'.format(name)))\n",
    "############################################################################\n",
    "#import re\n",
    "d = dictionary\n",
    "out = []\n",
    "\n",
    "for key in d:\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", d[key])    \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    \n",
    "    for m in matches:\n",
    "        #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "            out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "with open('genetag/genetag_all.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method above did not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 9, 9/5-2019\n",
    "From last time we still had the following tasks in our todo list left:\n",
    "    3. Evaluate and find possible improvements\n",
    "    4. Add list of cell death synonyms\n",
    "    5. Fix demo that we can show (!)\n",
    "    \n",
    "However, last time we ran into a problem with the evaluator (again!). It seems that the Gold.format does not include all the bits that is included in the annotation bits. Some of those seem to be included in Correct.format. What is the difference in these two? Where is the bug?\n",
    "\n",
    "We started by adding all the code we need below to prepare for the prototyping. Also, we ran the tests again to make sure that this did not change the outcome we got previously.\n",
    "\n",
    "Only do the following snippets once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "#in protein_names.txt\n",
    "name = 'short'\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'shortName' in tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'w+') as f:\n",
    "    f.write('\\n'.join(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "#in protein_names.txt\n",
    "name = 'long_no_anno'\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'fullName' == tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "    \n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'w+') as f:\n",
    "    f.write('\\n'.join(proteins))\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'ANNOTATION' and 'ALTGENE' not in s:\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "\n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'a', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(proteins))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#READ ABSTRACTS TO FILE\n",
    "import json\n",
    "\n",
    "data = ''\n",
    "\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        dictionary[id] = text\n",
    "\n",
    "with open('genetag/abstracts_genetag.txt', 'w+',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write(json.dumps(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is to be run everytime there is a change in the Java code (i.e changed filters) or you want to switch between long/short words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "p_type = 'uniprot'\n",
    "c_type = 'cell_death_names'\n",
    "\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(p_type)), get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "\n",
    "app.buildIndex(get_java_file(\"{}.txt\".format(p_type)), get_java_file(\"{}.fst\".format(p_type)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "\n",
    "d = {}\n",
    "\n",
    "with open('genetag/abstracts_genetag.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    d = json.loads(file.read())\n",
    "\n",
    "    \n",
    "# Write to genetag\n",
    "out = []\n",
    "\n",
    "for key in d:\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", d[key])    \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "    tuls = []\n",
    "    for term in doc[\"protmatches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    matches = tuls #[(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    \n",
    "    for m in matches:\n",
    "        #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "        out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "with open('genetag/genetag_{}.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write('\\n'.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Join genetag long and short\n",
    "data_long = ''\n",
    "data_short = ''\n",
    "\n",
    "out = []\n",
    "\n",
    "with open('genetag/genetag_long.out', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    data_long = file.read().split('\\n')\n",
    "with open('genetag/genetag_short.out', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    data_short = file.read().split('\\n')\n",
    "\n",
    "def read(data, dic):\n",
    "    for line in data:\n",
    "        tul = line.split('|')\n",
    "        key = tul[0]\n",
    "        line_nbr = tul[1].split()\n",
    "        start = line_nbr[0]\n",
    "        end = line_nbr[1]\n",
    "        word = tul[2]\n",
    "        dic.setdefault(key, []).append((int(start), int(end), word))\n",
    "    return dic\n",
    "\n",
    "dic = read(data_short, read(data_long, {}))\n",
    "\n",
    "for key in dic:\n",
    "    tuls = dic[key]\n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    matches = list(set(matches))\n",
    "    \n",
    "    for m in matches:\n",
    "        out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "with open('genetag/genetag.out', 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype code and to use to test for bugs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prototype\n",
    "text = \"The MB-CPK isoenzyme showed no percentage increase of total CPK higher than 5%, measured at 6, 12, and 24 h after the shock, independent of the number of attempts of cardioversion.\"\n",
    "file_name = 'protein_name/protein_names_long'\n",
    "\n",
    "\n",
    "def get_proteins(s, f):\n",
    "    from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "    import os.path\n",
    "    from docria.printout import options\n",
    "    from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "    set_large_screen()\n",
    "    def connect_jvm(port):\n",
    "        from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "        gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "        gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "        app = gateway.entry_point\n",
    "        return gateway, gateway.jvm, app\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    def get_java_file(path):\n",
    "        # Here we do the py4j equivalent for new java.io.File(path)\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "    app.buildIndex(get_java_file(\"{}.txt\".format(f)), get_java_file(\"{}.fst\".format(f)))\n",
    "    indx = app.loadIndex(get_java_file(\"{}.fst\".format(f)))\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", s)\n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "        #print(str(term[\"text\"]), \"-\", \"%d:%d\" % (term[\"text\"].start, term[\"text\"].stop))\n",
    "    print(tuls)\n",
    "    return dominant_right(tuls)\n",
    "\n",
    "matches = get_proteins(text, file_name)\n",
    "print('--------')\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New score after all of this:\n",
    "* TP: 8462\n",
    "* FP: 5066\n",
    "* FN: 9751\n",
    "* Precision: 0.62551744529864 Recall: 0.464613188381925\n",
    "* F1:0.533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fix correct format file\n",
    "with open('../../medtag/genetag/Correct.data', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "\n",
    "#READ INPUT\n",
    "d = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    d = file.read()\n",
    "    \n",
    "sp = d.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID:'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        dictionary[id] = text\n",
    "        \n",
    "prot = {}\n",
    "fixed = {} \n",
    "\n",
    "for line in data.split('\\n'):\n",
    "    l = line.split('|')\n",
    "    if len(l) > 2:\n",
    "        id = l[0]\n",
    "        [i1, i2] = l[1].split()\n",
    "        p = l[2]\n",
    "        prot.setdefault(id, []).append(p)\n",
    "\n",
    "for key in dictionary:\n",
    "    if key in prot:\n",
    "        proteins = prot[key]\n",
    "        start_i = 0\n",
    "        previous = ''\n",
    "        for p in proteins:\n",
    "            if p == previous:\n",
    "                start_i += 0\n",
    "            if p in dictionary[key][start_i:]:\n",
    "                match_start = start_i + dictionary[key][start_i:].index(p)\n",
    "                match_end = match_start + len(p)\n",
    "                start_i = match_start\n",
    "                fixed.setdefault(key, []).append('{}|{} {}|{}'.format(key, match_start, match_end, p))\n",
    "                previous = p\n",
    "            else:\n",
    "                print(previous)\n",
    "                print(p)\n",
    "                print(dictionary[key])\n",
    "                print(dictionary[key][start_i:])\n",
    "                print('------------------------')\n",
    "\n",
    "elems = []\n",
    "for key in sorted(fixed.keys()):\n",
    "    for e in fixed[key]:\n",
    "        elems.append(e)\n",
    "\n",
    "with open('Correct2.data', 'w+', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(elems))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a bug! When adding things to our dictionary we also add the 'ALTGENE', which is not included in the Gold.format file (it is in the Correct.data file). Thus things like this gets us in trouble:\n",
    "\n",
    "* ALTGENE: vesicular stomatitis virus glycoprotein (VSVG)\n",
    "* GENE: vesicular stomatitis virus glycoprotein\n",
    "* GENE: VSVG\n",
    "\n",
    "Because of dominant right we pick the ALTGENE as the correct one which Gold.format gives us the wrong answer for. Fixing that gives:\n",
    "\n",
    "* TP: 11046\n",
    "* FP: 2015\n",
    "* FN: 7167\n",
    "* Precision: 0.845723910879718 Recall: 0.606489869873168\n",
    "* F1 = 0.706\n",
    "\n",
    "---\n",
    "\n",
    "We found another bug:\n",
    "\n",
    "In the evaluation file:\n",
    "- FN|P07691837A1359|36 51|K3 keratin gene\n",
    "- TP|P07691837A1359|NFkB|185 189|NFkB|185 189\n",
    "\n",
    "In protein_names_long:\n",
    "- K3 keratin gene\n",
    "\n",
    "In genetag_long (nothing in genetag_short):\n",
    "- P07691837A1359|185 189|NFkB\n",
    "\n",
    "TEXT: \n",
    "- This 300 bp 5'-upstream sequence of K3 keratin gene, which can function in vitro as a keratinocyte-specific promoter, contains two clusters of partially overlapping motifs, one with an NFkB consensus sequence and another with a GC box.\n",
    "\n",
    "---\n",
    "Another bug:\n",
    "If we have the text\n",
    "- 'The **MB-CPK** isoenzyme showed no percentage increase of total CPK higher than 5%, measured at 6, 12, and 24 h after the shock, independent of the number of attempts of cardioversion.' \n",
    "\n",
    "We find CPK, but not MB. However if they switch places, i.e **'CPK-MB'**, then we only find MB and not CPK.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 10, 13/5-2019\n",
    "\n",
    "Today we need to solve the bugs from yesterday and also deliver a demo for tomorrow's meeting. Marcus told us to run the text through the analyzer pipeline and show the output to see if all the tokens are there. The MentionTokenizer might have some bugs, so we might to to switch that aswell.\n",
    "\n",
    "This is the output from the terms in 'The MB-CPK isoenzyme':\n",
    "\n",
    "```\n",
    "  field    value                   \n",
    "\n",
    "  term     'MB'                    \n",
    "  text     span(main[4:6]) = 'MB'  \n",
    "  type     'WORD_ACRONYM'          \n",
    "\n",
    "           Node terms#2           \n",
    "\n",
    "  field    value                  \n",
    "\n",
    "  term     '-'                    \n",
    "  text     span(main[6:7]) = '-'  \n",
    "  type     'HYPHEN'               \n",
    "\n",
    "            Node terms#3             \n",
    "\n",
    "  field    value                     \n",
    "\n",
    "  term     'CPK'                     \n",
    "  text     span(main[7:10]) = 'CPK'  \n",
    "  type     'WORD_ACRONYM'            \n",
    "\n",
    "                Node terms#4                \n",
    "``` \n",
    "\n",
    "And still we only get the match: [(7, 10, 'CPK')]\n",
    "\n",
    "---\n",
    "\n",
    "We found the bug! It was because of the dominant right we implemented in Java. We forgot about that since we have one in the pyhon code aswell.        //matches.retainAll(DominantRight.resolve(matches, \"text\"));\n",
    "\n",
    "Now we have the following score for genetag_long:\n",
    "\n",
    "```\n",
    "TP: 17090\n",
    "FP: 4845\n",
    "FN: 1123\n",
    "Precision: 0.779120127649875 Recall: 0.93834074562126\n",
    "```\n",
    "\n",
    "But genetag_short only gives:\n",
    "```\n",
    "TP: 2066\n",
    "FP: 4610\n",
    "FN: 16147\n",
    "Precision: 0.309466746554823 Recall: 0.113435458189206\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Apperantly we also need to divide the words from the annotaded part in genetag into training and test-set.\n",
    "\n",
    "genetag_long without the words in the annotations:\n",
    "```\n",
    "TP: 753\n",
    "FP: 2010\n",
    "FN: 17460\n",
    "Precision: 0.272529858849077 Recall: 0.0413440948772855\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide Gold into test and train\n",
    "div = 0.8\n",
    "\n",
    "gold = ''\n",
    "with open('Gold2.format', 'r') as f:\n",
    "    gold = f.read()\n",
    "\n",
    "gold = gold.split('\\n')\n",
    "\n",
    "train = round(len(gold)*div)\n",
    "\n",
    "train_gold = gold[:train]\n",
    "test_gold = gold[train:]\n",
    "\n",
    "with open('Gold2_train.format', 'w+', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(train_gold))\n",
    "    \n",
    "id = set()\n",
    "for g in test_gold:\n",
    "    id.add(g.split('|')[0])\n",
    "\n",
    "with open('Gold2_test.format', 'w+', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(test_gold))\n",
    "    \n",
    "with open('Gold2_test_ids.out', 'w+', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build prot_names based on test and train\n",
    "name = 'long_no_anno'\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'fullName' == tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "    \n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'w+') as f:\n",
    "    f.write('\\n'.join(proteins))\n",
    "\n",
    "p_id_test = []\n",
    "with open('Gold2_test_ids.out', 'r') as f:\n",
    "    p_id_test = f.read().split('\\n')\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'ANNOTATION' and 'ALTGENE' not in s:\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID:') and line.split('ID: ')[1] in p_id_test:\n",
    "                break\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "\n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'a', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build genetag with prot names train\n",
    "\n",
    "import json\n",
    "\n",
    "p_type = 'uniprot'\n",
    "\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(p_type)), get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "\n",
    "d = {}\n",
    "with open('genetag/abstracts_genetag.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    d = json.loads(file.read())\n",
    "\n",
    "p_id_test = []\n",
    "with open('Gold2_test_ids.out', 'r') as f:\n",
    "    p_id_test = f.read().split('\\n')\n",
    "    \n",
    "# Write to genetag\n",
    "out = []\n",
    "\n",
    "for key in d:\n",
    "    if key in p_id_test:\n",
    "        doc = Document()\n",
    "        doc.add_text(\"main\", d[key])    \n",
    "        binary_doc = MsgpackCodec.encode(doc) \n",
    "        search_binary_doc = app.search(indx, binary_doc)\n",
    "        doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "        tuls = []\n",
    "        for term in doc[\"matches\"]:\n",
    "            tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "        matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "\n",
    "        for m in matches:\n",
    "            #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "            out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "with open('genetag/genetag_{}.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "På testset får vi då (med long bara):\n",
    "```\n",
    "TP: 1293\n",
    "FP: 1079\n",
    "FN: 2350\n",
    "Precision: 0.545109612141653 Recall: 0.354927257754598\n",
    "```\n",
    "På train får vi:\n",
    "```\n",
    "TP: 13744\n",
    "FP: 4101\n",
    "FN: 826\n",
    "Precision: 0.770187727654805 Recall: 0.943308167467399\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prototype\n",
    "\n",
    "import re\n",
    "\n",
    "def get_cell(text, f):\n",
    "    s_out = text\n",
    "    synonyms = []\n",
    "    with open(f, 'r') as f:\n",
    "        synonyms = f.read().split('\\n')\n",
    "    \n",
    "    for s in synonyms:\n",
    "        if s.lower() in text.lower():\n",
    "            s_out = re.sub(s, \"\\033[45m{}\\033[m\".format(s), s_out, flags = re.IGNORECASE)            \n",
    "    return s_out\n",
    "\n",
    "def get_proteins(s, f):\n",
    "    from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "    import os.path\n",
    "    from docria.printout import options\n",
    "    from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "    set_large_screen()\n",
    "    def connect_jvm(port):\n",
    "        from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "        gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "        gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "        app = gateway.entry_point\n",
    "        return gateway, gateway.jvm, app\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    def get_java_file(path):\n",
    "        # Here we do the py4j equivalent for new java.io.File(path)\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "    app.buildIndex(get_java_file(\"{}.txt\".format(f)), get_java_file(\"{}.fst\".format(f)))\n",
    "    indx = app.loadIndex(get_java_file(\"{}.fst\".format(f)))\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", s)\n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "        #print(str(term[\"text\"]), \"-\", \"%d:%d\" % (term[\"text\"].start, term[\"text\"].stop))\n",
    "    tul = [x[2] for x in tuls]\n",
    "\n",
    "    dr = dominant_right(tuls)\n",
    "    #dr = tul # without dominant_right --> primase\n",
    "    \n",
    "    i = 0; j=0; s_out = ''\n",
    "\n",
    "    for d in dr:\n",
    "        while j < len(tuls):\n",
    "            j+=1\n",
    "            if d == tuls[j-1][2]:\n",
    "                print('MATCH:')\n",
    "                print(d)\n",
    "                print('------------------')\n",
    "                i_s = tuls[j-1][0]\n",
    "                i_e = tuls[j-1][1]\n",
    "\n",
    "                s_out += s[i:i_s] + \"\\033[43m{}\\033[m\".format(s[i_s:i_e])\n",
    "                i = i_e\n",
    "                break\n",
    "\n",
    "    return s_out + s[i:]\n",
    "\n",
    "\n",
    "# '\\033[43m{}\\033[m.format'\n",
    "#text = \"The MB-CPK isoenzyme showed no percentage increase of total CPK higher than 5%, measured at 6, 12, and 24 h after the shock, independent of the number of attempts of cardioversion. K3 keratin gene\"\n",
    "text = \"Apoptosis. The constraints of primase recognition sequences, nucleotide substrate requirements, and the effects of additional proteins on oligoribonucleotide synthesis by the 63-kDa gene 4 protein have been examined using templates of defined sequence. cell death is caused by cells dying.\"\n",
    "\n",
    "file_name = 'protein_name/protein_names_long'\n",
    "\n",
    "matches = \"{} \\n\\n\\033[43m{}\\033[m\\n\\033[45m{}\\033[m\".format(get_cell(get_proteins(text, file_name), 'cell_death_names.txt'), 'PROTEIN', 'CELL DEATH')\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "So today we finished a prototype that can highlight words in the terminal (probably won't work if we write to file), fixed the bugs by removing dominant right in java, and finally, splitting the annotated words into test/train data to use for the genetag which gave a worse result than before (as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Day 11, 14/5-2019\n",
    "\n",
    "Meeting w. Sonja and Pierre:\n",
    "* Collect all the names (even the alternative ones, use gene name as key, in some of them there are multiple gene ones - but not in the text file??), USE THE UNIPROT IDENTIFIER INSTEAD AS THE KEY!\n",
    "* Use spart of speach tagger to find what might be protein names and to remove unnecessary bits.\n",
    "* Use spacey\n",
    "* Apply this to a part of PUBMED (100 000- 200 000 nbr of abstracts) + benchmark the time it takes\n",
    "\n",
    "* Pro vs euc, 'taxanomic lineage', if it is pro we do not use those entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read the new uniprot file and store them in protein names\n",
    "filename='uniprot_less.txt'\n",
    "data = []\n",
    "with open(filename, 'r') as f:\n",
    "    data = f.read().split('//')[:-1] #remove last elem (only whitespace)\n",
    "\n",
    "dic = {}\n",
    "prots = []\n",
    "ids = []\n",
    "\n",
    "for entry in data:\n",
    "    lines = entry.split('\\n')\n",
    "    if 'OC   Eukaryota' not in entry:\n",
    "        continue\n",
    "    id = lines[1].split()[1]\n",
    "    prot = []\n",
    "    \n",
    "    for line in lines[2:]:\n",
    "        if line.startswith('DE') or line.startswith('GN'):\n",
    "            p = re.search('(?<!C)=.+?( {|;)', line)\n",
    "            if p:\n",
    "                prot.append(p.group()[1:-1])\n",
    "                prots.append(p.group()[1:-1])\n",
    "                ids.append(id)\n",
    "                \n",
    "    dic[id] = prot  \n",
    "    \n",
    "with open('protein_name/protein_names_uniprot.txt', 'w+') as f:\n",
    "    f.write('\\n'.join(prots))\n",
    "    \n",
    "with open('protein_name/protein_names_uniprot_indx.txt', 'w+') as f:\n",
    "    f.write('\\n'.join(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 12, 16/5 - 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Print to file using docria and fix output for Vilhelm and Olof\n",
    "\n",
    "import re\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "###\n",
    "from docria.storage import DocumentIO\n",
    "\n",
    "def get_proteins(s, f):\n",
    "    set_large_screen()\n",
    "    def connect_jvm(port):\n",
    "        from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "        gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "        gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "        app = gateway.entry_point\n",
    "        return gateway, gateway.jvm, app\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    def get_java_file(path):\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "    app.buildIndex(get_java_file(\"{}.txt\".format(f)), get_java_file(\"{}.fst\".format(f)))\n",
    "    app.buildIndex(get_java_file(\"cell_death_names.txt\"), get_java_file(\"cell_death_names.fst\"))\n",
    "\n",
    "    \n",
    "    indx = app.loadIndex(get_java_file(\"{}.fst\".format(f)))\n",
    "    indx2 = app.loadIndex(get_java_file(\"cell_death_names.fst\"))\n",
    "    \n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", s)   \n",
    "    \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, indx2, binary_doc)\n",
    "\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "    \n",
    "    tuls = []\n",
    "    for term in doc[\"protmatches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    \n",
    "    tuls2 = []\n",
    "    for term in doc[\"lysomatches\"]:\n",
    "        tuls2.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    \n",
    "    dr = dominant_right(tuls)\n",
    "    \n",
    "    i = 0; j=0; s_out = ''\n",
    "\n",
    "    for d in dr:\n",
    "        while j < len(tuls):\n",
    "            j+=1\n",
    "            if d == tuls[j-1][2]:\n",
    "                print('MATCH, PROT:')\n",
    "                print(d)\n",
    "                print('------------------')\n",
    "                i_s = tuls[j-1][0]\n",
    "                i_e = tuls[j-1][1]\n",
    "\n",
    "                s_out += s[i:i_s] + \"\\033[43m{}\\033[m\".format(s[i_s:i_e])\n",
    "                i = i_e\n",
    "                break\n",
    "    s_out += s[i:]\n",
    "\n",
    "    for d in tuls2:\n",
    "        print('MATCH, LYSO: ')\n",
    "        print(d[2])\n",
    "        print('------------------')\n",
    "        s_out = re.sub(d[2], \"\\033[45m{}\\033[m\".format(d[2]), s_out, flags = re.IGNORECASE) \n",
    "\n",
    "    with DocumentIO.write('output-file.docria') as dw:\n",
    "        dw.write(doc)\n",
    "    #with DocumentIO.read('output-file.docria') as dr:\n",
    "    #    for doc in dr:\n",
    "    #        print(doc[\"protmatches\"])  \n",
    "    #        print(doc[\"lysomatches\"])  \n",
    "                \n",
    "    return s_out\n",
    "\n",
    "\n",
    "# '\\033[43m{}\\033[m.format'\n",
    "#text = \"The MB-CPK isoenzyme showed no percentage increase of total CPK higher than 5%, measured at 6, 12, and 24 h after the shock, independent of the number of attempts of cardioversion. K3 keratin gene\"\n",
    "text = \"apoptosis. The constraints of Cr10HGO, 110 kDa antigen, and the effects of additional proteins on oligoribonucleotide synthesis by the 63-kDa gene 4 protein have been examined using templates of defined sequence. cell death is caused by cells dying.\"\n",
    "\n",
    "file_name = 'protein_name/protein_names_uniprot'\n",
    "\n",
    "matches = \"{} \\n\\n\\033[43m{}\\033[m\\n\\033[45m{}\\033[m\".format(get_proteins(text, file_name), 'PROTEIN', 'CELL DEATH')\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "p_type = 'uniprot(2)'\n",
    "c_type = 'cell_death_names'\n",
    "\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "from docria.storage import DocumentIO\n",
    "\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(p_type)), get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "\n",
    "app.buildIndex(get_java_file(\"{}.txt\".format(c_type)), get_java_file(\"{}.fst\".format(c_type)))\n",
    "indx2 = app.loadIndex(get_java_file(\"{}.fst\".format(c_type)))\n",
    "\n",
    "d = {}\n",
    "\n",
    "with open('genetag/abstracts_genetag.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    d = json.loads(file.read())\n",
    "\n",
    "    \n",
    "# Write to genetag\n",
    "out = []\n",
    "dr = []\n",
    "for key in d:\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", d[key])  \n",
    "    doc.props[\"id\"] = key \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, indx2, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "    dr.append(doc)\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "while i < len(dr):\n",
    "    with DocumentIO.write('pubmed/pubmed1905({}).docria'.format(str(j))) as dw:\n",
    "        while i < len(dr) and i < 50000:\n",
    "            dw.write(dr[i])\n",
    "            i += 1\n",
    "    j += 1\n",
    "\n",
    "    #for m in matches:\n",
    "        #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "    #    out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "#with open('genetag/genetag_{}.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "#    file.write('\\n'.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Turn docra file into readable temp file\n",
    "with DocumentIO.read('docria/out_json_pubmed19n0001.txt.docria') as dr:\n",
    "    with open('docria/temp.txt', 'w+', encoding=\"utf-8\", errors='ignore') as f:\n",
    "        for doc in dr:\n",
    "            f.write(str(doc.text[\"main\"]))\n",
    "            f.write(str(doc.props))\n",
    "            for d in doc[\"protmatches\"]:\n",
    "                f.write(str(d))  \n",
    "            for d in doc[\"lysomatches\"]:\n",
    "                f.write(str(d))\n",
    "            f.write(\"--------------------\\n\\n\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Append genetag names to protein names\n",
    "name = 'uniprot'\n",
    "\n",
    "prot_names = ''\n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'r') as f:\n",
    "    prot_names = f.read()\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "for s in data.split('\\n>>'):\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'ANNOTATION' and 'ALTGENE' not in s:\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "\n",
    "with open('protein_name/protein_names_{}(2).txt'.format(name), 'w+', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write(prot_names)\n",
    "    file.write('\\n'.join(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "import json\n",
    "\n",
    "filename='pubmed19n0651.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "\n",
    "id = ''\n",
    "abbe = {}\n",
    "for event, element in parser:\n",
    "    tag = element.tag\n",
    "    if tag == 'PMID':\n",
    "        id = element.text\n",
    "    elif tag == 'AbstractText':\n",
    "        abbe.setdefault(id, []).append(element.text)\n",
    "    element.clear()\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "for key in abbe:\n",
    "    to_add = abbe[key]\n",
    "    if abbe[key] is str and len(abbe[key]) > 1:\n",
    "        to_add = ' '.join(abbe[key]) \n",
    "    dictionary[key] = to_add\n",
    "\n",
    "with open('pubmed/abstracts19n0651.txt', 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    for key in abbe:\n",
    "        file.write(key)\n",
    "        file.write(str(abbe[key]))\n",
    "        \n",
    "with open('pubmed/abstracts19n0651_json.txt', 'w+',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write(json.dumps(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 13, 17/5\n",
    "Todays goals:\n",
    "1. Benchmark time it takes for pubmed CHECK!\n",
    "2. Experiment with spacey +  Try a part of speech tagger to improve findings CHECK!\n",
    "3. Find a way to remove noise, how much should we remove? CHECK!\n",
    "4. A method that takes a protein name and outputs it's genetag id? Or add it to docria?\n",
    "5. Clean up code CHECK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "MATCH, PROT:\n",
      "phospholipase\n",
      "------------------\n",
      "------------------\n",
      "MATCH, PROT:\n",
      "A2\n",
      "------------------\n",
      "------------------\n",
      "MATCH, PROT:\n",
      "PLA2R\n",
      "------------------\n",
      "------------------\n",
      "MATCH, PROT:\n",
      "PCR\n",
      "------------------\n",
      "------------------\n",
      "MATCH, PROT:\n",
      "angiotensin II\n",
      "------------------\n",
      "------------------\n",
      "MATCH, PROT:\n",
      "Ang II\n",
      "------------------\n",
      "------------------\n",
      "MATCH, PROT:\n",
      "PLA2R\n",
      "------------------\n",
      "------------------\n",
      "MATCH, PROT:\n",
      "luciferase reporter gene\n",
      "------------------\n",
      "------------------\n",
      "MATCH, PROT:\n",
      "Ang II\n",
      "------------------\n",
      "------------------\n",
      "MATCH, PROT:\n",
      "Ang II\n",
      "------------------\n",
      "------------------\n",
      "MATCH, PROT:\n",
      "PLA2R\n",
      "------------------\n",
      "------------------\n",
      "MATCH, PROT:\n",
      "PLA2R\n",
      "------------------\n",
      "------------------\n",
      "MATCH, PROT:\n",
      "PLA2R\n",
      "------------------\n",
      "------------------\n",
      "MATCH, LYSO:\n",
      "apoptosis\n",
      "------------------\n",
      "------------------\n",
      "MATCH, LYSO:\n",
      "apoptosis\n",
      "------------------\n",
      "------------------\n",
      "MATCH, LYSO:\n",
      "apoptosis\n",
      "------------------\n",
      "------------------\n",
      "MATCH, LYSO:\n",
      "apoptosis\n",
      "------------------\n",
      "------------------\n",
      "MATCH, LYSO:\n",
      "apoptosis\n",
      "------------------\n",
      "------------------\n",
      "MATCH, LYSO:\n",
      "apoptosis\n",
      "------------------\n",
      "Podocyte \u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45mapoptosis\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m is considered as the important element that promotes the development and progress of membranous nephropathy (MN). Unfortunately, the underlying mechanism of podocytes \u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45mapoptosis\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m in MN remains elusive. We compared the renal expressions of miR-130a-5p and M-type \u001b[43mphospholipase\u001b[m \u001b[43mA2\u001b[m receptor (\u001b[43mPLA2R\u001b[m) between MN patients (n = 30) and 30 controls by qRT-\u001b[43mPCR\u001b[m and western blot, respectively. The podocyte damage model in vitro was established by \u001b[43mangiotensin II\u001b[m (\u001b[43mAng II\u001b[m, 100 nmol/L) exposure for 24 h. Interaction between miR-130a-5p and \u001b[43mPLA2R\u001b[m was determined using dual-\u001b[43mluciferase reporter gene\u001b[m assay. MN mice were induced by intravenous injection of cBSA. In this study, miR-130a-5p expression was significantly decreased both in the renal biopsy specimens from MN patients and podocyte cell line AB8/13 following stimulation of \u001b[43mAng II\u001b[m. Overexpressed miR-130a-5p in AB8/13 cells significantly attenuated the \u001b[43mAng II\u001b[m induced-\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45mapoptosis\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m in vitro. In contrast, down-regulated miR-130a-5p induced podocyte \u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45mapoptosis\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m. \u001b[43mPLA2R\u001b[m was identified as the target of miR-130a-5p in AB8/13 cells. And up-regulated or down-regulated \u001b[43mPLA2R\u001b[m could obviously attenuate the effect of miR-130a-5p overexpression or knockdown on the \u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45mapoptosis\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m of AB8/13 cells. Furthermore, it was also observed that overexpressed miR-130a-5p by miR-130a-5p agomir could obviously alleviate renal injury in MN mice. In conclusion, decreased miR-130a-5p was contributed to the pathological mechanism of MN through increasing \u001b[43mPLA2R\u001b[m expression, which induced podocyte \u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45mapoptosis\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m. \n",
      "\n",
      "\u001b[43mPROTEIN\u001b[m\n",
      "\u001b[45mCELL DEATH\u001b[m\n",
      "== Document ==\n",
      "                                                                                                       Texts                                                                                                       \n",
      "\n",
      "  main    'Podocyte apoptosis is considered as the important element that promotes the development and prog ... hological mechanism of MN through increasing PLA2R expression, which induced podocyte apoptosis.'  \n",
      "\n",
      "         Layers         \n",
      "\n",
      "  lysomatches    N=6    \n",
      "  protmatches    N=13   \n",
      "  terms          N=624  \n",
      "\n",
      "                                         Layer: protmatches                                         \n",
      "\n",
      "  #      id        terms                          text                                              \n",
      "\n",
      "  #0     703295    [Node<terms#311>]              span(main[279:292]) = 'phospholipase'             \n",
      "  #1     15984     [Node<terms#311>]              span(main[293:295]) = 'A2'                        \n",
      "  #2     462220    [Node<terms#311>]              span(main[306:311]) = 'PLA2R'                     \n",
      "  #3     469720    [Node<terms#311>]              span(main[365:368]) = 'PCR'                       \n",
      "  #4     713206    [2 nodes from layer: terms]    span(main[455:469]) = 'angiotensin II'            \n",
      "  #5     33667     [2 nodes from layer: terms]    span(main[471:477]) = 'Ang II'                    \n",
      "  #6     462220    [Node<terms#311>]              span(main[546:551]) = 'PLA2R'                     \n",
      "  #7     705614    [3 nodes from layer: terms]    span(main[578:602]) = 'luciferase reporter gene'  \n",
      "  #8     33667     [2 nodes from layer: terms]    span(main[838:844]) = 'Ang II'                    \n",
      "  #9     33667     [2 nodes from layer: terms]    span(main[917:923]) = 'Ang II'                    \n",
      "  #10    462220    [Node<terms#311>]              span(main[1020:1025]) = 'PLA2R'                   \n",
      "  #11    462220    [Node<terms#311>]              span(main[1122:1127]) = 'PLA2R'                   \n",
      "  #13    462220    [Node<terms#311>]              span(main[1488:1493]) = 'PLA2R'                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><thead><tr><th colspan='4' style='text-align:center'><em>Layer: lysomatches</em></th></tr><tr><th style='text-align:left'>#</th><th style='text-align:left'>id</th><th style='text-align:left'>terms</th><th style='text-align:left'>text</th></tr></thead><tbody><tr><td style='text-align:left'>#0</td><td style='text-align:left'>0</td><td style='text-align:left'>[Node&lt;terms#623&gt;]</td><td style='text-align:left'>span(main[9:18]) = &#x27;apoptosis&#x27;</td></tr><tr><td style='text-align:left'>#1</td><td style='text-align:left'>0</td><td style='text-align:left'>[Node&lt;terms#623&gt;]</td><td style='text-align:left'>span(main[186:195]) = &#x27;apoptosis&#x27;</td></tr><tr><td style='text-align:left'>#2</td><td style='text-align:left'>0</td><td style='text-align:left'>[Node&lt;terms#623&gt;]</td><td style='text-align:left'>span(main[932:941]) = &#x27;apoptosis&#x27;</td></tr><tr><td style='text-align:left'>#3</td><td style='text-align:left'>0</td><td style='text-align:left'>[Node&lt;terms#623&gt;]</td><td style='text-align:left'>span(main[1009:1018]) = &#x27;apoptosis&#x27;</td></tr><tr><td style='text-align:left'>#4</td><td style='text-align:left'>0</td><td style='text-align:left'>[Node&lt;terms#623&gt;]</td><td style='text-align:left'>span(main[1215:1224]) = &#x27;apoptosis&#x27;</td></tr><tr><td style='text-align:left'>#5</td><td style='text-align:left'>0</td><td style='text-align:left'>[Node&lt;terms#623&gt;]</td><td style='text-align:left'>span(main[1529:1538]) = &#x27;apoptosis&#x27;</td></tr></tbody></table>"
      ],
      "text/plain": [
       "Layer(lysomatches, N=6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Process protein names and cell death names using docria and print docria to file\n",
    "\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "from docria.storage import DocumentIO\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "set_large_screen()\n",
    "\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "\n",
    "def get_java_file(jvm, path):\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "def setup_java(p, c):\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    app.buildIndex(get_java_file(jvm, \"{}.txt\".format(p)), get_java_file(jvm, \"{}.fst\".format(p)))\n",
    "    app.buildIndex(get_java_file(jvm, \"{}.txt\".format(c)), get_java_file(jvm, \"{}.fst\".format(c)))\n",
    "\n",
    "    indx = app.loadIndex(get_java_file(jvm, \"{}.fst\".format(p)))\n",
    "    indx2 = app.loadIndex(get_java_file(jvm, \"{}.fst\".format(c)))\n",
    "    \n",
    "    return (app, indx, indx2)\n",
    "\n",
    "# Create a document and tag it\n",
    "def create_doc(id, text, app, i, i2):\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", text)  \n",
    "    doc.props[\"id\"] = id \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(i, i2, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)\n",
    "    return doc\n",
    "\n",
    "def filter_away_nodes(d, stop_w):\n",
    "    rm_nodes = []\n",
    "    for node in d['protmatches']:\n",
    "        #If node is a stopword or number or length less than 2\n",
    "        if len(str(node[\"text\"])) < 2 or re.match( \"^[0-9-]+$\", str(node[\"text\"])) or str(node[\"text\"]) in stop_w:\n",
    "            rm_nodes.append(node)\n",
    "    [n.detach() for n in rm_nodes]\n",
    "    return d\n",
    "    \n",
    "#Taggs the abstracts in abstracts_genetag\n",
    "def file_tagger(app, indx, indx2, stop_w):\n",
    "    # Add the abstracts[key = PMID] = Abstract Text\n",
    "    abstracts = {}\n",
    "    with open('genetag/abstracts_genetag.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "        abstracts = json.loads(file.read())\n",
    "\n",
    "    # One doc for each abstract\n",
    "    out = []; dr = []\n",
    "    for key in abstracts:\n",
    "        dr.append(create_doc(key, abstracts[key], app, indx, indx2))\n",
    "    for d in dr:\n",
    "        d = filter_away_nodes(d, stop_w)\n",
    "    \n",
    "    # Print one file per 50 000 abstracts\n",
    "    i = 0; j = 0\n",
    "    while i < len(dr):\n",
    "        with DocumentIO.write('pubmed/pubmed1905({}).docria'.format(str(j))) as dw:\n",
    "            while i < len(dr) and i < 50000:\n",
    "                dw.write(dr[i])\n",
    "                i += 1\n",
    "        j += 1\n",
    "    return len(dr)\n",
    "\n",
    "\n",
    "# Tags a given string, s, and prints matches and returns new string, s_out\n",
    "def text_tagger(s, app, indx, indx2, stop_w):\n",
    "   \n",
    "    doc = filter_away_nodes(create_doc('id', s, app, indx, indx2), stop_w)\n",
    "\n",
    "    tuls = []\n",
    "    for term in doc[\"protmatches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    \n",
    "    tuls2 = []\n",
    "    for term in doc[\"lysomatches\"]:\n",
    "        tuls2.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "        \n",
    "    i = 0; s_out = ''\n",
    "\n",
    "    for d in tuls:\n",
    "        print('------------------\\nMATCH, PROT:\\n{}\\n------------------'.format(d[2]))\n",
    "        i_s = d[0]\n",
    "        i_e = d[1]\n",
    "        s_out += s[i:i_s] + \"\\033[43m{}\\033[m\".format(s[i_s:i_e])\n",
    "        i = i_e\n",
    "        \n",
    "    s_out += s[i:]\n",
    "    \n",
    "    for d in tuls2:\n",
    "        print('------------------\\nMATCH, LYSO:\\n{}\\n------------------'.format(d[2]))\n",
    "        s_out = re.sub(d[2], \"\\033[45m{}\\033[m\".format(d[2]), s_out, flags = re.IGNORECASE) \n",
    "    return s_out, doc\n",
    "\n",
    "#A tagger which prints to genetag in eval format\n",
    "def score_tagger(c):\n",
    "    p = 'protein_name/protein_names_uniprot(2)'\n",
    "    \n",
    "    (app, indx, indx2) = setup_java(p, c)\n",
    "    \n",
    "    # Read dictionary with genetag abstracts \n",
    "    with open('genetag/abstracts_genetag.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "        d = json.loads(file.read())\n",
    "\n",
    "    # Write to genetag\n",
    "    out = []\n",
    "\n",
    "    for key in d:\n",
    "        doc = filter_away_nodes(create_doc(key, d[key], app, indx, indx2))\n",
    "        tuls = []\n",
    "        \n",
    "        for term in doc[\"protmatches\"]:\n",
    "            tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "\n",
    "        for m in tuls:\n",
    "            out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "\n",
    "    with open('genetag/genetag_all.out', 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "        file.write('\\n'.join(out))\n",
    "        \n",
    "    return len(d)\n",
    "\n",
    "p = 'protein_name/protein_names_uniprot(2)'\n",
    "c = 'cell_death_names'\n",
    "\n",
    "stop_w = set()\n",
    "[stop_w.add(t) for t in stopwords.words('english')]\n",
    "\n",
    "(app, indx, indx2) = setup_java(p, c)\n",
    "\n",
    "#file_tagger = 'file', text_tagger = 'text',  score_tagger = 'score'\n",
    "run = 'text'\n",
    "\n",
    "if run == 'file':\n",
    "    start = time.time()\n",
    "    l = file_tagger(app, indx, indx2, stop_w)\n",
    "    end = time.time()\n",
    "    print('total time: {} s\\nnbr of abstracts: {}\\ntime per abstract: {} s'.format(str(end-start), l, str((end-start)/l)))\n",
    "\n",
    "elif run == 'text':\n",
    "    #text = 'Improvement of nursing instruction to be given at the time of discharge from the ward for premature infants'\n",
    "    text = \"Podocyte apoptosis is considered as the important element that promotes the development and progress of membranous nephropathy (MN). Unfortunately, the underlying mechanism of podocytes apoptosis in MN remains elusive. We compared the renal expressions of miR-130a-5p and M-type phospholipase A2 receptor (PLA2R) between MN patients (n = 30) and 30 controls by qRT-PCR and western blot, respectively. The podocyte damage model in vitro was established by angiotensin II (Ang II, 100 nmol/L) exposure for 24 h. Interaction between miR-130a-5p and PLA2R was determined using dual-luciferase reporter gene assay. MN mice were induced by intravenous injection of cBSA. In this study, miR-130a-5p expression was significantly decreased both in the renal biopsy specimens from MN patients and podocyte cell line AB8/13 following stimulation of Ang II. Overexpressed miR-130a-5p in AB8/13 cells significantly attenuated the Ang II induced-apoptosis in vitro. In contrast, down-regulated miR-130a-5p induced podocyte apoptosis. PLA2R was identified as the target of miR-130a-5p in AB8/13 cells. And up-regulated or down-regulated PLA2R could obviously attenuate the effect of miR-130a-5p overexpression or knockdown on the apoptosis of AB8/13 cells. Furthermore, it was also observed that overexpressed miR-130a-5p by miR-130a-5p agomir could obviously alleviate renal injury in MN mice. In conclusion, decreased miR-130a-5p was contributed to the pathological mechanism of MN through increasing PLA2R expression, which induced podocyte apoptosis.\"\n",
    "    s, doc = text_tagger(text, app, indx, indx2, stop_w)\n",
    "    matches = \"{} \\n\\n\\033[43m{}\\033[m\\n\\033[45m{}\\033[m\".format(s, 'PROTEIN', 'CELL DEATH')\n",
    "    print(matches)\n",
    "    print(doc)\n",
    "    print(doc[\"protmatches\"])\n",
    "\n",
    "elif run == 'score':\n",
    "    start = time.time()\n",
    "    l = score_tagger(c)\n",
    "    end = time.time()\n",
    "    print('total time: {} s\\ntime per abstract: {} s'.format(str(end-start), str((end-start)/l)))\n",
    "    \n",
    "doc[\"lysomatches\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podocyte \u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45mapoptosis\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m is considered as the important element that promotes the development and progress of membranous nephropathy (MN). Unfortunately, the underlying mechanism of podocytes \u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45mapoptosis\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m in MN remains elusive. We compared the renal expressions of miR-130a-5p and M-type \u001b[43mphospholipase\u001b[m \u001b[43mA2\u001b[m receptor (\u001b[43mPLA2R\u001b[m) between MN patients (n = 30) and 30 controls by qRT-PCR and western blot, respectively. The podocyte damage model in vitro was established by \u001b[43mangiotensin II\u001b[m (\u001b[43mAng II\u001b[m, 100 nmol/L) exposure for 24 h. Interaction between miR-130a-5p and \u001b[43mPLA2R\u001b[m was determined using dual-luciferase reporter gene assay. MN mice were induced by intravenous injection of cBSA. In this study, miR-130a-5p expression was significantly decreased both in the renal biopsy specimens from MN patients and podocyte cell line AB8/13 following stimulation of \u001b[43mAng II\u001b[m. Overexpressed miR-130a-5p in AB8/13 cells significantly attenuated the \u001b[43mAng II\u001b[m induced-\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45mapoptosis\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m in vitro. In contrast, down-regulated miR-130a-5p induced podocyte \u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45mapoptosis\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m. \u001b[43mPLA2R\u001b[m was identified as the target of miR-130a-5p in AB8/13 cells. And up-regulated or down-regulated \u001b[43mPLA2R\u001b[m could obviously attenuate the effect of miR-130a-5p overexpression or knockdown on the \u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45mapoptosis\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m of AB8/13 cells. Furthermore, it was also observed that overexpressed miR-130a-5p by miR-130a-5p agomir could obviously alleviate renal injury in MN mice. In conclusion, decreased miR-130a-5p was contributed to the pathological mechanism of MN through increasing \u001b[43mPLA2R\u001b[m expression, which induced podocyte \u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45m\u001b[45mapoptosis\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m\u001b[m.\n"
     ]
    }
   ],
   "source": [
    "b = 'Podocyte \\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45mapoptosis\\x1b[m\\x1b[m\\x1b[m\\x1b[m\\x1b[m\\x1b[m is considered as the important element that promotes the development and progress of membranous nephropathy (MN). Unfortunately, the underlying mechanism of podocytes \\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45mapoptosis\\x1b[m\\x1b[m\\x1b[m\\x1b[m\\x1b[m\\x1b[m in MN remains elusive. We compared the renal expressions of miR-130a-5p and M-type \\x1b[43mphospholipase\\x1b[m \\x1b[43mA2\\x1b[m receptor (\\x1b[43mPLA2R\\x1b[m) between MN patients (n\\xa0=\\xa030) and 30 controls by qRT-PCR and western blot, respectively. The podocyte damage model in vitro was established by \\x1b[43mangiotensin II\\x1b[m (\\x1b[43mAng II\\x1b[m, 100\\xa0nmol/L) exposure for 24\\xa0h. Interaction between miR-130a-5p and \\x1b[43mPLA2R\\x1b[m was determined using dual-luciferase reporter gene assay. MN mice were induced by intravenous injection of cBSA. In this study, miR-130a-5p expression was significantly decreased both in the renal biopsy specimens from MN patients and podocyte cell line AB8/13 following stimulation of \\x1b[43mAng II\\x1b[m. Overexpressed miR-130a-5p in AB8/13 cells significantly attenuated the \\x1b[43mAng II\\x1b[m induced-\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45mapoptosis\\x1b[m\\x1b[m\\x1b[m\\x1b[m\\x1b[m\\x1b[m in vitro. In contrast, down-regulated miR-130a-5p induced podocyte \\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45mapoptosis\\x1b[m\\x1b[m\\x1b[m\\x1b[m\\x1b[m\\x1b[m. \\x1b[43mPLA2R\\x1b[m was identified as the target of miR-130a-5p in AB8/13 cells. And up-regulated or down-regulated \\x1b[43mPLA2R\\x1b[m could obviously attenuate the effect of miR-130a-5p overexpression or knockdown on the \\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45mapoptosis\\x1b[m\\x1b[m\\x1b[m\\x1b[m\\x1b[m\\x1b[m of AB8/13 cells. Furthermore, it was also observed that overexpressed miR-130a-5p by miR-130a-5p agomir could obviously alleviate renal injury in MN mice. In conclusion, decreased miR-130a-5p was contributed to the pathological mechanism of MN through increasing \\x1b[43mPLA2R\\x1b[m expression, which induced podocyte \\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45m\\x1b[45mapoptosis\\x1b[m\\x1b[m\\x1b[m\\x1b[m\\x1b[m\\x1b[m.'\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meeting with Marcus:\n",
    "* Our stopword filter is making things crazy, should be done in post instead\n",
    "* Use group by span in one of the layers in java\n",
    "* Use remove to delete layers\n",
    "\n",
    "* Without stopwordfilterfactory:\n",
    "```\n",
    "Accepting arbitrarily chosen limits of maximized errors of +/- 10%, it could be shown that the system did not work acceptably when the mean carbon dioxide concentration was below 1.5 vol.% within the fresh gas flow rates (2.2--7.7 1 min-1) and the range of minute ventilation (4--10 1 min-1) employed.{'id': 'P00155973A0387'}--------------------\n",
    "```\n",
    "* With stopfilterfactory\n",
    "\n",
    "```\n",
    "Accepting arbitrarily chosen limits of maximized errors of +/- 10%, it could be shown that the system did not work acceptably when the mean carbon dioxide concentration was below 1.5 vol.% within the fresh gas flow rates (2.2--7.7 1 min-1) and the range of minute ventilation (4--10 1 min-1) employed.{'id': 'P00155973A0387'}          Node protmatches#0          \n",
    "\n",
    "  field    value                      \n",
    "\n",
    "  id       42401                      \n",
    "  terms    [Node<terms#53>]           \n",
    "  text     span(main[231:232]) = '1'  \n",
    "          Node protmatches#1           \n",
    "\n",
    "  field    value                       \n",
    "\n",
    "  id       600341                      \n",
    "  terms    [Node<terms#53>]            \n",
    "  text     span(main[236:238]) = '-1'  \n",
    "          Node protmatches#2          \n",
    "\n",
    "  field    value                      \n",
    "\n",
    "  id       42401                      \n",
    "  terms    [Node<terms#53>]           \n",
    "  text     span(main[283:284]) = '1'  \n",
    "          Node protmatches#3           \n",
    "\n",
    "  field    value                       \n",
    "\n",
    "  id       600341                      \n",
    "  terms    [Node<terms#53>]            \n",
    "  text     span(main[288:290]) = '-1'  \n",
    "-------------------- \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 14, 21/5\n",
    "* Run through all pubmed abstracts and run the code on in\n",
    "* Write function to connect protein name to id -> maybe fix write to docria\n",
    "* Fix so that you can run through terminal and send files as input\n",
    "* Start looking at report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From xml.gz files to .xml \n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "path = 'pubmed2018/pubmed/'\n",
    "\n",
    "directory = os.fsencode(path)\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith('.xml.gz'):\n",
    "        with gzip.open(path + filename, 'rb') as f_in:\n",
    "            with open(path + filename[:-3], 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read xml files to json dictionaries\n",
    "from xml.etree import ElementTree as ET\n",
    "import json\n",
    "import os\n",
    "\n",
    "path = 'pubmed2018/pubmed/'\n",
    "\n",
    "directory = os.fsencode(path)\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    \n",
    "    if filename.endswith('.xml'):\n",
    "        parser = ET.iterparse(path + filename)\n",
    "        id = ''\n",
    "        abbe = {}\n",
    "        for event, element in parser:\n",
    "            tag = element.tag\n",
    "            if tag == 'PMID':\n",
    "                id = element.text\n",
    "            elif tag == 'AbstractText':\n",
    "                abbe.setdefault(id, []).append(element.text)\n",
    "            element.clear()\n",
    "\n",
    "        with open('pubmed/json_{}.txt'.format(filename[:-4]), 'w+',encoding=\"utf-8\", errors='ignore') as file:\n",
    "            file.write(json.dumps(abbe))\n",
    "        os.remove(path + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2min 10 sec for 10 files => 12 s per each pubmed.xml\n",
    "We have 972 in total => 3,4 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read dictionary with genetag abstracts \n",
    "import json\n",
    "with open('pubmed/json_pubmed19n0302.txt') as file:\n",
    "    d = json.loads(file.read())\n",
    "i = 0\n",
    "for key in d:\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(key)\n",
    "    print(d[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Process protein names and cell death names using docria and print docria to file\n",
    "\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "from docria.storage import DocumentIO\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "\n",
    "set_large_screen()\n",
    "\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "\n",
    "def get_java_file(jvm, path):\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "def setup_java(p, c):\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    app.buildIndex(get_java_file(jvm, \"{}.txt\".format(p)), get_java_file(jvm, \"{}.fst\".format(p)))\n",
    "    app.buildIndex(get_java_file(jvm, \"{}.txt\".format(c)), get_java_file(jvm, \"{}.fst\".format(c)))\n",
    "\n",
    "    indx = app.loadIndex(get_java_file(jvm, \"{}.fst\".format(p)))\n",
    "    indx2 = app.loadIndex(get_java_file(jvm, \"{}.fst\".format(c)))\n",
    "    \n",
    "    return (app, indx, indx2)\n",
    "\n",
    "# Create a document and tag it\n",
    "def create_doc(key, text, app, i, i2):\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", text)  \n",
    "    doc.props[\"id\"] = key\n",
    "    binary_doc = MsgpackCodec.encode(doc)\n",
    "    search_binary_doc = app.search(i, i2, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)\n",
    "    return doc\n",
    "\n",
    "def filter_away_nodes(d, stop_w):\n",
    "    rm_nodes = []\n",
    "    for node in d['protmatches']:\n",
    "        #If node is a stopword or number or length less than 2\n",
    "        if len(str(node[\"text\"])) < 2 or re.match( \"^[0-9-]+$\", str(node[\"text\"])) or str(node[\"text\"]) in stop_w:\n",
    "            rm_nodes.append(node)\n",
    "    [n.detach() for n in rm_nodes]\n",
    "    return d\n",
    "    \n",
    "#Taggs the abstracts in abstracts_genetag\n",
    "def file_tagger(app, indx, indx2, stop_w, abstracts, filename):\n",
    "    # One doc for each abstract\n",
    "    out = []; dr = []\n",
    "    for key in abstracts:\n",
    "        text = ''.join(filter(None, abstracts[key]))\n",
    "        dr.append(create_doc(key, text, app, indx, indx2))\n",
    "    with DocumentIO.write('docria/out_{}.docria'.format(filename[:-4])) as dw:\n",
    "        prot = 0\n",
    "        lyso = 0\n",
    "        for d in dr:\n",
    "            d = filter_away_nodes(d, stop_w)\n",
    "            dw.write(d)\n",
    "            \n",
    "    return len(dr)\n",
    "\n",
    "p = 'protein_name/protein_names_uniprot(2)'\n",
    "c = 'cell_death_names'\n",
    "path = 'pubmed/'\n",
    "\n",
    "\n",
    "stop_w = set()\n",
    "[stop_w.add(t) for t in stopwords.words('english')]\n",
    "\n",
    "(app, indx, indx2) = setup_java(p, c)\n",
    "\n",
    "directory = os.fsencode(path)\n",
    "\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.startswith('json'):\n",
    "        with open(path + filename, 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "            start = time.time()\n",
    "            d = json.loads(file.read())\n",
    "            l = file_tagger(app, indx, indx2, stop_w, d, filename)\n",
    "            end = time.time()\n",
    "            div = 0\n",
    "            if l > 0:\n",
    "                div = (end-start)/l\n",
    "            print('total time: {} s\\nnbr of abstracts: {}\\ntime per abstract: {} s'.format(str(end-start), l, str(div)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pubmed/json_pubmed19n0040.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    abstracts = json.loads(file.read())\n",
    "for key in abstracts:\n",
    "    print(key)\n",
    "    print(''.join(abstracts[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method: input protein name --> return ID\n",
    "import re\n",
    "\n",
    "def getID(p_name):\n",
    "    with open('protein_name/protein_names_uniprot.txt', 'r') as file:\n",
    "        p_names_db = file.readlines()\n",
    "    with open('protein_name/protein_names_uniprot_indx.txt', 'r') as file:\n",
    "        id_db = file.readlines()        \n",
    "    \n",
    "    for i,line in enumerate(p_names_db):\n",
    "        line = line.rstrip()\n",
    "        if line == p_name:\n",
    "            return id_db[i]\n",
    "\n",
    "result = getID('Helianthinin-G3')\n",
    "print(result)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total time: 134.1003954410553 s\n",
    "nbr of abstracts: 15401\n",
    "time per abstract: 0.008707252479777631 s\n",
    "\n",
    "\n",
    "total time: 99.64722609519958 s\n",
    "nbr of abstracts: 13455\n",
    "time per abstract: 0.007405962548881426 s\n",
    "\n",
    "\n",
    "total time: 94.12135577201843 s\n",
    "nbr of abstracts: 12659\n",
    "time per abstract: 0.007435133562842123 s\n",
    "\n",
    "\n",
    "total time: 104.57637310028076 s\n",
    "nbr of abstracts: 14984\n",
    "time per abstract: 0.006979202689554242 s\n",
    "\n",
    "\n",
    "total time: 104.60536503791809 s\n",
    "nbr of abstracts: 14135\n",
    "time per abstract: 0.007400450303354658 s\n",
    "\n",
    "\n",
    "total time: 124.9278507232666 s\n",
    "nbr of abstracts: 16385\n",
    "time per abstract: 0.007624525524764517 s\n",
    "\n",
    "\n",
    "total time: 128.22814011573792 s\n",
    "nbr of abstracts: 16407\n",
    "time per abstract: 0.007815453167290664 s\n",
    "\n",
    "\n",
    "total time: 122.386305809021 s\n",
    "nbr of abstracts: 15397\n",
    "time per abstract: 0.00794871116509846 s\n",
    "\n",
    "\n",
    "total time: 44.054216623306274 s\n",
    "nbr of abstracts: 5575\n",
    "time per abstract: 0.007902101636467493 s\n",
    "\n",
    "\n",
    "total time: 65.35626196861267 s\n",
    "nbr of abstracts: 9382\n",
    "time per abstract: 0.006966133230506573 s\n",
    "\n",
    "\n",
    "total time: 93.0322675704956 s\n",
    "nbr of abstracts: 13524\n",
    "time per abstract: 0.006879049657682313 s\n",
    "\n",
    "\n",
    "total time: 81.38241481781006 s\n",
    "nbr of abstracts: 11875\n",
    "time per abstract: 0.0068532559846576895 s\n",
    "\n",
    "\n",
    "total time: 83.66334176063538 s\n",
    "nbr of abstracts: 11817\n",
    "time per abstract: 0.007079913832667799 s\n",
    "\n",
    "\n",
    "total time: 103.12827587127686 s\n",
    "nbr of abstracts: 14871\n",
    "time per abstract: 0.006934858171695034 s\n",
    "\n",
    "\n",
    "total time: 95.45782041549683 s\n",
    "nbr of abstracts: 13904\n",
    "time per abstract: 0.006865493413082338 s\n",
    "\n",
    "\n",
    "total time: 97.63898944854736 s\n",
    "nbr of abstracts: 14116\n",
    "time per abstract: 0.00691690205784552 s\n",
    "\n",
    "\n",
    "total time: 89.64432525634766 s\n",
    "nbr of abstracts: 12711\n",
    "time per abstract: 0.007052499823487346 s\n",
    "\n",
    "\n",
    "total time: 78.13509917259216 s\n",
    "nbr of abstracts: 11572\n",
    "time per abstract: 0.006752082541703436 s\n",
    "\n",
    "\n",
    "total time: 95.29924273490906 s\n",
    "nbr of abstracts: 13738\n",
    "time per abstract: 0.006936908045924375 s\n",
    "\n",
    "\n",
    "total time: 68.49690270423889 s\n",
    "nbr of abstracts: 10307\n",
    "time per abstract: 0.006645668254995527 s\n",
    "\n",
    "\n",
    "total time: 83.46684193611145 s\n",
    "nbr of abstracts: 12423\n",
    "time per abstract: 0.00671873476101678 s\n",
    "\n",
    "\n",
    "total time: 85.12544417381287 s\n",
    "nbr of abstracts: 12752\n",
    "time per abstract: 0.006675458294684196 s\n",
    "\n",
    "\n",
    "total time: 84.06328272819519 s\n",
    "nbr of abstracts: 12132\n",
    "time per abstract: 0.006929053967045433 s\n",
    "\n",
    "\n",
    "total time: 86.26507711410522 s\n",
    "nbr of abstracts: 12277\n",
    "time per abstract: 0.007026559999519852 s\n",
    "\n",
    "\n",
    "total time: 79.69249677658081 s\n",
    "nbr of abstracts: 11562\n",
    "time per abstract: 0.006892622104876389 s\n",
    "\n",
    "\n",
    "total time: 84.80725598335266 s\n",
    "nbr of abstracts: 11789\n",
    "time per abstract: 0.007193761640796731 s\n",
    "\n",
    "\n",
    "total time: 111.64475703239441 s\n",
    "nbr of abstracts: 15651\n",
    "time per abstract: 0.007133394481655767 s\n",
    "\n",
    "\n",
    "total time: 99.37922358512878 s\n",
    "nbr of abstracts: 14249\n",
    "time per abstract: 0.00697447003895914 s\n",
    "\n",
    "\n",
    "total time: 91.07553434371948 s\n",
    "nbr of abstracts: 13111\n",
    "time per abstract: 0.006946497928740712 s\n",
    "\n",
    "\n",
    "total time: 80.18461608886719 s\n",
    "nbr of abstracts: 12144\n",
    "time per abstract: 0.006602817530374439 s\n",
    "\n",
    "\n",
    "total time: 93.22969317436218 s\n",
    "nbr of abstracts: 13310\n",
    "time per abstract: 0.007004484836541111 s\n",
    "\n",
    "\n",
    "total time: 90.79783272743225 s\n",
    "nbr of abstracts: 13103\n",
    "time per abstract: 0.006929545350487083 s\n",
    "\n",
    "\n",
    "total time: 92.4807436466217 s\n",
    "nbr of abstracts: 12889\n",
    "time per abstract: 0.007175168255614998 s\n",
    "\n",
    "\n",
    "total time: 59.71534466743469 s\n",
    "nbr of abstracts: 8212\n",
    "time per abstract: 0.0072717175678805035 s\n",
    "\n",
    "\n",
    "total time: 48.48636507987976 s\n",
    "nbr of abstracts: 7073\n",
    "time per abstract: 0.006855134324880498 s\n",
    "\n",
    "\n",
    "total time: 81.16503405570984 s\n",
    "nbr of abstracts: 11474\n",
    "time per abstract: 0.007073822037276437 s\n",
    "\n",
    "\n",
    "total time: 84.95286870002747 s\n",
    "nbr of abstracts: 12301\n",
    "time per abstract: 0.006906175814976625 s\n",
    "\n",
    "\n",
    "total time: 84.01410555839539 s\n",
    "nbr of abstracts: 12067\n",
    "time per abstract: 0.006962302606977325 s\n",
    "\n",
    "\n",
    "total time: 89.616943359375 s\n",
    "nbr of abstracts: 13170\n",
    "time per abstract: 0.006804627438069476 s\n",
    "\n",
    "\n",
    "\n",
    "Number of found matches: 76792, Number of prot: 76056, Number of lyso: 736\n",
    "total time: 1020.4657201766968 s\n",
    "nbr of abstracts: 22178\n",
    "time per abstract: 0.04601252232738285 s\n",
    "Number of found matches: 78568, Number of prot: 77981, Number of lyso: 587\n",
    "total time: 954.7520699501038 s\n",
    "nbr of abstracts: 21032\n",
    "time per abstract: 0.0453952106290464 s\n",
    "Number of found matches: 89160, Number of prot: 88378, Number of lyso: 782\n",
    "total time: 560.9668989181519 s\n",
    "nbr of abstracts: 20845\n",
    "time per abstract: 0.026911340797224844 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary about work since last meeting:\n",
    "We have\n",
    "* Collected all names from uniprot, only euc\n",
    "\n",
    "* Changed the filters in the java files and instead did it all in post processing as using the java files led to unexpected indexing. poS tagger/spacey -> solution takes too long/complicated, solved by checking if stopwords and lengthfilter which seemd to work great (see output)\n",
    "\n",
    "* Started benchmarking time -> takes ~14s to read a pubmed.xml file + ~40s to process with docria = ~1 min\n",
    "\n",
    "* Docria output has one layer for lyso matches and one for prot matches. Should we append genetag names to this?\n",
    "\n",
    "* Wrote a method that takes a protein name and outputs the genetag id (perhaps add to docria)\n",
    "\n",
    "* Started to process all the abstracts\n",
    "\n",
    "* Time to run everything; almost 2 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 15, 22/5-19\n",
    "### Meeting with Sonja:\n",
    "* Booked a meeting to present, maybe have a common theme for the presentation? Overall aim, so much literature, cannot just read it all, lots of scattered pieces, do this to piece them together, also present what the groups are doing. Don't need to go into biology more than one slide. Show an example where proteins and lysosomes are highlighted.\n",
    "* Can read the files compressed\n",
    "* 20 (10 enough) articles, manually tag them and then compare it with our dictionary solution to give some evaluating numbers. Also end presentation with improvement steps. Biocreative or bioinfer are usuall benchmark, but not feasible in this timeframe. Send 3 of these to sonja so that she can pick the proteins. Cherry pick those with at least 3 different protein names. Recall, Precision and other measurements.\n",
    "* Important part of the process to visualize the answers, either with GUI or with our terminal\n",
    "* Finish up work and start process of presentation.\n",
    "* How many articles have these keywords and separate by category (i.e how many match cell death related)\n",
    "* Outline report, provided publishable, it's good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25/5-29\n",
    "For presentation:\n",
    "* Number of abstracts? \n",
    "* How many proteins/genes in uniprot?\n",
    "* How many protein names and lysosome names?\n",
    "    * 703 316 protein names uniprot\n",
    "    * 10 393 protein names genetag\n",
    "    * 713 709 protein names in total\n",
    "    * 70 names related to cell death\n",
    "\n",
    "* Pick 10 articles and manually tag them\n",
    "\n",
    "\n",
    "\n",
    "* Tested 10 xml files got the following data: (Use it get number of proteins/lyso per file/per abstracts)\n",
    "* 19n0968:2043 lyso matches, 26390 abstracts, 109 589 prot\n",
    "* 19n0969:1593 lyso matches, 19 999 abstracts, 81 761 prot\n",
    "* 19n0800:2475 lyso matches, 25 463 abstracts, 125 870 prot\n",
    "* 19n0001:189 lyso matches, 15 401 abstracts, 39 768 prot\n",
    "* 19n0812:2234 lyso matches, 26 019 abstracts, 124273 prot\n",
    "* 19n0143:10 lyso matches, 1382 abstracts, 3019 prot\n",
    "* 19n0322: 2498 lyso matches, 23646 abstracts, 127316 prot\n",
    "* 19n0694: 1994 lyso matches, 26675 abstracts, 104257 prot\n",
    "* 19n0601: 2431 lyso matches, 24885 abstracts, 119678 prot\n",
    "* 19n0566: 2524 lyso matches, 24490 abstracts, 120822 prot\n",
    "* 19n0887: 1969 lyso matches, 26152 abstracts, 108726 prot\n",
    "* 19n0787: 2315 lyso matches, 25665 abstracts, 128747 prot\n",
    "* 19n0706: 2549 lyso matches, 25304 abstracts, 124674 prot\n",
    "* 19n0669: 503 lyso matches, 5314 abstracts, 24188 prot\n",
    "* 19n0048: 712 lyso matches, 21548 abstracts, 73002 prot\n",
    "\n",
    "tot = 26039 lyso matches, 318 333 abstracts, 1 415 690 prot\n",
    "\n",
    "21 222 abstracts per xml file, ~20 600 000 abstracts in total\n",
    "\n",
    "1736 lyso matches per xml file, 0.08 lyso matches per abstract\n",
    "\n",
    "94 379 prot matches per xml file, 4,45 prot matches per abstract\n",
    "\n",
    "\n",
    "* Wrong:\n",
    "When reading abstract for '30395490', since we read from tag to tag the last bit after <sub> is not included in the abstract added to the database.\n",
    "```decreases in ED<sub>50</sub> values at concentrations of DHA equal to human plasma levels, suggesting that DHA could be used as an attractive radiosensitizer agent in CRC patients with mutant-p53.</AbstractText>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example list of protein names:\n",
    "\n",
    "```\n",
    "104 kDa microneme/rhoptry antigen\n",
    "p104\n",
    "TP04_0437\n",
    "Protein 108\n",
    "8-hydroxygeraniol dehydrogenase\n",
    "Cr10HGO\n",
    "10HGO\n",
    "110 kDa antigen\n",
    "PK110\n",
    "11S globulin seed storage protein 2\n",
    "11S globulin seed storage protein II\n",
    "Alpha-globulin\n",
    "11S globulin seed storage protein 2 acidic chain\n",
    "11S globulin seed storage protein II acidic chain\n",
    "11S globulin seed storage protein 2 basic chain\n",
    "11S globulin seed storage protein II basic chain\n",
    "11S globulin seed storage protein G3\n",
    "Helianthinin-G3\n",
    "11S globulin seed storage protein G3 acidic chain\n",
    "11S globulin seed storage protein G3 basic chain\n",
    "```\n",
    "\n",
    "Example list of cell death names:\n",
    "```\n",
    "apoptosis\n",
    "lysosome-dependent cell death\n",
    "Exposure of Phosphatidylserine\n",
    "necroptosis\n",
    "Lower cell number\n",
    "Parthanatos\n",
    "Neuronal death\n",
    "MOMP\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Turn docra file into readable temp file\n",
    "from docria.storage import DocumentIO\n",
    "nbrs = ['322', '694', '601', '566', '887', '787', '706', '669', '048']\n",
    "for nbr in nbrs:\n",
    "    count_abstracts = 0\n",
    "    lyso_matches = []\n",
    "    prot_matches = []\n",
    "    print('--------------------' + nbr + '------------------\\n\\n\\n')\n",
    "    with DocumentIO.read('docria/out_json_pubmed19n0{}.docria'.format(nbr)) as dr:\n",
    "        with open('docria/temp_19n0{}.txt'.format(nbr), 'w+', encoding=\"utf-8\", errors='ignore') as f:\n",
    "            for doc in dr:\n",
    "                count_abstracts += 1\n",
    "                f.write(str(doc.text[\"main\"]))\n",
    "                f.write(str(doc.props))\n",
    "                prot_matches.append(len(doc[\"protmatches\"]))\n",
    "                lyso_matches.append(len(doc[\"lysomatches\"]))\n",
    "\n",
    "                for d in doc[\"protmatches\"]:\n",
    "                    f.write(str(d))  \n",
    "                for d in doc[\"lysomatches\"]:\n",
    "                    f.write(str(d))\n",
    "                f.write(\"--------------------\\n\\n\")\n",
    "    print(str(sum(lyso_matches)) + ' lyso matches, ' + str(count_abstracts) + ' abstracts, ' + str(sum(prot_matches)) + ' prot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match '[phospholipase:PROT] [A2:PROT]' instead of [phospholipase A2] because 'Phospholipase A2' starts with a capital letter and thus the dominant right does not work as it only matches with 'phospholipase' from genetag.\n",
    "\n",
    "Right now we match 'using dual-[luciferase reporter gene:PROT] assay', eventough this is a method. Marking it as false.\n",
    "\n",
    "TNF-$\\alpha$ does not get matched as it is written in uniprot as TNF-alpha\n",
    "\n",
    "Future improvement: mix of case sensetive?\n",
    "* We assume this is a TP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
