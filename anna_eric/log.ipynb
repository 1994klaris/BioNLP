{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log for project in Computer Science\n",
    "\n",
    "By Anna and Eric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1, 4/4 - 2019\n",
    "\n",
    "Today the goal is to understand the project and be prepared for mondays meeting.\n",
    "\n",
    "Step \n",
    "\n",
    "     1) Download UniProt and check out the dataset\n",
    "\n",
    "     2) Create the dictionary\n",
    "     \n",
    "     3) Get tool from Marcus\n",
    "     \n",
    "     4) Train a tagger with machine learning\n",
    "     \n",
    "So we would like to finish step 1 and start reseach on 2.\n",
    "\n",
    "---\n",
    "Today we sat from 13-16. We started by creating a private git repo with this file in it. Then we downloaded the uniprot dataset in xml form. We proceded to try and parse it with BeautifulSoup. The file was too big for that method.\n",
    "\n",
    "After that we tried with ElementTree instead which can go through each element one by one. This worked and we eventually got a list of proteins (over 200 000).\n",
    "\n",
    "We got stuck at which dictionary we want to create. Do we want one that's protein -> gene1 gene2 gene3 and thus also mine genes from the xml file. Or is the dictionary something else? For next time this needs to be cleared up in order to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "#in protein_names.txt\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'shortName' in tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "f = open('protein_names_short.txt', 'w')\n",
    "f.write('\\n'.join(proteins))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 2, 8/4 - 2019\n",
    "\n",
    "Today we had our second meeting with Pierre and Sonja. We got clarification about the biology bit of the project but also some tips going forward. Now we have a Slack and are to get an invite to the Github repo. Also, an invite to Endnote.\n",
    "\n",
    "TODO: Invite Sonja, Pierre to Overleaf document\n",
    "\n",
    "Tips:\n",
    "\n",
    "    1) Search PubMed for 'biomedical text-mining' and 'bio NLP' (Jensen)\n",
    "    \n",
    "    2) Get annotation from Hannes\n",
    "\n",
    "So this week we wish to get the infrastructure up and running.\n",
    "\n",
    "---\n",
    "We started by looking at Marcus email and getting his program to work. After a lot of problems with installation of the various packages and programs we finally got it to work, almost 2h later.\n",
    "\n",
    "Question: What is the file 'disease_index.fst' and how is it created? Should we create this?\n",
    "\n",
    "---\n",
    "We now have the basic infrastructure up and running. We have done some tests on running it with our dataset of proteins from UniProt. Also we started looking at BioCreative's datasets, but we'll leave that for next time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions to start JVM (need Marcus repo):\n",
    "\n",
    "    1) cd [katalog]/mention-index-py4j\n",
    "\n",
    "    2) mvn package\n",
    "\n",
    "    3) cd target\n",
    "\n",
    "    4) java -jar mentions-index-py4j-1.0-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install docria\n",
    "#!{sys.executable} -m pip install py4j\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    # Here we do the py4j equivalent for new java.io.File(path)\n",
    "    return jvm.java.io.File(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app.buildIndex(get_java_file(\"protein_names.txt\"), get_java_file(\"protein_names.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"protein_names.fst\"))\n",
    "\n",
    "doc = Document()\n",
    "doc.add_text(\"main\",\"Comparison with alkaline phosphatases and 5-nucleotidase\")\n",
    "binary_doc = MsgpackCodec.encode(doc) \n",
    "search_binary_doc = app.search(indx, binary_doc)\n",
    "doc = MsgpackCodec.decode(search_binary_doc)\n",
    "\n",
    "tuls = []\n",
    "for term in doc[\"matches\"]:\n",
    "    tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    #print(str(term[\"text\"]), \"-\", \"%d:%d\" % (term[\"text\"].start, term[\"text\"].stop))\n",
    "print(tuls)\n",
    "dominant_right(tuls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 3, 9/4-2019\n",
    "We have had some questions about the purpose and think we have finally figured it out. To identify the entities 'protein' and 'diseases' like cell death. To start with we will identify proteins by checking a pubmed article and using our uniprot list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through the file pubmed19n0651.xml and saves the abstract text as one textfile\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='pubmed19n0651.xml'\n",
    "out_name = 'abstract_text19n0651.txt'\n",
    "parser = ET.iterparse(filename)\n",
    "f=open(out_name, \"a+\", encoding=\"utf-8\")\n",
    "for event, element in parser:\n",
    "    if element.tag == 'AbstractText':\n",
    "        if element.text:\n",
    "            f.write(element.text)\n",
    "    element.clear()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Prints all occurences of proteins in the abstract file.\n",
    "app.buildIndex(get_java_file(\"protein_names_long.txt\"), get_java_file(\"protein_names_long.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"protein_names_long.fst\"))\n",
    "data = ''\n",
    "with open('abstract_text19n0651.txt', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('.')\n",
    "for s in sp:\n",
    "    hits = indx.search(s)\n",
    "    for hit in hits:\n",
    "        print(hit.start, hit.end, \" \".join([term.raw for term in hit.terms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints all occurences of cell deaths in the abstract file.\n",
    "app.buildIndex(get_java_file(\"cell_death_names.txt\"), get_java_file(\"cell_name_index.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"cell_name_index.fst\"))\n",
    "data = ''\n",
    "with open('abstract_text19n0651.txt', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('.')\n",
    "for s in sp:\n",
    "    hits = indx.search(s)\n",
    "    for hit in hits:\n",
    "        print(hit.start, hit.end, \" \".join([term.raw for term in hit.terms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tried to use the model that https://aclweb.org/anthology/W16-5104 implemented at github: https://github.com/withtwist/medical-ner. After transpiling the code to Python 2, changing the encoding of the data etc. we still got compilation errors. It seemed too big of a job so we scraped it. Since we have been able to identify proteins from the abstract and where they are located, we are now interested in finding a way to create the tracer with machine learning. We found the article *Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts. \n",
    "\n",
    "\n",
    "Note: We noticed that the indx.search is case sensitive, which needs to be fixed. (e.g. toLowerCase or similar). Check w Marcus!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion of todays work: We have fixed the dictionaries and spent some time making sure the xml reader for uniprot included the edgecases and included 'shortName'. We also started tinkering with neural networks by downloading existing repos but we could not make it work. We started reading a couple of papers to get inspiration for our tagger. \n",
    "\n",
    "For next time, we will look deeper into the neural networks and use the tips Marcus sent us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 4, 14/4-2019\n",
    "We prepared for tomorrows meeting by looking at Marcus tips that he sent us. Also we started looking at more journals and papers and found a really good one: [A Novel Approach for Protein-Named Entity Recognition and Protein-Protein Interaction Extraction](https://www.hindawi.com/journals/mpe/2015/942435/). They describe step-by-step a method with SVM to extract the entities. If they agree on tomorrows meeting we will follow this guide next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 5, 15/4-2019\n",
    "Today after the meeting we have a clearer understanding how to move forward. The process follows:\n",
    "    1. Add list of corpus with summarization of each content and size to github\n",
    "    2. [x] Fix java to ignore case\n",
    "    3. [x] Complete a working model with the dictionary model\n",
    "        i. [x] ~~Fix reader and use Element tree instead.~~ Also read in 'pmid' and change output to 'pmid \\t abstract_text \\n'.\n",
    "        ii. [x] Match output to corpus annotation\n",
    "    4. [ ] Create evaluator for dictionary model\n",
    "        i. Use [genetag](https://www.ncbi.nlm.nih.gov/pubmed/15960837) for evaluation or chemner? or both?\n",
    "        ii. Compare with e.g recall\n",
    "    5. Add more to dictionary and run evaluator to see if it improves. If ambigueties use dominant write.\n",
    "    \n",
    "After this is completed we will start on the machine learning model.\n",
    "    1. Use POS? Pre-processing?\n",
    "    2. Sentence -> Tokens -> Tagger -> Phrases -> ML (BioLSTM and Bioascembedding(spelling?))\n",
    "    3. Check out page [nlpprogress](http://nlpprogress.com/english/named_entity_recognition.html) for NER datasets\n",
    "   \n",
    "\n",
    "Möte med Sonja: https://biocreative.bioinformatics.udel.edu/resources/corpora/biocreative-iii-corpus/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = app.loadIndex(get_java_file(\"protein_names_long.fst\"))\n",
    "\n",
    "data = ''\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('>>')\n",
    "dictionary = {}\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    #If it is an excerpt\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ''.join(line.split(': ')[1:])\n",
    "        dictionary[id] = text                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use indx.search to find all occurrences of proteins in abstracts from genetag. \n",
    "indx = app.loadIndex(get_java_file(\"protein_name_index.fst\"))\n",
    "\n",
    "d = dictionary\n",
    "out = []\n",
    "for key in d:\n",
    "    value = d[key]\n",
    "    hits = indx.search(value)    \n",
    "    for hit in hits:\n",
    "        # term.raw for term in hit.terms prints lists of protein names. Each list is protein name\n",
    "        out.append('|'.join([key, str(hit.start) + ' ' + str(hit.end), \" \".join([term.raw for term in hit.terms])]))\n",
    "file = open('genetag.out', 'w+', encoding=\"utf-8\", errors='ignore')\n",
    "file.write('\\n'.join(out))\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "Today we sat for quite some time with merge conflicts in jupyter notebooks as it would not compile after a while. Also we tried to implement 2 models for evaluation; genetag with perl and another one with python 2.5. The gentag could have worked, but we got such a low score that it probably checks line by line simmilarity or something like it. This means we only got 8 correct matches which seems very low. We will look further into this next time.\n",
    "\n",
    "We succesfully changed the tokenizers in java by removing 2 of them and adding one to ignore cases. We became aware of how many small words were in the 'shortName' tag from chemprot. So we got protein names like 'AM, OR, AND' etc. We thus created a textfile 'protein_names_long' which contains only the long names. This meant we got a more expected result, but we are also missing out on some of the real names. TODO: Think about how to include the short names - do we want to set a limit of min 2 characters?\n",
    "\n",
    "Finally we created an out file to match the evaluation file in genetag. It reads from the database and using our proteinnames builds the same style file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 6, 17/4-2019\n",
    "We started by creating another file 'protein_names_short', and discussed whether to use this in combination with long names or only long names. For now we will only use long.\n",
    "\n",
    "Today's goal is to create/find/adapt an evaluator that works on our data.\n",
    "\n",
    "We can run the perl evaluator from genetag and get the following result:\n",
    "```\n",
    "TP: 2\n",
    "FP: 18211\n",
    "FN: 406\n",
    "Precision: 0.000109811672980838\n",
    "Recall: 0.00490196078431373\n",
    "```\n",
    "Which is very bad. we think this is because the java tokenizer changes words like \"h-55\" to \"h - 55\". So it matches both 'h' and '55'. We need to change such that the hyphen does not get split in this way. There are probably more errors as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "\n",
    "app.buildIndex(get_java_file(\"protein_names_long.txt\"), get_java_file(\"protein_names_long.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"protein_names_long.fst\"))\n",
    "\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    #If it is an excerpt\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "                print(id)\n",
    "            elif line.startswith('TEXT'):\n",
    "                print(line)\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "                print(text)\n",
    "        dictionary[id] = text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = dictionary\n",
    "out = []\n",
    "\n",
    "for key in d:\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", d[key])    \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "        \n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    \n",
    "    for m in matches:\n",
    "        out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "file = open('genetag_short.out', 'w+', encoding=\"utf-8\", errors='ignore')\n",
    "file.write('\\n'.join(out))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We chatted with Marcus for quite a bit today. As we wanted to add dominant_right we first tried to do it in Java but after a lot of trial-and-error it did not work and we switched to the python version. We also had to switch the way wee send in data as we needed to use docria instead of python.\n",
    "\n",
    "Additionally we started getting back to the filters in java that we originally had. We also tried to work on the evaluator, but we are suspecting that something is weird in it as we get an unexpected nbr of matches (only ~27 TruePositive). \n",
    "\n",
    "For next time: add stopwords and fix min_length of matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run the perl evaluator\n",
    "- go to medtag/genetag\n",
    "- run `atperl alt_eval.perl ../[PATH]/genetag_short.out Gold.format > answer.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
