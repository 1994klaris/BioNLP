{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log for project in Computer Science\n",
    "\n",
    "By Anna and Eric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1, 4/4 - 2019\n",
    "\n",
    "Today the goal is to understand the project and be prepared for mondays meeting.\n",
    "\n",
    "Step \n",
    "\n",
    "     1) Download UniProt and check out the dataset\n",
    "\n",
    "     2) Create the dictionary\n",
    "     \n",
    "     3) Get tool from Marcus\n",
    "     \n",
    "     4) Train a tagger with machine learning\n",
    "     \n",
    "So we would like to finish step 1 and start reseach on 2.\n",
    "\n",
    "---\n",
    "Today we sat from 13-16. We started by creating a private git repo with this file in it. Then we downloaded the uniprot dataset in xml form. We proceded to try and parse it with BeautifulSoup. The file was too big for that method.\n",
    "\n",
    "After that we tried with ElementTree instead which can go through each element one by one. This worked and we eventually got a list of proteins (over 200 000).\n",
    "\n",
    "We got stuck at which dictionary we want to create. Do we want one that's protein -> gene1 gene2 gene3 and thus also mine genes from the xml file. Or is the dictionary something else? For next time this needs to be cleared up in order to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "#in protein_names.txt\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='../../EDAN70-project/uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'fullName' in tag: # or 'shortName' in tag\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "f = open('protein_names_long.txt', 'w')\n",
    "f.write('\\n'.join(proteins))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 2, 8/4 - 2019\n",
    "\n",
    "Today we had our second meeting with Pierre and Sonja. We got clarification about the biology bit of the project but also some tips going forward. Now we have a Slack and are to get an invite to the Github repo. Also, an invite to Endnote.\n",
    "\n",
    "TODO: Invite Sonja, Pierre to Overleaf document\n",
    "\n",
    "Tips:\n",
    "\n",
    "    1) Search PubMed for 'biomedical text-mining' and 'bio NLP' (Jensen)\n",
    "    \n",
    "    2) Get annotation from Hannes\n",
    "\n",
    "So this week we wish to get the infrastructure up and running.\n",
    "\n",
    "---\n",
    "We started by looking at Marcus email and getting his program to work. After a lot of problems with installation of the various packages and programs we finally got it to work, almost 2h later.\n",
    "\n",
    "Question: What is the file 'disease_index.fst' and how is it created? Should we create this?\n",
    "\n",
    "---\n",
    "We now have the basic infrastructure up and running. We have done some tests on running it with our dataset of proteins from UniProt. Also we started looking at BioCreative's datasets, but we'll leave that for next time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions to start JVM (need Marcus repo):\n",
    "\n",
    "    1) cd [katalog]/mention-index-py4j\n",
    "\n",
    "    2) mvn package\n",
    "\n",
    "    3) cd target\n",
    "\n",
    "    4) java -jar mentions-index-py4j-1.0-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install docria\n",
    "#!{sys.executable} -m pip install py4j\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    # Here we do the py4j equivalent for new java.io.File(path)\n",
    "    return jvm.java.io.File(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8 neurosin\n",
      "212 221 synuclein\n",
      "251 259 Neurosin\n",
      "261 273 kallikrein -6\n"
     ]
    }
   ],
   "source": [
    "app.buildIndex(get_java_file(\"protein_names_long.txt\"), get_java_file(\"disease_index.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"disease_index.fst\"))\n",
    "hits = indx.search(\"neurosin Multiple system atrophy (MSA) is a progressive, neurodegenerative disease characterized by parkinsonism, resistance to dopamine therapy, ataxia, autonomic dysfunction, and pathological accumulation of α-synuclein (α-syn) in oligodendrocytes. Neurosin (kallikrein-6) is a serine\")\n",
    "for hit in hits:\n",
    "    print(hit.start, hit.end, \" \".join([term.raw for term in hit.terms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 3, 9/4-2019\n",
    "We have had some questions about the purpose and think we have finally figured it out. To identify the entities 'protein' and 'diseases' like cell death. To start with we will identify proteins by checking a pubmed article and using our uniprot list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pubmed19n0651.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-2f63dbf805e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pubmed19n0651.xml'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'abstract_text19n0651.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ericholmstrom/anaconda/lib/python3.6/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36miterparse\u001b[0;34m(source, events, parser)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0mclose_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m         \u001b[0mclose_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pubmed19n0651.xml'"
     ]
    }
   ],
   "source": [
    "#Goes through the file pubmed19n0651.xml and saves the abstract text as one textfile\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='pubmed19n0651.xml'\n",
    "out_name = 'abstract_text19n0651.txt'\n",
    "parser = ET.iterparse(filename)\n",
    "f=open(out_name, \"a+\", encoding=\"utf-8\")\n",
    "for event, element in parser:\n",
    "    if element.tag == 'AbstractText':\n",
    "        if element.text:\n",
    "            f.write(element.text)\n",
    "    element.clear()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Prints all occurences of proteins in the abstract file.\n",
    "app.buildIndex(get_java_file(\"protein_names.txt\"), get_java_file(\"protein_name_index.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name_index.fst\"))\n",
    "data = ''\n",
    "with open('abstract_text19n0651.txt', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('.')\n",
    "for s in sp:\n",
    "    hits = indx.search(s)\n",
    "    for hit in hits:\n",
    "        print(hit.start, hit.end, \" \".join([term.raw for term in hit.terms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prints all occurences of cell deaths in the abstract file.\n",
    "app.buildIndex(get_java_file(\"cell_death_names.txt\"), get_java_file(\"cell_name_index.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"cell_name_index.fst\"))\n",
    "data = ''\n",
    "with open('abstract_text19n0651.txt', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('.')\n",
    "for s in sp:\n",
    "    hits = indx.search(s)\n",
    "    for hit in hits:\n",
    "        print(hit.start, hit.end, \" \".join([term.raw for term in hit.terms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tried to use the model that https://aclweb.org/anthology/W16-5104 implemented at github: https://github.com/withtwist/medical-ner. After transpiling the code to Python 2, changing the encoding of the data etc. we still got compilation errors. It seemed too big of a job so we scraped it. Since we have been able to identify proteins from the abstract and where they are located, we are now interested in finding a way to create the tracer with machine learning. We found the article *Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts. \n",
    "\n",
    "\n",
    "Note: We noticed that the indx.search is case sensitive, which needs to be fixed. (e.g. toLowerCase or similar). Check w Marcus!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion of todays work: We have fixed the dictionaries and spent some time making sure the xml reader for uniprot included the edgecases and included 'shortName'. We also started tinkering with neural networks by downloading existing repos but we could not make it work. We started reading a couple of papers to get inspiration for our tagger. \n",
    "\n",
    "For next time, we will look deeper into the neural networks and use the tips Marcus sent us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 4, 14/4-2019\n",
    "We prepared for tomorrows meeting by looking at Marcus tips that he sent us. Also we started looking at more journals and papers and found a really good one: [A Novel Approach for Protein-Named Entity Recognition and Protein-Protein Interaction Extraction](https://www.hindawi.com/journals/mpe/2015/942435/). They describe step-by-step a method with SVM to extract the entities. If they agree on tomorrows meeting we will follow this guide next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 5, 15/4-2019\n",
    "Today after the meeting we have a clearer understanding how to move forward. The process follows:\n",
    "    1. Add list of corpus with summarization of each content and size to github\n",
    "    2. Fix java to ignore case\n",
    "    3. Complete a working model with the dictionary model\n",
    "        i. Fix reader and use Element tree instead. Also read in 'pmid' and change output to 'pmid \\t abstract_text \\n'.\n",
    "        ii. Match output to corpus annotation\n",
    "    4. Create evaluator for dictionary model\n",
    "        i. Use [genetag](https://www.ncbi.nlm.nih.gov/pubmed/15960837) for evaluation or chemner? or both?\n",
    "        ii. Compare with e.g recall\n",
    "    5. Add more to dictionary and run evaluator to see if it improves. If ambigueties use dominant write.\n",
    "    \n",
    "After this is completed we will start on the machine learning model.\n",
    "    1. Use POS? Pre-processing?\n",
    "    2. Sentence -> Tokens -> Tagger -> Phrases -> ML (BioLSTM and Bioascembedding(spelling?))\n",
    "    3. Check out page [nlpprogress](http://nlpprogress.com/english/named_entity_recognition.html) for NER datasets\n",
    "   \n",
    "\n",
    "Möte med Sonja: https://biocreative.bioinformatics.udel.edu/resources/corpora/biocreative-iii-corpus/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indx = app.loadIndex(get_java_file(\"protein_name_index.fst\"))\n",
    "\n",
    "data = ''\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('>>')\n",
    "dictionary = {}\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    #If it is an excerpt\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ''.join(line.split(': ')[1:])\n",
    "        dictionary[id] = text\n",
    "print(dictionary)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "We tried using t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use indx.search to find all occurrences of proteins in abstracts from genetag. \n",
    "indx = app.loadIndex(get_java_file(\"protein_name_index.fst\"))\n",
    "\n",
    "d = dictionary\n",
    "out = []\n",
    "for key in d:\n",
    "    value = d[key]\n",
    "    hits = indx.search(value)    \n",
    "    for hit in hits:\n",
    "        # term.raw for term in hit.terms prints lists of protein names. Each list is protein name\n",
    "        out.append('|'.join([key, str(hit.start) + ' ' + str(hit.end), \" \".join([term.raw for term in hit.terms])]))\n",
    "file = open('genetag.out', 'w+', encoding=\"utf-8\", errors='ignore')\n",
    "file.write('\\n'.join(out))\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
